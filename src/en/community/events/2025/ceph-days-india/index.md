---
title: Ceph Days India 2025
date: 2025-01-23
end: 2025-01-23
location: Bengaluru, Karnataka
venue: Hotel Royal Orchid - No 1, Golf Avenue, adjoining KGA Golf Course, HAL Old Airport Rd, Domlur, Bengaluru, Karnataka 560008
image: "/assets/bitmaps/ceph-days.png"
sponsors:
  - label:
    list:
      - name: Clyso
        logo: /assets/bitmaps/logo-clyso.png
  - label:
    list:
      - name: Canonical
        logo: /assets/bitmaps/logo-canonical.png
  - label:
    list:
      - name: IBM
        logo: /assets/bitmaps/logo-ibm.png
tags:
  - ceph days
---

### Bringing Ceph to India

A full-day event dedicated to sharing Ceph’s transformative power and fostering
the vibrant Ceph community with the community in India!

The expert Ceph team, Ceph’s customers and partners, and the Ceph community
join forces to discuss things like the status of the Ceph project, recent Ceph
project improvements and roadmap, and Ceph community news. The day ends with
a networking reception, to foster more Ceph learning.

## Important Dates

- **CFP Opens:** 2024-12-10
- **CFP Closes:** 2025-01-07
- **Speakers receive confirmation of acceptance:** 2025-01-08
- **Schedule Announcement:** 2025-01-09
- **Event Date:** 2025-01-23

<br />

## Schedule

<table>
  <tr>
   <td width="10%">Time
   </td>
   <td width="70%">Abstract
   </td>
   <td width="60%">&emsp; Speaker
   </td>
  </tr>
  <tr>
   <td>9:00
   </td>
   <td><strong>Registration</strong>
   </td>
  </tr>
   <tr>
   <td>9:30
   </td>
   <td><strong>Keynote talk</strong>
   </td>
   <td><strong>&emsp; Vincent Hsu - IBM Fellow, VP and CTO of IBM Storage, IBM</strong> 
   </td>
  </tr>
    <tr>
   <td>10:00
   </td>
   <td><strong>Ceph NVMe-oF roadmap</strong>
   </td>
   <td><strong>&emsp; Orit Wasserman</strong>
<p>
&emsp; Distinguished Engineer, IBM
   </td>
  </tr>
  <tr>
   <td>10:30
   </td>
   <td><strong>State of CephFS: Three Easy Pieces</strong>
<p>
This talk focusses on the current (and near future) state of the three pieces that make up a Ceph File System - Ceph Metadata Sever (MDS), Clients and a set of Ceph Manager Plugins. Much advancements have been made to the Ceph File System recently, opening up gateways for wider adoption. Some features are already available in recent releases and some are under development. We detail these enhancements by breaking up nicely into each of the three pieces. Ceph File System specific manager plugins have come a long way to now becoming the de-facto for subvolume/crash-consistent snapshot management and mirroring. We discuss about those. And finally, we peek into what is upcoming in CephFS for Tentacle ("T") release. Existing and new CephFS users would find it helpful to assess and plan ahead for its adoption.
</p>
   </td>
   <td><strong> &emsp; Venky Shankar & Greg Farnum</strong>
<p>
&emsp; IBM 
   </td>
  </tr>  
  <tr>
   <td>11:00
   </td>
   <td><strong>RGW Roadmap</strong>
   <p>
   Ceph RGW (Rados Gateway) is a component of the Ceph Storage Cluster that provides an S3-compatible interface for object storage. 
   The major highlights of current release Squid includes:
    New S3 Bucket Notification Data Layout:- A new data layout for S3 Bucket Notification topics is introduced. This new layout improves scalability and supports multisite replication. 
    Versioned Bucket Index Cleanup Tools:- New tools are introduced to identify and fix issues with versioned bucket indexes, including extra entries and unlinked objects. 
    User Accounts Feature:- A new User Accounts feature unlocks several new AWS-compatible IAM APIs for managing users, keys, groups, roles, and policies 
Upcoming Tentacle release will have:  
    Bucket logging feature, enabling granular auditing and monitoring of bucket-level activities, enhancing security and operational visibility. 
     A significant milestone for the d4n caching in RGW, with performance improvements and key bug fixes. 
    Introduction of a new in-order sharding layout promises to significantly increase the number of objects that can be stored within a single bucket. This innovative approach eliminates the performance degradation associated with increasing shard counts during listing operations, optimizing overall system scalability. 
    Clean up unused bucket index shards after resharding in multisite deployments has been merged, enhancing efficiency and resource utilization 
    Restore support added to retrieve transitioned objects in cloud/tape/archive zone. Similar to glacier api in AWS S3. 
    Added GetObjectAttributes functionality provide users with more granular and efficient access to object metadata. 
    Dedup Prototype, efficient deduplication of whole objects
    </p>
   </td>
   <td><strong> &nbsp; &nbsp; &nbsp; &nbsp; Matthew Benjamin - Program Director, Ceph Object Storage Development </strong>
   <br />
&nbsp; &nbsp; &nbsp; &nbsp; 
   </td>
  </tr>  
  <tr>
   <td>11:30
   </td>
   <td><strong>Break</strong>
   </td>
  </tr> 
  <tr>
   <td>11:45
   </td>
   <td><strong>State of Ceph Dashboard</strong>
<p>
The Ceph Dashboard is an integral part of the Ceph ecosystem, offering an intuitive web-based interface for managing, monitoring, and troubleshooting Ceph clusters. This talk will explore the current state of the Ceph Dashboard, highlighting its evolution, recent enhancements, and the roadmap for future improvements and showcase them with a demo.
   </td>
   <td><strong>&emsp; Nizamudeen A & Afreen Misbah</strong>
<p>
&emsp; IBM
   </td>
  </tr> 
  <tr>
   <td>12:15
   </td>
   <td><strong>Ceph-LLM: Unlocking the Power of Community driven and Open Source AI for Ceph</strong>
<p>
In the ever-evolving world of artificial intelligence (AI), Large Language Models (LLMs) have transformed the way we interact with technology. However, challenges such as data transparency and the lack of community-driven contributions often hinder their development. Enter Ceph-LLM, an innovative initiative made possible by Instructlab, an open source finetuning framework created with the collaboration of IBM Research and Red Hat, that seeks to create an open-source, community-driven LLM tailored to the Ceph ecosystem.
</p>
   </td>
   <td><strong> &emsp; Pavan Govindraj</strong>
<p>
&emsp; IBM
   </td>
  </tr>
<tr>
   <td>12:45
   </td>
   <td><strong>Lunch</strong>
   </td>
  </tr>
<tr>
   <td>13:30
   </td>
   <td><strong>Panel Discussion with Ceph Community Leaders</strong>
   </td>
   <td><strong> &emsp; Moderated by Gaurav Sitlani </strong>
   <p>
   &emsp; IBM
   </td>
  </tr> 
  </tr>  
  <tr>
   <td>14:15
   </td>
   <td><strong>Lightning talks</strong>
   </td>
   <td><strong> &emsp; TBD</strong>
   </td>
  </tr> 
  </tr> 
   <tr>
   <td>14:15
   </td>
   <td><strong>Crimson</strong>
<p>
The Crimson project is an effort to build a replacement ceph-osd well suited to the new reality of low latency, high throughput, persistent memory and NVMe technologies.
In this talk, our aim to is to explain how exactly we plan to do this with the Seastar framework. 
Additionally, we will be explaining how the current classical OSD is different from the Crimson OSD and what features are currently supported in Crimson. 
<br />
   </td>
   <td><strong> &emsp; Aishwarya Mathuria</strong>
   <p>
   &emsp; IBM
   </td>
  </tr>
  <tr>
   <td>15:00
   </td>
   <td><strong>Samba with Ceph: A new data service solution for windows clients</strong>
<p>
So, you have a Ceph File System up and running and wondering how to manage it. Especially, you have been wondering how not to break it and therefore leave the defaults as-it-is. But now, you have increased users and that results in a temptation to fiddle with the defaults -- to extract the last bit of performance from the cluster. But wait, before you do that, make sure you understand various configuration options and how they affect the cluster all around. This talk ensures that such pitfalls are avoided and if at all you end up in one, how to come out clean.
   </td>
   <td><strong> &emsp; Mohit Bisht</strong>
<p>
&emsp; IBM
</p>
   </td>
  </tr> 
  <tr>
   <td>15:30
   </td>
   <td><strong>Break</strong>
   </td>
  </tr> 
<tr>
   <td>16:00
   </td>
   <td><strong>Performance benchmarking of a Ceph cluster and IO500</strong>
   <p>
"As Ceph continues to evolve as a leading solution for software-defined storage, understanding and optimizing cluster performance is crucial for administrators and developers alike. There are various commonly used performance benchmarking techniques for Ceph clusters using popular tools such as Elbencho, CBT (Ceph Benchmarking Tool), and hsbench. There are also advanced profiling techniques, such as CPU profiling, to uncover potential bottlenecks and opportunities for optimization in various Ceph workloads.
The session will cover the IO500 benchmark, a standardized and widely recognized tool for evaluating file system performance in HPC environments. Attendees will gain insights into how IO500 can be used to assess CephFS performance and compare it against other storage solutions, highlighting its advantages and use cases."
   </p>
   </td>
   <td><strong>&emsp; Sirisha Guduru</strong>
<p>
&emsp; Clyso GmbH
</p>
   </td>
  </tr> 
  <tr>
   <td>16:30
   </td>
   <td><strong>MicroCeph: Cloud storage on ground</strong>
<p>
MicroCeph is an opinionated ceph orchestration tool which allows for single command operations for deploying, operating and configuring a Ceph cluster. Over the past year, we have added interesting features like a standardized API for remote replication, support for QAT acceleration, and many improvements in code-base and documentation. This talk will provide a comprehensive overview of MicroCeph offering insights for professionals, students and anyone interested in deploying and operating a Ceph cluster. 
</p>
   </td>
   <td><strong> &emsp; Utkarsh Bhatt</strong>
<p>
&emsp; Canonical
   </td>
  </tr>
  <tr>
   <td>17:00
   </td>
   <td><strong>Closing Remarks</strong>
  </tr>


Join the Ceph announcement list, or follow Ceph on social media for Ceph event
updates:

- [Ceph Announcement list](https://lists.ceph.io/postorius/lists/ceph-announce.ceph.io/)
- [Twitter](https://twitter.com/ceph)
- [LinkedIn](https://www.linkedin.com/company/ceph/)
- [FaceBook](https://www.facebook.com/cephstorage/)