---
title: Ceph Days London 2024
date: 2024-07-17
end: 2024-07-17
location: London
venue: Canonical London, 3 More London Pl, London SE1 2RE
image: "/assets/bitmaps/ceph-days.png"
sponsors:
  - label:
    list:
      - name: Canonical
        logo: /assets/bitmaps/logo-canonical.png
  - label:
    list:
      - name: IBM
        logo: /assets/bitmaps/logo-ibm.png
tags:
  - ceph days
---

### Bringing Ceph to London

<a class="button" href="https://ti.to/open-source-events/ceph-days-london-2024">Register Now!</a>

A full-day event dedicated to sharing Ceph’s transformative power and fostering
the vibrant Ceph community with the community in London!

The expert Ceph team, Ceph’s customers and partners, and the Ceph community
join forces to discuss things like the status of the Ceph project, recent Ceph
project improvements and roadmap, and Ceph community news. The day ends with
a networking reception, to foster more Ceph learning.

The registration will be limited!

## Important Dates

- **CFP Opens:** 2024-06-10
- **CFP Closes:** 2024-06-30
- **Speakers receive confirmation of acceptance:** 2024-07-03
- **Schedule Announcement:** 2024-07-08
- **Event Date:** 2024-07-17

<br />

## Schedule

<table>
  <tr>
   <td width="10%">Time
   </td>
   <td width="50%">Abstract
   </td>
   <td width="40%"><center>Speaker</center>
   </td>
  </tr>
  <tr>
   <td>8:00 AM
   </td>
   <td><strong>Check-in and Breakfast</strong>
   </td>
   <td>
   </td>
  </tr>
  <tr>
   <td>9:00 AM
   </td>
   <td><strong>Welcome</strong>
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/phil.jpg" width="400" />Phil Williams (Canonical)</strong></center>
   </td>
  </tr>
  <tr>
   <td>9:10 AM
   </td>
   <td><strong>Keynote - State of Ceph</strong>
<p>
A look at the newest Ceph release, current development priorities, and the latest activity in the Ceph community.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-nyc/neha-ojha.jpg" width="400" />Neha Ojha (IBM)</strong></center>
   </td>
  </tr>
  <tr>
   <td>9:30 AM
   </td>
   <td><strong>Crimson Project (Squid Release)</strong>
<p>
In the presentation I will introduce the Crimson project and explain the rationale behind it.
As Squid is the first release to include Crimson as a tech preview, the presentation will cover the current status and future goals of the project.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/matan.jpeg" width="400" />Matan Breizman (IBM)</strong></center>
   </td>
  </tr>
  <tr>
   <td>10:00 AM
   </td>
   <td><strong>Taming a Cephalopod Swarm: Multi-cluster Monitoring Comes to the Ceph Dashboard</strong>
<p>
Traditionally matching Ceph core's fast pace, the Dashboard is steadily surpassing CLI with enhanced features: monitoring, centralized logging, guided workflows... and the pioneering multi-cluster monitoring.
One Ceph Dashboard to rule them all.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_EP_IBM.jpeg" width="400" />Ernesto Puerta, IBM</strong></center>
   </td>
  </tr>
    <tr>
   <td>10:30 AM
   </td>
   <td><strong>Tea/Coffee Break</strong>
   </td>
   <td>
   </td>
  </tr>
  <tr>
   <td>11:00 AM
   </td>
   <td><strong>We ran out of IOPSWe ran out of IOPS! Adding NVMe devices to an overworked S3 service</strong>
<p>
Our Ceph service was deployed with all HDD OSDs, intended for OpenStack Cinder volumes. We found radosgw/S3 was the use which really took off and we ran out of HDD IOPS performance in the rgw pools. Adding NVMe OSDs restored stability and sanity.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_Holland.jpg" width="400" />David Holland, Wellcome Sanger</strong></center>
   </td>
  </tr>
  <tr>
   <td>11:10 AM
   </td>
   <td><strong>Crimson: experiments to reach saturation</strong>
<p>
In this talk, we describe the experiments we conducted to gain an empirical understanding of the performance for Crimson (the new technology OSD for Ceph). We show our methodology and discuss the results, as well as next steps.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_JJPP_IBM.jpeg " width="400" />Jose Juan Palacios Perez, IBM</strong></center>
   </td>
  </tr>
  <tr>
   <td>11:20 AM
   </td>
   <td><strong>All things in moderation: Our experience applying cluster-wide rate limiting to Ceph object storage</strong>
<p>
We built a distributed QoS system to limit both request and data-transfer rate on a per-user level across our Ceph object storage clusters. We present learnings from a couple years of using this to protect our clusters and our user's experience.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_JH_Bloomberg.jpg" width="400" />Jacques Heunis, Bloomberg</strong></center>
   </td>
  </tr>
  <tr>
   <td>11:50 AM
   </td>
   <td><strong>Unlocking Ceph's Potential with NVMe-oF Integration</strong>
<p>
Discover how NVMe-oF integrates with Ceph to enhance data storage efficiency. We'll cover implementation challenges and solutions for high availability, performance, and scalability. Join us to unlock Ceph's full potential with NVMe-oF.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_OW_IBM.jpg" width="400" />Orit Wasserman, IBM</strong></center>
   </td>
  </tr>
   <tr>
   <td>12:20 PM
   </td>
   <td><strong>Lunch</strong>
   </td>
   <td>
   </td>
  </tr>
   <tr>
   <td>1:30 PM
   </td>
   <td><strong>Integrating NVMe-oF with Ceph and Juju</strong>
<p>
This talk describes the development and usage of a Juju charm meant to allow users to create NVMe-oF devices backed by RBD images in a scalable way to provide high-availability guarantees in a user-friendly fashion.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_LLG_Canonical.jpeg" width="400" />Luciano Lo Giudice, Canonical</strong></center>
   </td>
  </tr>
   <tr>
   <td>2:00 PM
   </td>
   <td><strong>Making the right hardware choices</strong>
<p>
With the increasing diversity and ever changing hardware options that are available this talk will take a look through some of those options and what makes for sensible configuration options for Ceph. 
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_DS_Croit.jpeg" width="400" />Darren Soothill, Croit</strong></center>
   </td>
  </tr>
  <tr>
   <td>2:30 PM
   </td>
   <td><strong>DisTRaC: Transient Ceph On RAM</strong>
<p>
DisTRaC is a fast and scalable open-source deployment tool for creating quick and efficient Ceph storage systems on high-performance computing utilising RAM. The talk introduces DisTRaX and shows use cases and results using this tool.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_RFIgabryelMasonWilliams0001.jpg" width="400" />Gabryel Mason-Williams, Rosalind Franklin Institute</strong></center>
   </td>
  </tr>
  <tr>
   <td>2:40 PM
   </td>
   <td><strong>Ceph Performance Benchmarking and CBT (Ceph Benchmark Tool) Improvement plans</strong>
<p>
CBT (Ceph Benchmarking Tool) is a utility that benchmarks clusters to highlight the max performance of the system.  This talk is about the CBT vision and improvement plans to simplify and generate full comprehensive reports with comparisons.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_LS_IBM.jpeg" width="400" />Lee Sanders, IBM</strong></center>
   </td>
  </tr>
  <tr>
   <td>2:50 PM
   </td>
   <td><strong>NVMeoF and VMware Integrations for Ceph</strong>
<p>
Ceph NVMe over TCP unleashes the power of high performing NVMe drives with the scalability of disaggregated storage. Find out how the new protocol exposes high-performing block access over networks, and how VMware environments can benefit from it!
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_MB_IBM.jpeg" width="400" />Mike Bukhart, IBM</strong></center>
   </td>
  </tr>
  <tr>
   <td>3:00 PM
   </td>
   <td><strong>Snack Break</strong>
   </td>
   <td>
   </td>
  </tr>
  <tr>
   <td>3:30 PM
   </td>
   <td><strong>The art of redundancy across failure domains</strong>
<p>
Proper cluster design is an art, especially with complex failure domains. This talk strives some aspects of role instance distribution, best practices for redundant instances placement and minimizing cluster footprint with room for future growth.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_MM_RH.jpeg" width="400" />Matthias Muench, Red Hat</strong></center>
   </td>
  </tr>
  <tr>
   <td>4:00 PM
   </td>
   <td><strong>Next Generation Erasure Coding</strong>
<p>
Erasure coding has lower storage overheads than replicated pools, but does not perform as well for short random I/O. This talk looks at a variety of techniques that we are planning on implementing to improve erasure coding performance.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/CDL_BS_IBM.jpeg" width="400" />Bill Scales, IBM</strong></center>
   </td>
  </tr>
  <tr>
   <td>4:30 PM
   </td>
   <td><strong>Sunbeam and Ceph sitting in a tree</strong>
<p>
OpenStack Sunbeam is a re-think of how we deploy and operate OpenStack Clouds including how we integrate with Ceph - in the form of MicroCeph.   Discover more about the motivations for this project and what we're doing technically.
   </td>
   <td><center><strong><img src="/assets/bitmaps/events/2024/ceph-days-london/jamesheadshot.jpeg" width="400" />James Page, Canonical</strong></center>
   </td>
  </tr>
  <tr>
   <td>5:00 PM
   </td>
   <td><strong>Closing Remarks</strong>
   </td>
   <td><center><strong>Phil, Neha, Danny, Orit</strong></center>
   </td>
  </tr>
  <tr>
   <td>5:15 PM
   </td>
   <td><strong>Networking Reception</strong>
   </td>
   <td>
   </td>
  </tr>

 
</table>
