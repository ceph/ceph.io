How long have you been using Ceph?,Why do you use Ceph?,Sector(s) of your organization?,Which resources do you check when you need help?  [Ceph Documentation],Which resources do you check when you need help?  [Ceph-users mailing list],Which resources do you check when you need help?  [Slack],Which resources do you check when you need help?  [Meet-up or other local groups],Which resources do you check when you need help?  [IRC],Which resources do you check when you need help?  [Commercial Provider],Which resources do you check when you need help?  [Source code],Which resources do you check when you need help?  [Additional Resource],Where should the Ceph community focus its efforts?[Reliability],Where should the Ceph community focus its efforts?[Management / Configuration],Where should the Ceph community focus its efforts?[Scalability],Where should the Ceph community focus its efforts?[Security],Where should the Ceph community focus its efforts?[Interoperability],Where should the Ceph community focus its efforts?[Performance],Where should the Ceph community focus its efforts?[Monitoring],Where should the Ceph community focus its efforts?[Documentation],How do you participate in the Ceph community?,How likely are you to recommend Ceph to a colleague?,Please share the top reason for picking the previous answer (optional),"Top 3 wishlist features (add each on a separate line, optional)",What countries are your deployments in?,"Do you have Telemetry enabled on your cluster(s)?  (Telemetry will automatically report anonymized information about your cluster, like the size of your cluster, what version of Ceph you are running, and what bugs you have encountered.)",Why is telemetry not enabled?,Have you disabled any of the default telemetry channels?,Have you enabled the 'ident' telemetry channel (which allows you to provide a cluster description and contact email address)?,Have you used the public Ceph Telemetry dashboards and/or Devices dashboard?,Why did you use the public telemetry dashboard and/or the device telemetry dashboard?,How many clusters does this information represent?,Raw capacity (in TB) of your largest cluster,Total raw capacity of all clusters (before replication),Total usable capacity of all clusters (after replication),Which Ceph releases do you run?,Whose Ceph packages do you use?,What operating systems are you using on cluster nodes?,Do you use cache tiering?,How soon do you apply dot/minor releases to your cluster?,Why?,How soon do you perform major version upgrades?,Why?,Which Ceph Manager modules do you enable?,Which chassis / server vendors do you use for cluster nodes?,Which processor architectures and manufacturers do you use?,Which type of storage devices are used?,Which OSD layout strategies do you employ?,Do you use a dedicated cluster / replication / private network for OSD node replication and recovery?,Network technologies in use,IP address families in use,Which SSD form factor are important to you,Preferred number of CPU sockets per server and why,Preferred number of CPU sockets per server and why[Explain why?],How many OSDs in your largest cluster?,Which OSD back ends do your clusters employ?,Data protection strategies in use,With which platforms do you use Ceph?,Do you provide RADOS Block Device (RBD) services with your Ceph clusters?,"Do you provide Ceph Object Gateway (RGW, Rados Gateway) services with your Ceph clusters?",Do you provide Ceph File system (Ceph FS) services with your Ceph clusters?,RBD deployment status,What are your use cases for RBD?,Are you using rbd-mirror?,RBD (block storage) client access methods in use,Do you use RBD snapshots and / or clones?,"RGW Ceph cluster roles (dev, staging, production, PoC)",What are your RGW workloads?,Average number of RGW daemons per object service cluster,Size in GB of the largest S3 / Swift object stored,RGW APIs used,RGW clients and libraries used,RGW authentication mechanism,What load balancer(s) or caches do you use with RGW?,Number of RGW federated multi-sites?,Is RGW Data Caching and CDN  feature useful to you? if so list your use case/s?,Do you have Ceph Multisite Clusters?,"Do you have more than one realm in your setup? if so, how many?",Do you have more than one zone group in your setup?,"Do you have more than one zone in your setup? if so, how many in the largest zone group?",Is the syncing policy between zones global or per bucket?,"Do you use Sync Modules? if so, which modules? and list with which cloud providers. ","Do you use Sync Modules? if so, which modules? and list with which cloud providers. [List with which Cloud providers:]","Do you use Bucket Notifications? if so, which endpoints do you use? ","Do you use Bucket Notifications? if so, which endpoints do you use? [Which other endpoints would you like to have?]",CephFS deployment status,What are your use cases for CephFS?,CephFS consumption methods,"Typical number of file system clients (for largest cluster, if multiple clusters)","MDS cache size (for largest cluster, if multiple clusters)","Number of active MDS daemons (for largest cluster, if multiple clusters)",Do you use CephFS subtree pinning?,Do you use CephFS snapshots?,Do you use CephFS Quotas?,Which of these experimental Ceph FS features are important to you?,How likely are you to recommend the Ceph Dashboard to a colleague?,Does the dashboard help you to perform tasks better/faster than using the CLI?,How often do you use certain dashboard features?[Landing page (at a glance monitoring)],How often do you use certain dashboard features?[Display cluster logs],How often do you use certain dashboard features?[Mon status page],How often do you use certain dashboard features?[OSD management],How often do you use certain dashboard features?[Pools management],How often do you use certain dashboard features?[RBD management],How often do you use certain dashboard features?[iSCSI target management],How often do you use certain dashboard features?[NFS Ganesha management],How often do you use certain dashboard features?[CephFS management],How often do you use certain dashboard features?[RGW management],How often do you use certain dashboard features?[View embedded Grafana dashboards],What functionality do you miss most in the dashboard?,What metrics and monitoring tools do you use?,Which deployment and configuration platforms do you use to manage your Ceph clusters?,
More than 5 years,"Open source,Feature set,Data durability / reliability / integrity",Non-profit,3,,,,,,,,4,5,4,3,3,3,3,2,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Contributing code,Contributing / enhancing documentation,Ceph events / conferences,Member of the Ceph Foundation",10 (Promoter),,,United States,,,,,,,1,100,200,100,Octopus (15.x),Upstream packages,"Ubuntu,RHEL / CentOS / Fedora","Yes, in front of EC pools for functionality",Within a month after release,I delay upgrades because of the effort involved,Within a month of release,I upgrade quickly to get new features,"balancer,crash,devicehealth,diskprediction,pg_autoscaler",Supermicro,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Containerized daemons",No,10 Gb/s Ethernet,IPv4 only,"M.2,U.2 (2.5-inch)",Single,,10 to 100,BlueStore,"Replication (3x),Erasure coding",Kubernetes,No,No,Yes,,,,,,,,,,,,,,,,,,,,,,,,,Production,"Home Directories,Archive Storage",Linux kernel CephFS mount,1 - 5,4G - 15G,1,No,Yes,No,Multiple File Systems within a Ceph Cluster,10 (Promoter),No (Please specify) - don't use it,,,,,,,,,,,,none,Ceph Dashboard,cephadm,
More than 5 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity,Performance",Commercial,4,5,,,,,3,,5,,,,,5,,5,"Mailing list,Reporting issues via the bug tracker,Contributing / enhancing documentation,Ceph events / conferences",10 (Promoter),,,Netherlands,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,1,524,545,295,Nautilus (14.x),"Distribution packages,Vendor packages",Ubuntu,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,crash,iostat,Other (Please specify) - telegraf","Supermicro,Fujitsu","ARM,Intel Xeon,AMD Epyc","SSD (SATA, SAS),NVMe","LVM (ceph-volume),Containerized daemons",No,"10 Gb/s Ethernet,DAC (Copper)",IPv6 only,U.2 (2.5-inch),Single,,100 to 500,BlueStore,Replication (3x),"OpenNebula,rbd-nbd",Yes,No,Yes,"Development / Testing,Production","Cloud,Virtualization","Other (Please specify) - No, no other ceph cluster to replicate to (yet)",Other (Please specify) - iSCSI (LIO with rbd-nbd),Yes,Proof of Concept (PoC),"Video,CDN,Cloud,Containers,Home Directories,Archive Storage",3,1,S3,s3cmd / s3tools,RGW (built-in),Other (Please specify) - Varnish,0,Use case:,No,,,,,,,,,Production,Cloud,"Linux kernel CephFS mount,ceph-fuse",101 - 500,128G+,1,No,No,Yes,"Multiple File Systems within a Ceph Cluster,LazyIO",10 (Promoter),No (Please specify),3,,,,,,,,,,,"Don't use it much, so don't know",,,
Less than 1 year,Open source,Commercial,,,,,,,,,,,,,,,,,,10 (Promoter),,,That would be Telling :p,"No, telemetry is not enabled on any clusters",,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability",Academic,3,4,0,3,5,0,2,,2,3,2,2,3,3,4,5,"IRC / Slack / etc,Ceph events / conferences",10 (Promoter),I just like how difficult is to get Ceph in a state where it doesn't work.,- More AWS features (channeling our users),Finland,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,1024,1024,873,Nautilus (14.x),Distribution packages,RHEL / CentOS / Fedora,No,Longer,I only upgrade as needed to address issues,Within a year,I only upgrade as needed to address specific bugs or needed features,"balancer,iostat","Dell,HPE (Hewlett Packard)",Intel Xeon,"HDD (SATA, SAS)","LVM (ceph-volume),Partitions",No,25 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),No preference / beats me,,100 to 500,BlueStore,Erasure coding,OpenShift,Yes,Yes,No,Development / Testing,Archive Storage,"No, it is not needed",Linux kernel RBD,No,Production,Cloud,1,1024,S3,"Boto3,s3cmd / s3tools,Other (Please specify) - Nextcloud",RGW (built-in),HAproxy,0,Use case:,No,,,,,,,,,,Other (Please specify) - none,Other (Please specify) - none,1 - 5,1G,0,No,No,No,LazyIO,5 (Detractor),No (Please specify) - I haven't used dashboard yet,0,0,0,0,0,0,0,0,0,0,0,Haven't tried it yet,Prometheus,Ansible (ceph-ansible),
Between 2-5 years,"Scalability,Cost,Data durability / reliability / integrity",Academic,5,4,0,2,2,0,3,,5,3,4,3,5,4,3,3,"Reporting issues via the bug tracker,Ceph events / conferences",8 (Passive),"Cost, and adaptability",More advanced user management with object gateway. Tools to let authenticated users to create own key pairs in object gateway.,Finland,"Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted)",I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,960,960,873,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Longer,I delay upgrades because of the effort involved,Within a year,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability","balancer,iostat","Dell,HPE (Hewlett Packard)","Intel (other),AMD (other)","HDD (SATA, SAS)",LVM (ceph-volume),Yes,25 Gb/s Ethernet,IPv4 only,,No preference / beats me,,100 to 500,BlueStore,Erasure coding,None,Yes,Yes,No,Production,"Log,Archive Storage","No, it is not needed",Linux kernel RBD,No,Production,"Cloud,Backup",2,1000,S3,"Boto3,s3cmd / s3tools",RGW (built-in),HAproxy,1,Use case:,No,,,,,,,,,,"Other (Please specify) - None, we don't have CephFS","Other (Please specify) - None, we don't have CephFS",1 - 5,1G,0,No,No,Unknown,Multiple File Systems within a Ceph Cluster,0 (Detractor),"No (Please specify) - Can't really judge, since we haven't used this.",0,0,0,0,0,0,0,0,0,0,0,"Can't really answer, since we don't use this.","Prometheus,Grafana (custom),node_exporter,ceph_exporter,Telegraf","Ansible (ceph-ansible),Ansible (other / homebrew)",
Between 2-5 years,"Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Commercial,5,3,0,0,5,0,2,,5,3,4,3,2,4,3,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",9 (Promoter),,,Denmark,"No, telemetry is not enabled on any clusters",I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,,,"No, I didn't realize they existed",,1,128,128,43,Nautilus (14.x),Distribution packages,RHEL / CentOS / Fedora,No,Longer,"I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","devicehealth,iostat",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,"U.2 (2.5-inch),Add In Cards",Dual,Most bang for the buck without risking IO starvation,10 to 100,Both Filestore and BlueStore,Replication (3x),None,Yes,No,Yes,Production,"Log,Big Data & Analytics,Archive Storage","No, it is not needed",Linux kernel RBD,No,,Other (Please specify) - Not using RGW - forced to answer even though I answered no to using RGW,0,0,S3,None,Other (Please specify) - Not using RGW - forced to answer even though I answered no to using RGW,None,0,Use case:Not using RGW - forced to answer even though I answered no to using RGW,No,,,,,,,,,Production,"Log,Big Data & Analytics,Archive Storage",Linux kernel CephFS mount,6 - 50,32G - 64G,3,No,No,Yes,"Mantle (Programmable Metadata Balancer),Multiple File Systems within a Ceph Cluster",9 (Promoter),No (Please specify) - I prefer the CLI,4,2,3,2,2,2,0,0,2,0,3,N/A,"Prometheus,Grafana (custom),node_exporter,ceph_exporter",Other (Please specify) - None - small cluster,
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,5,3,0,0,3,0,4,,4,1,5,2,4,5,3,2,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Contributing code,Ceph events / conferences",10 (Promoter),,,"Germany,Poland,United Kingdom","Yes, telemetry is enabled on some of my clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,I enabled 'ident' and configured a cluster description and/or email address,Yes,Out of curiosity,1,43008,43008,28672,Nautilus (14.x),We built a custom version,Ubuntu,No,Longer,"I delay upgrades because of the effort involved,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,crash,devicehealth","Supermicro,We build our own",Intel Xeon,"HDD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,10 Gb/s Ethernet,IPv4 only,M.2,Dual,,5000+,BlueStore,Erasure coding,Other (Please specify) - S3,No,Yes,No,,,,,,Production,Archive Storage,8,6,"S3,RGW admin API,NFS re-export of S3 buckets","Boto,Boto3,s3cmd / s3tools,custom / in-house",RGW (built-in),DNS round-robin,0,Use case:,No,,,,,,,,,Production,Cloud,NFS (nfs-ganesha),6 - 50,4G - 15G,2,No,No,No,Multiple File Systems within a Ceph Cluster,6 (Detractor),No (Please specify) - don't using,0,0,0,0,0,0,0,0,0,0,0,n/a,"Prometheus,Grafana (custom),node_exporter,ceph_exporter",Ansible (ceph-ansible),
Between 1-2 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Other (Please specify) - Financial,5,3,0,0,0,0,3,,5,4,3,4,4,4,3,5,"Mailing list,Contributing / enhancing documentation",10 (Promoter),,More user/bucket management out of the dashboard and maybe minor updates through dashboard,Germany,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),Yes,Out of curiosity,1,12,12,4,Octopus (15.x),Upstream packages,Ubuntu,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I delay upgrades because of the effort involved,I apply upgrades quickly because they require little effort",Within half a year of release,"I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features","balancer,devicehealth,pg_autoscaler,I don't know / default set",Dell,Intel Xeon,"SSD (SATA, SAS)",LVM (ceph-volume),No,"10 Gb/s Ethernet,RJ45 (Copper)",IPv4 only,"M.2,U.2 (2.5-inch)",Dual,"In case of failures, one CPU can be removed and the server restarted (So one node fails only short time)",10 or fewer,BlueStore,"Replication (3x),Erasure coding",KRBD directly on Linux systems,No,Yes,No,,,,,,"Staging,Production","Build,Containers,Archive Storage",3,2,"S3,RGW admin API","Amazon SDK,s3cmd / s3tools",RGW (built-in),HAproxy,0,Use case:no,No,,,,,,,Http,,,,,,,,,,,,10 (Promoter),No (Please specify) - User and Bucket Management lacks,5,3,3,4,4,0,0,0,0,4,4,User and Bucket Management,"Ceph Dashboard,InfluxDB,Grafana (custom),Nagios/icinga",Ansible (ceph-ansible),
More than 5 years,"Open source,Scalability,Cost,Feature set,Data durability / reliability / integrity,Performance",Commercial,4,4,0,0,2,0,1,,5,4,4,3,2,4,3,3,"IRC / Slack / etc,Contributing code,Contributing / enhancing documentation",10 (Promoter),,,France,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,6,277,756,300,Octopus (15.x),Upstream packages,Debian,No,Within a month after release,I delay upgrades due to concerns about regression,Within half a year of release,I delay upgrades due to concerns about stability,"balancer,pg_autoscaler","Dell,Supermicro",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt)",No,10 Gb/s Ethernet,Dual stack,U.2 (2.5-inch),Single,"Single high performance socket is enough, and better than dual low performance socket",10 to 100,BlueStore,"Replication (2x),Replication (3x)","Kubernetes,VMware,Proxmox,rbd-nbd",Yes,Yes,No,Production,"Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),iSCSI (tcmu-runner),rbd-nbd",Yes,Production,Backup,2,10,"S3,RGW admin API","Boto,s3cmd / s3tools",RGW (built-in),Nginx,0,Use case:,No,,,,,,,,,,Other (Please specify) - I no longer use cephfs,Other (Please specify) - None,2500+,128G+,1024,Unknown,Unknown,Unknown,Inline Data,4 (Detractor),No (Please specify) - I prefer cli over gui. Nothing personnal.,,,,,,,,,,,,I don't use it;,"InfluxDB,Grafana (custom),Telegraf","Puppet,Rook",
Between 2-5 years,"Feature set,Data durability / reliability / integrity",Academic,,,,,,,,,,,,,,,,,,1 (Detractor),,,Afghanistan,"No, telemetry is not enabled on any clusters",,,,"No, I didn't realize they existed",,1,1,1,1,Octopus (15.x),Vendor packages,Ubuntu,No,Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within half a year of release,I upgrade quickly to get new features,,IBM,"ARM,Intel Xeon","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),At-rest encryption (dmcrypt)",Yes,"100 Mb/s Ethernet,1000 Mb/s Ethernet (GigE, gigabit)",IPv4 only,,Single,,10 or fewer,"Filestore (XFS, EXT4, BTRFS)",Replication (2x),KVM / QEMU,No,No,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 1-2 years,"Feature set,Data durability / reliability / integrity",Commercial,,,,,,,,,,,,,,,,,,3 (Detractor),,,Afghanistan,"No, telemetry is not enabled on any clusters",,,,,,1,1,1,1,Infernalis (9.x) or older,Upstream packages,Ubuntu,No,Within a week after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a month of release,I upgrade quickly to get new features,,IBM,ARM,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),At-rest encryption (dmcrypt)",No,"100 Mb/s Ethernet,1000 Mb/s Ethernet (GigE, gigabit)",IPv4 only,,"Single,Quad",,10 or fewer,"Filestore (XFS, EXT4, BTRFS),BlueStore",Replication (2x),OpenStack,No,No,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1 (Detractor),Yes,,,,,,,,,,,,r,,,
More than 5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity,Other (Please specify) - Openstack integration","Academic,Non-profit",4,3,0,5,0,0,4,,5,4,5,4,3,5,5,5,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",10 (Promoter),I have more than 30PB usable ceph capacity for reasons?,- Bluestore speed and stabilization - Unified component development (previous releases  are not fully backporting things )  - multi-tenant rgw aware config management ( can't specify per RGW values on centralized config),Finland,"Yes, telemetry is enabled on some of my clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations",,I enabled 'ident' and configured a cluster description and/or email address,Yes,Out of curiosity,8,20000,30000,20000,Nautilus (14.x),Distribution packages,RHEL / CentOS / Fedora,No,Within a week after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a year,Other (Please specify) - End user  bindings,"balancer,crash,iostat,pg_autoscaler","Dell,HPE (Hewlett Packard),Supermicro","Intel Xeon,Intel (other),AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe",LVM (ceph-volume),Yes,"10 Gb/s Ethernet,40 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet,RJ45 (Copper),DAC (Copper),AOC / Fiber",IPv4 only,"M.2,U.2 (2.5-inch),Add In Cards",No preference / beats me,depends on cluster and aquisition method,1000 to 3000,BlueStore,"Replication (3x),Erasure coding","OpenStack,Kubernetes,OpenShift",Yes,Yes,No,Production,"Cloud,Containers,Virtualization","No, it is not compatible with my RBD feature requirements","librbd (e.g., in combination with qemu)",Yes,Production,"Log,Video,Big Data & Analytics,AI/ML,HPC,Cloud,Containers,Virtualization,Archive Storage,Backup",8,5000,"S3,Swift","Amazon SDK,Boto3,s3cmd / s3tools,custom / in-house,Other (Please specify) - restic,rclone",Keystone,"HAproxy,DNS round-robin",0,Use case:,No,,,,,,,,bucket notification is not working with nautilus?,,,,,,,,,,,5 (Detractor),No (Please specify) - Currently it kills mgr on big ceph clusters,0,0,0,0,0,0,0,0,0,0,5,stability,"Ceph Dashboard,Grafana (custom),Nagios/icinga,Graphite","Ansible (ceph-ansible),Ansible (other / homebrew)",
Less than 1 year,Open source,Commercial,3,,,,,,,,,,,,,,,3,,10 (Promoter),,,India,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,Scalability,Cost,Data durability / reliability / integrity,Integration with adjacent technologies","Commercial,Academic",4,4,0,0,5,1,1,,5,3,4,3,1,4,3,4,"IRC / Slack / etc,Reporting issues via the bug tracker,Contributing code,Ceph events / conferences",9 (Promoter),not-losing-data,,"Norway,Sweden","Yes, telemetry is enabled on some of my clusters",My cluster(s) are running a Ceph version older than Luminous,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,5,3200,10000,3000,"Octopus (15.x),Mimic (13.x),Jewel (10.x)","Upstream packages,Distribution packages",RHEL / CentOS / Fedora,No,Longer,"I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Never (clusters remain on major release they were deployed with),"I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,pg_autoscaler,I don't know / default set",Supermicro,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",No,"10 Gb/s Ethernet,40 Gb/s Ethernet",IPv4 only,"M.2,U.2 (2.5-inch)",Single,AMD Epycs have enough cores for one host.,100 to 500,Both Filestore and BlueStore,"Replication (3x),Replication (4 or more copies),Erasure coding","OpenStack,KVM / QEMU",Yes,Yes,No,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",No,Production,"Video,Cloud,Archive Storage,Backup",3,1024,S3,"Amazon SDK,Boto3,s3cmd / s3tools","RGW (built-in),Keystone","HAproxy,Other (Please specify) - caddyserver",0,Use case:,No,,,,,,,,,,,,,,,,,,,7 (Passive),No (Please specify) - accustomed to the cli,3,1,3,1,2,0,0,0,0,0,0,.,"Ceph Dashboard,InfluxDB,Prometheus,Nagios/icinga,Telegraf","Ansible (ceph-ansible),Puppet,Ansible (other / homebrew)",
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Performance,Integration with adjacent technologies",Academic,4,5,0,3,0,2,0,,4,3,5,3,3,4,4,3,"Reporting issues via the bug tracker,Ceph events / conferences",9 (Promoter),Scalable performant and industry standard,Updates to kernel client vs FUSE Lazy IO on kernel client,Sweden,"Yes, telemetry is enabled on all of my clusters",,"device: anonymized device health metrics (e.g., SMART hard disk metrics)",I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,1,2100,2100,1000,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy","balancer,iostat,pg_autoscaler",Supermicro,Intel Xeon,"HDD (SATA, SAS),NVMe",LVM (ceph-volume),Yes,25 Gb/s Ethernet,IPv4 only,Add In Cards,Dual,It is enough,100 to 500,BlueStore,"Replication (3x),Erasure coding","OpenStack,KRBD directly on Linux systems",Yes,Yes,Yes,Production,"Cloud,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Feature set,High availability,Data durability / reliability / integrity,Other (Please specify) - proxmox",Commercial,5,3,0,0,5,0,1,,5,3,4,5,3,5,3,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",10 (Promoter),experience with ceph reliabillity.,https://tracker.ceph.com/issues/1680 https://tracker.ceph.com/issues/10679 better debian repo (support multiple versions ceph for all current debian versions),Norway,"Yes, telemetry is enabled on some of my clusters","I haven't gotten around to it yet,My cluster(s) are on a protected network that does not have access to the internet",,I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,22,22,7,"Luminous (12.x),Nautilus (14.x)",Vendor packages,Debian,No,Longer,"I delay upgrades due to concerns about regression,Other (Please specify) - i wait to see if dirtwash run into issues",Longer,"I delay upgrades due to concerns about performance regressions,Other (Please specify) - changed operational model eg containers.","balancer,crash,devicehealth,pg_autoscaler,Other (Please specify) - dashboard",Supermicro,Intel Xeon,"HDD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",No,"10 Gb/s Ethernet,DAC (Copper)",IPv6 only,"M.2,E1.L (EDSFF, ruler form factor)",Dual,"HCI deployment, suiteable size for workload",10 to 100,BlueStore,"Replication (3x),Erasure coding","Proxmox,KVM / QEMU",Yes,No,Yes,Production,"Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,Production,"Containers,Virtualization,Archive Storage,Other (Please specify) - image templates",Linux kernel CephFS mount,1 - 5,2G - 3G,2,No,No,No,Inline Data,10 (Promoter),Yes,5,2,5,1,1,1,1,1,1,1,4,osd class separated graphs   ceph df  iow the ssd class can be overfull while total is fine.,"Ceph Dashboard,Proxmox,Prometheus,Grafana (custom),node_exporter",Other (Please specify) - proxmox,
Between 2-5 years,Other (Please specify) - Part of the job,Academic,3,3,,,5,,,,0,0,0,0,0,5,0,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Contributing / enhancing documentation",6 (Detractor),The documentation is extremely lacking. The examples makes use of basic features. No demo's/docs on features or scaled out servers.,Documentation Documentation Documentation,South Africa,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations",,,"No, I didn't realize they existed",,2,6000,12000,8000,"Octopus (15.x),Luminous (12.x)",Distribution packages,Ubuntu,No,Within a week after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a month of release,I only upgrade as needed to address specific bugs or needed features,"balancer,crash,I don't know / default set",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Yes,100 Gb/s Ethernet,IPv4 only,"M.2,Add In Cards",Dual,,500 to 1000,BlueStore,Erasure coding,OpenStack,Yes,No,Yes,Production,Scratch,"No, it is not needed","librbd (e.g., in combination with qemu)",Unknown,,,,,,,,,,,,,,,,,,,,Production,"Scratch,Big Data & Analytics,Home Directories",Linux kernel CephFS mount,501 - 2500,65G - 127G,1,Unknown,No,No,Multiple File Systems within a Ceph Cluster,0 (Detractor),No (Please specify) - Dashboard doesn't work with ceph-ansible and no one is fixing it,0,0,0,0,0,0,0,0,0,0,0,"That it cant be deployed with ceph-ansible so cannot use it. Logged multiple bugs over months. Dashboard team says its not supported by them. Ansible team says they are not going to support it, since its the dashboard guys baby.",Other (Please specify) - none can't get dashboard working with ceph-ansible,Ansible (ceph-ansible),
More than 5 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity",Commercial,4,2,0,0,4,0,0,,5,2,4,3,2,4,3,5,"IRC / Slack / etc,Reporting issues via the bug tracker",10 (Promoter),Extremely reliable and scalable,,Netherlands,"No, telemetry is not enabled on any clusters",I do not believe that the Ceph community should be collecting any of this information,,,"No, I didn't realize they existed",,15,559,981,327,"Octopus (15.x),Luminous (12.x),Nautilus (14.x)","Upstream packages,Distribution packages,Vendor packages","Ubuntu,Debian",No,Longer,I delay upgrades because of the effort involved,Longer,"I delay upgrades due to the effort required to upgrade,Other (Please specify)",iostat,Supermicro,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),Mix of yes and no,"10 Gb/s Ethernet,RJ45 (Copper),AOC / Fiber",IPv6 only,"M.2,U.2 (2.5-inch)",No preference / beats me,,10 to 100,Both Filestore and BlueStore,Replication (3x),Proxmox,Yes,Yes,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,"CDN,Backup",2,0,"S3,RGW admin API","Amazon SDK,Boto,s3cmd / s3tools",RGW (built-in),HAproxy,1,Use case: CDN,No,,,,,,,,,Production,"Archive Storage,Other (Please specify) - webservers",Linux kernel CephFS mount,6 - 50,32G - 64G,3,Yes,No,No,Multiple File Systems within a Ceph Cluster,0 (Detractor),No (Please specify) - I don't use it.,0,0,0,0,0,0,0,0,0,0,0,?,"Proxmox,Nagios/icinga,Other (Please specify) - LibreNMS","ceph-deploy,Other (Please specify) - Proxmox",
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Commercial,5,2,0,0,4,0,1,,5,3,4,1,0,3,2,4,IRC / Slack / etc,9 (Promoter),robust and works well for vm storage,more proxmox integration easier maintenance better balancing,France,"Yes, telemetry is enabled on some of my clusters","My cluster(s) are on a protected network that does not have access to the internet,Enabling telemetry would require review by my organization's security team and I have chosen not request a review",,I left 'ident' disabled (the default),"No, I didn't realize they existed",,4,60,144,48,"Luminous (12.x),Nautilus (14.x)","Upstream packages,Distribution packages","Debian,Other OS","Yes, for performance reasons",Longer,"I delay upgrades because of the effort involved,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to the effort required to upgrade,Other (Please specify) - proxmox integration",I don't know / default set,Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe,MicroSD or other low-end flash","LVM (ceph-volume),Partitions,At-rest encryption (dmcrypt),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Yes,"10 Gb/s Ethernet,RJ45 (Copper),DAC (Copper)",IPv4 only,U.2 (2.5-inch),No preference / beats me,,10 to 100,Both Filestore and BlueStore,Replication (3x),"Kubernetes,Proxmox,rbd-nbd",Yes,Yes,Yes,Production,"Containers,Virtualization,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD,rbd-nbd",Yes,Staging,"CDN,Backup",3,1,S3,"Boto3,custom / in-house",RGW (built-in),None,0,Use case:cdn,No,,,,,,,Kafka,pulsar,Production,"Scratch,Containers,Home Directories","Linux kernel CephFS mount,ceph-fuse",6 - 50,1G,2,Unknown,No,No,Multiple File Systems within a Ceph Cluster,2 (Detractor),No (Please specify) - I do not use the dashboard regularly,1,1,1,0,0,0,0,0,0,0,0,-,Proxmox,"Puppet,ceph-deploy,Proxmox",
Between 2-5 years,"Open source,Data durability / reliability / integrity,Other (Please specify) - Fun :)","Personal,Other (Please specify) - homelab",5,5,0,0,5,0,0,,5,4,3,5,,4,4,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",10 (Promoter),,,Norway,"Yes, telemetry is enabled on some of my clusters",I haven't gotten around to it yet,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,28,28,8,Nautilus (14.x),Upstream packages,Debian,No,Within a week after release,"Other (Please specify) - this is the ""test/lab"" cluster",Within half a year of release,Other (Please specify) - this is the test/lab cluster,"balancer,crash,devicehealth,diskprediction,pg_autoscaler",Supermicro,Intel Xeon,"HDD (SATA, SAS)",LVM (ceph-volume),No,10 Gb/s Ethernet,IPv6 only,,No preference / beats me,,10 to 100,BlueStore,"Replication (3x),Erasure coding",KVM / QEMU,Yes,No,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,Production,"Video,Home Directories,Archive Storage","Linux kernel CephFS mount,libcephfs,NFS (Kernel NFS server),CIFS (SMB, Samba)",6 - 50,2G - 3G,1,No,No,No,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,5,4,5,0,0,0,0,0,0,0,0,have not started using the dashboard much yet.,"Ceph Dashboard,Other (Please specify) - proxmox built in",Other (Please specify) - proxmox,
Between 2-5 years,"Open source,High availability,Data durability / reliability / integrity",Commercial,5,4,0,0,5,1,1,,5,3,3,5,5,4,2,4,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",8 (Passive),experience ceph is not allways the answer it depends on workload,https://tracker.ceph.com/issues/1680 https://tracker.ceph.com/issues/24464,Norway,"Yes, telemetry is enabled on some of my clusters","I haven't gotten around to it yet,My cluster(s) are on a protected network that does not have access to the internet","basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted)",I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,50,50,20,Nautilus (14.x),Vendor packages,Debian,No,Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features","balancer,pg_autoscaler","Dell,HPE (Hewlett Packard),Supermicro","Intel Xeon,AMD (other)","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",No,"10 Gb/s Ethernet,DAC (Copper)",IPv6 only,,No preference / beats me,,10 to 100,BlueStore,"Replication (3x),Erasure coding","Kubernetes,Proxmox,KVM / QEMU,KRBD directly on Linux systems,rbd-nbd",Yes,Yes,Yes,Production,"Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",No,Production,"Log,Cloud,Containers,Archive Storage,Backup",3,1,S3,"s3cmd / s3tools,custom / in-house",RGW (built-in),HAproxy,0,Use case: N/A,No,,,,,,,,,Production,"Scratch,Log,Cloud,Containers,Virtualization,Home Directories,Archive Storage","Linux kernel CephFS mount,NFS (Kernel NFS server),CIFS (SMB, Samba)",1 - 5,4G - 15G,2,No,No,No,Inline Data,10 (Promoter),Yes,5,4,2,2,2,2,0,0,0,0,5,per class usage,"Ceph Dashboard,Proxmox",Proxmox,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,Security,High availability","Commercial,Academic",5,4,0,0,3,0,3,,5,4,3,4,3,4,4,4,"IRC / Slack / etc,Reporting issues via the bug tracker",7 (Passive),,,Belgium,"Yes, telemetry is enabled on some of my clusters",My cluster(s) are running a Ceph version older than Luminous,,I left 'ident' disabled (the default),Yes,"Out of curiosity,Other",4,54,216,54,"Octopus (15.x),Nautilus (14.x)","Upstream packages,Distribution packages",Debian,No,Longer,"I apply upgrades quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about regression",Within a year,I upgrade quickly to get any available bug fixes or security fixes,crash,Dell,"Intel Xeon,AMD Epyc","SSD (SATA, SAS),NVMe","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Yes,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),"Single,Dual",,10 to 100,BlueStore,Replication (3x),"Proxmox,None",Yes,No,Yes,Production,Virtualization,"No, it is not needed",Linux kernel RBD,Yes,,,,,,,,,,,,,,,,,,,,Production,"Cloud,Other (Please specify) - Websites",Linux kernel CephFS mount,1 - 5,16G - 31G,2,No,No,Yes,"Inline Data,Multiple File Systems within a Ceph Cluster",8 (Passive),Yes,5,4,5,5,4,0,0,0,5,0,4,-,"Ceph Dashboard,Proxmox,Prometheus","cephadm,Proxmox",
Between 1-2 years,"Open source,Scalability,Cost,High availability",Commercial,5,0,0,0,0,0,0,,2,5,3,3,2,3,2,5,Reporting issues via the bug tracker,7 (Passive),,,Germany,"No, telemetry is not enabled on any clusters",Enabling telemetry would require review by my organization's security team and I have chosen not request a review,,,"No, I didn't realize they existed",,2,393,715,238,"Octopus (15.x),Nautilus (14.x)",Vendor packages,RHEL / CentOS / Fedora,No,Longer,I only upgrade as needed to address issues,Never (clusters remain on major release they were deployed with),I delay upgrades due to the effort required to upgrade,"devicehealth,iostat,Other (Please specify) - dashboard",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe",LVM (ceph-volume),Yes,"25 Gb/s Ethernet,100 Gb/s Ethernet,DAC (Copper),AOC / Fiber",IPv4 only,"U.2 (2.5-inch),Add In Cards",Dual,to keep a node running even if one cpu fails,100 to 500,BlueStore,"Replication (2x),Replication (3x),Erasure coding","OpenStack,Kubernetes,KVM / QEMU,Other (Please specify) - everything that speaks S3",Yes,Yes,No,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),iSCSI (tcmu-runner)",Yes,Proof of Concept (PoC),"Archive Storage,Backup",9,2,S3,custom / in-house,RGW (built-in),"HAproxy,DNS round-robin",0,Use case:,No,,,,,,,,,,,,,,,,,,,10 (Promoter),Yes,4,1,4,2,4,4,4,0,0,4,0,non specific,"Ceph Dashboard,Grafana (custom),node_exporter","Ansible (ceph-ansible),cephadm",
Less than 1 year,Open source,Commercial,,,,,,,,,,,,,,,,,,10 (Promoter),,,Zimbabwe,"Yes, telemetry is enabled on all of my clusters",,"device: anonymized device health metrics (e.g., SMART hard disk metrics)",I enabled 'ident' and configured a cluster description and/or email address,Yes,Out of curiosity,1,1,1,1,Octopus (15.x),Upstream packages,Ubuntu,"Yes, in front of EC pools for functionality",Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a month of release,I upgrade quickly to get new features,balancer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Less than 1 year,Open source,Commercial,,,,,,,,,,,,,,,,,,10 (Promoter),,,Afghanistan,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,Yes,Out of curiosity,1,1,1,1,Octopus (15.x),Upstream packages,Ubuntu,No,Within a week after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within half a year of release,I upgrade quickly to get new features,balancer,IBM,ARM,"HDD (SATA, SAS)",LVM (ceph-volume),Yes,100 Mb/s Ethernet,IPv4 only,M.2,Single,,10 or fewer,"Filestore (XFS, EXT4, BTRFS)",Replication (2x),OpenStack,Yes,Yes,Yes,Development / Testing,Scratch,"Yes, for DR","librbd (e.g., in combination with qemu)",No,Development / Testing,Scratch,1,1,S3,Amazon SDK,RGW (built-in),HAproxy,1,Use case:,,,,,,,,,,Development / Testing,Scratch,Linux kernel CephFS mount,1 - 5,1G,1,Yes,Yes,Yes,Inline Data,10 (Promoter),Yes,,,,,,,,,,,,test,Ceph Dashboard,Ansible (ceph-ansible),
Less than 1 year,Open source,Commercial,,,,,,,,,,,,,,,,,,10 (Promoter),,,Afghanistan,"Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted)",I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,1,1,1,Octopus (15.x),Upstream packages,Ubuntu,"Yes, in front of EC pools for functionality",Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within half a year of release,I upgrade quickly to get new features,,IBM,ARM,"HDD (SATA, SAS)",LVM (ceph-volume),No,100 Mb/s Ethernet,IPv6 only,M.2,Single,,10 or fewer,"Filestore (XFS, EXT4, BTRFS)",Replication (2x),OpenStack,No,No,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10 (Promoter),No (Please specify),,,,,,,,,,,,1,Ceph Dashboard,Ansible (ceph-ansible),
More than 5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity,Performance",Commercial,4,2,0,0,0,0,0,,5,2,5,2,2,5,5,5,,10 (Promoter),,,Netherlands,"Yes, telemetry is enabled on some of my clusters",My cluster(s) are running a Ceph version older than Luminous,,I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,4,150,390,130,"Octopus (15.x),Jewel (10.x)",Upstream packages,RHEL / CentOS / Fedora,No,Longer,I delay upgrades due to concerns about regression,Longer,I delay upgrades due to the effort required to upgrade,"balancer,devicehealth,iostat,pg_autoscaler,restful","HPE (Hewlett Packard),We build our own",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Partitions",Yes,"10 Gb/s Ethernet,40 Gb/s Ethernet,DAC (Copper),AOC / Fiber",IPv4 only,U.2 (2.5-inch),Quad,,100 to 500,"Filestore (XFS, EXT4, BTRFS),BlueStore","Replication (3x),Erasure coding",KVM / QEMU,Yes,Yes,No,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,Production,"CDN,Containers",2,10,"S3,Swift",custom / in-house,RGW (built-in),HAproxy,0,Use case:,No,,,,,,,,,,,,,,,,,,,7 (Passive),Yes,3,0,0,0,0,0,0,0,0,0,0,None,"Prometheus,Grafana (custom),node_exporter",ceph-deploy,
More than 5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity,Performance",Commercial,4,2,,,,,4,,3,3,4,3,3,4,3,4,"Reporting issues via the bug tracker,Other (Please specify) - architecture",10 (Promoter),,Block: Contiguous block devices (crush bucket adjacent primaries per volume) File: Direct reads for EC (in lieu of primary mediation) More robust RDMA in async messenger,United States,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I knew they existed by have not used them",,1,13440,13440,9575,Nautilus (14.x),Distribution packages,RHEL / CentOS / Fedora,No,Within a week after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a month of release,I upgrade quickly to get new features,,Prefer not to say,AMD Epyc,"HDD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Yes,100 Gb/s Ethernet,IPv4 only,"U.2 (2.5-inch),Add In Cards",Dual,Density,500 to 1000,BlueStore,Erasure coding,OpenShift,No,Yes,No,,,,,,Proof of Concept (PoC),"Big Data & Analytics,HPC,Other (Please specify) - Machine Learning",80,1,S3,"Boto3,Other (Please specify) - S3A",RGW (built-in),"HAproxy,Other (Please specify) - Routing",0,Use case: maybe,No,,,,,,,Kafka,Redis,,,,,,,,,,,5 (Detractor),No (Please specify) - most familiar / fastest with cli in general,0,0,0,0,0,0,0,0,0,0,0,Don't use it really.,"Grafana (custom),node_exporter,ceph_exporter","Ansible (ceph-ansible),Rook,Ansible (other / homebrew)",
Between 2-5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity",Personal,5,3,0,0,0,0,0,,4,2,4,5,1,4,2,3,Reporting issues via the bug tracker,10 (Promoter),,,Slovakia,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,44,44,44,Nautilus (14.x),Distribution packages,Other OS,No,Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a year,"I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features",pg_autoscaler,Supermicro,Intel Xeon,"HDD (SATA, SAS)","All-in-one OSD (default, colocated, no separate metadata device)",No,10 Gb/s Ethernet,IPv4 only,,Dual,Performance reasons as using older hardware,10 or fewer,BlueStore,Replication (3x),"KVM / QEMU,KRBD directly on Linux systems",Yes,No,Yes,Production,Virtualization,Other (Please specify) - Only running one cluster,"librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,Production,"Home Directories,Archive Storage,Other (Please specify) - Mail server storage","Linux kernel CephFS mount,CIFS (SMB, Samba)",6 - 50,2G - 3G,1,No,No,No,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,5,0,0,4,3,5,0,0,3,0,3,"Probably adding an OSD, but using Gentoo where the process of creating and adding one to start during the boot isn't the most straight forward one, thus probably hard to implement in GUI.","Ceph Dashboard,Prometheus,Other (Please specify) - Nagios CEPH plugin to export CEPH status via SNMP, Grafana dashboards provided in the CEPH repository","Other (Please specify) - ceph-authtool, ceph-volume, ceph-mon",
Less than 1 year,"Scalability,Cost,Data durability / reliability / integrity",Commercial,5,5,0,0,3,0,0,Telegram,1,5,1,2,2,5,4,5,"IRC / Slack / etc,Mailing list",6 (Detractor),,,Russian Federation,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations",,,"No, I knew they existed by have not used them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Less than 1 year,"Open source,Scalability,Cost,Data durability / reliability / integrity",Non-profit,5,3,1,0,0,0,2,,3,4,3,3,3,3,5,4,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),,"Better documentation and bugfixes around the configuration and use of the Grafana, Prometehus, alert and monitoring functionality in cephadm based installations. A bare metal installation with cephadm similar to oVirt:s solution would be nice",Sweden,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,Yes,Out of curiosity,2,150,160,50,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy","balancer,crash,pg_autoscaler",We build our own,Intel (other),"HDD (SATA, SAS),NVMe","All-in-one OSD (default, colocated, no separate metadata device)",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet,RJ45 (Copper),DAC (Copper),AOC / Fiber",IPv4 only,M.2,Single,I want more nodes with fewer OSD:s on each and have to stay within a fairly small budget,10 to 100,BlueStore,Replication (3x),"Kubernetes,Proxmox",Yes,No,Yes,Staging,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,Production,"Log,Video,Virtualization,Archive Storage","Linux kernel CephFS mount,CIFS (SMB, Samba)",6 - 50,4G - 15G,2,No,Yes,No,,10 (Promoter),Yes,5,1,4,3,3,1,0,0,1,1,5,Better documentation on how to setup certificates and hostnames for grafana and prometheus,"Ceph Dashboard,Proxmox","cephadm,Proxmox",
Less than 1 year,"Open source,Scalability,Feature set,Security,High availability,Data durability / reliability / integrity",Government,5,3,0,0,0,0,0,,4,3,3,5,3,4,3,5,Mailing list,9 (Promoter),Ceph is an exciting product with many faces. How could I miss the oportunity to share my excitement :),multi-site bucket sharding GUI for radosgw-admin synchronous pool replication between clusters,France,"No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I knew they existed by have not used them",,2,420,750,380,Nautilus (14.x),Distribution packages,RHEL / CentOS / Fedora,No,Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a year,I delay upgrades due to concerns about stability,"balancer,devicehealth,pg_autoscaler,restful",HPE (Hewlett Packard),Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),Yes,"10 Gb/s Ethernet,AOC / Fiber",IPv4 only,U.2 (2.5-inch),Dual,no single socket available on our product list,10 to 100,BlueStore,"Replication (3x),Erasure coding","OpenStack,rbd-nbd",Yes,Yes,No,Production,"Big Data & Analytics,Cloud,Containers","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,Production,"Cloud,Archive Storage,Backup",2,25,"S3,Swift,RGW admin API,NFS re-export of S3 buckets",custom / in-house,RGW (built-in),"Dedicated custom hardware (Alteon, F5, etc)",4,Use case:,Yes,2,No,1,Global,,,,,,,,,,,,,,,9 (Promoter),Yes,5,4,2,3,4,4,1,5,2,5,4,S3 Multi-realm management integration,"Ceph Dashboard,Prometheus,Grafana (custom),Zabbix",Other (Please specify) - Manual managment,
Between 1-2 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Commercial,5,4,0,0,0,0,1,,5,4,3,4,5,5,4,3,Reporting issues via the bug tracker,10 (Promoter),,,Germany,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,70,70,17,Octopus (15.x),Distribution packages,Ubuntu,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a month of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy","balancer,crash,devicehealth,iostat,pg_autoscaler",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Yes,"10 Gb/s Ethernet,RJ45 (Copper),DAC (Copper),AOC / Fiber",IPv4 only,M.2,Dual,,10 to 100,BlueStore,Replication (3x),"KVM / QEMU,KRBD directly on Linux systems",Yes,No,Yes,Production,"Virtualization,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,Production,Archive Storage,ceph-fuse,1 - 5,2G - 3G,1,No,Yes,No,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,5,3,4,5,5,5,0,0,5,0,1,none,"Ceph Dashboard,Nagios/icinga",cephadm,
Less than 1 year,"Open source,Scalability,Feature set,High availability",Academic,5,3,0,0,0,0,0,,5,5,5,5,5,5,5,5,Mailing list,9 (Promoter),,The dashboard should tell the ceph orch command which does the same like the chosen operation in the graphicel interface.  Better balancing for nodes with different sized OSD.  Make more clear which SSD are used as DB or WAL device.,Germany,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,"No, I didn't realize they existed",,1,500,500,160,Octopus (15.x),Distribution packages,Ubuntu,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within half a year of release,"I upgrade quickly to get new features,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability",I don't know / default set,Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe",Defaults / I don't know,Yes,10 Gb/s Ethernet,IPv4 only,,Dual,Best in performance and costs relation,10 to 100,BlueStore,Replication (3x),KVM / QEMU,Yes,No,Yes,Production,"Scratch,Virtualization,Archive Storage",Other (Please specify) - Too expensive for our university,"librbd (e.g., in combination with qemu)",No,,,,,,,,,,,,,,,,,,,,Staging,"Big Data & Analytics,Home Directories","Linux kernel CephFS mount,NFS (nfs-ganesha),NFS (Kernel NFS server),CIFS (SMB, Samba)",51 - 100,4G - 15G,1,Unknown,No,Yes,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,5,1,3,3,1,3,0,3,3,0,5,List of pools is way too slow Give us the equivalent ceph orch command,Ceph Dashboard,"Puppet,cephadm",
More than 5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity",Commercial,5,4,0,0,2,0,1,,5,1,1,3,,4,1,1,"IRC / Slack / etc,Mailing list",8 (Passive),,,France,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Academic,5,4,3,0,0,0,0,,5,4,5,4,3,5,4,4,Mailing list,8 (Passive),,"zero downtime during upgrade, better tool hand off to other services(mon,osd) Better performance(latency and throughput)",United States,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,5,64,150,50,"Octopus (15.x),Nautilus (14.x)",Distribution packages,RHEL / CentOS / Fedora,No,Longer,I delay upgrades because of the effort involved,Within a year,Other (Please specify) - Ceph is an integral part of our cloud solution.  We upgrade major versions annually.,"balancer,iostat,pg_autoscaler",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe",LVM (ceph-volume),No,10 Gb/s Ethernet,IPv4 only,M.2,Single,Intel does a lot of routing between sockets.  Single CPU with 20+ cores exist removing need for dual socket,10 to 100,BlueStore,Replication (3x),KVM / QEMU,Yes,No,No,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9 (Promoter),Yes,2,0,2,2,1,1,0,0,0,0,0,Get more detail out of the cluster.  i.e. Tell me exactly where a rbd image is.  In the event of an emergency situation,"Ceph Dashboard,Prometheus,Ceph-metrics",Other (Please specify) - Home made scripts,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Academic,4,3,0,3,0,3,1,,2,4,4,3,2,4,3,5,"Mailing list,Reporting issues via the bug tracker",9 (Promoter),,,Sweden,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,Yes,Out of curiosity,1,2000,2100,1100,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy",I don't know / default set,Supermicro,Intel Xeon,"HDD (SATA, SAS),NVMe",LVM (ceph-volume),Yes,25 Gb/s Ethernet,IPv4 only,Add In Cards,Dual,,100 to 500,BlueStore,"Replication (3x),Erasure coding","OpenStack,KRBD directly on Linux systems",Yes,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Feature set,High availability,Integration with adjacent technologies",Commercial,5,0,5,0,0,0,0,,5,5,4,5,5,4,4,4,IRC / Slack / etc,6 (Detractor),,,Germany,"No, telemetry is not enabled on any clusters",I do not believe that the Ceph community should be collecting any of this information,,,"No, I didn't realize they existed",,5,660,33000,16000,Octopus (15.x),Upstream packages,"Ubuntu,Debian",No,Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within half a year of release,I delay upgrades due to concerns about stability,I don't know / default set,Supermicro,Intel Xeon,"HDD (SATA, SAS)","All-in-one OSD (default, colocated, no separate metadata device)",No,25 Gb/s Ethernet,IPv4 only,,No preference / beats me,,10 to 100,BlueStore,Erasure coding,Kubernetes,No,Yes,No,,,,,,"Development / Testing,Production","Log,Cloud,Containers,Archive Storage,Backup",3,0,S3,"Amazon SDK,Other (Please specify) - minio",RGW (built-in),Nginx,0,Use case:,No,,,,,,,,,,,,,,,,,,,6 (Detractor),No (Please specify) - using rook,3,0,0,0,0,0,0,0,0,0,0,?,Prometheus,Rook,
More than 5 years,"Open source,Feature set,High availability,Data durability / reliability / integrity","Academic,Non-profit",5,5,2,1,1,2,0,,5,5,3,3,3,3,5,4,Mailing list,10 (Promoter),,,Austria,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,2,150,250,100,"Luminous (12.x),Nautilus (14.x)",Upstream packages,"Ubuntu,Debian",No,Longer,"I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Within half a year of release,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability","devicehealth,iostat",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,"M.2,U.2 (2.5-inch)",Dual,,10 to 100,BlueStore,"Replication (3x),Erasure coding","Kubernetes,Proxmox,KVM / QEMU,KRBD directly on Linux systems",Yes,Yes,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Staging,"Log,Big Data & Analytics",2,5,S3,None,RGW (built-in),"Dedicated custom hardware (Alteon, F5, etc)",0,Use case:,No,,,,,,,,,Development / Testing,Containers,Linux kernel CephFS mount,6 - 50,4G - 15G,2,No,No,Yes,,8 (Passive),Yes,4,3,3,3,3,3,0,0,3,3,3,none,"croit,Prometheus,Grafana (custom)","Ansible (ceph-ansible),Croit",
Between 2-5 years,"Scalability,Cost,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Academic,5,4,1,0,0,0,0,,5,4,4,3,4,4,4,5,Mailing list,9 (Promoter),Ceph covers many aspects of storage demands,unified deployment load balancer understandable logs,France,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,Yes,Out of curiosity,3,7600,8600,5500,"Octopus (15.x),Nautilus (14.x)",Upstream packages,RHEL / CentOS / Fedora,No,Longer,"I apply upgrades quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about regression",Longer,I only upgrade as needed to address specific bugs or needed features,"balancer,iostat,pg_autoscaler,Other (Please specify) - prometheus",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Mix of yes and no,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,,100 to 500,BlueStore,"Replication (3x),Erasure coding","OpenStack,Kubernetes,OpenShift",Yes,Yes,Yes,Production,Cloud,"No, it is not needed","librbd (e.g., in combination with qemu)",No,Production,"Big Data & Analytics,Backup",1,10000,"S3,Swift","Boto3,custom / in-house",RGW (built-in),None,0,Use case:no,No,,,,,,,,,Production,"Scratch,Big Data & Analytics,HPC,Home Directories",Linux kernel CephFS mount,101 - 500,16G - 31G,2,No,No,Yes,"Mantle (Programmable Metadata Balancer),Multiple File Systems within a Ceph Cluster",8 (Passive),Yes,5,3,2,1,1,1,0,0,2,2,0,logs details,"Ceph Dashboard,Prometheus,Grafana (custom)",Ansible (ceph-ansible),
More than 5 years,"Open source,Scalability,High availability",Commercial,5,4,0,0,1,0,2,,5,3,4,4,2,5,4,3,,6 (Detractor),Too long outages to host VM. Network problems are too long to detect.,Tuning tips. Uptodate benchmarks. Simplier backup to external Ceph.,France,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,3,400,600,200,Luminous (12.x),"Upstream packages,Distribution packages",Debian,No,Longer,I only upgrade as needed to address issues,Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability,Other (Please specify) - Compatibility with distribution + Xen hypervisor",I don't know / default set,"Supermicro,Other (Please specify) - OVH (mainly supermicro)","Intel Xeon,Other (Please specify) - AMD Epyc for kernel RBD client","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","Partitions,lvmcache,Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Mix of yes and no,10 Gb/s Ethernet,IPv4 only,,Single,Cost.,100 to 500,"BlueStore,Both Filestore and BlueStore",Replication (3x),"Xen,Other (Please specify) - CephFS directly on Linux Systems",Yes,Yes,Yes,Production,"Cloud,Virtualization",Other (Please specify) - What is it?,Linux kernel RBD,Yes,Production,CDN,1,0,"Swift,RGW admin API",Other (Please specify) - Symfony,RGW (built-in),"Nginx,Other (Please specify) - CloudFlare",0,Use case:,No,,,,,,,,,Production,Other (Please specify) - share temp files between servers,Linux kernel CephFS mount,1 - 5,1G,3,No,No,No,,5 (Detractor),No (Please specify) - I never used nor saw this dashboard,0,0,0,0,0,0,0,0,0,0,0,Don't know this dashboard.,"Prometheus,Grafana (custom),Other (Please specify) - Netdata + home made scripts",Other (Please specify) - Bcfg2 + home made scripts,
Between 1-2 years,"Open source,Scalability,Data durability / reliability / integrity,Integration with adjacent technologies",Other (Please specify) - Professional,4,3,3,1,1,1,2,,4,5,5,5,5,5,5,3,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",10 (Promoter),,,United States,"Yes, telemetry is enabled on some of my clusters",I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,,I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,3,1024,4098,1400,Nautilus (14.x),Distribution packages,Ubuntu,"Yes, in front of EC pools for functionality",Longer,I delay upgrades because there are too many updates,Within a year,I delay upgrades due to concerns about stability,"balancer,crash,devicehealth,iostat,pg_autoscaler,restful,Other (Please specify) - rbd_support, volumes,prometheus","Dell,HPE (Hewlett Packard)",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),bcache,Containerized daemons",Yes,"40 Gb/s Ethernet,100 Gb/s Ethernet",IPv4 only,U.2 (2.5-inch),Dual,,100 to 500,BlueStore,"Replication (2x),Replication (3x),Erasure coding","OpenStack,Kubernetes",Yes,Yes,No,"Staging,Proof of Concept (PoC)","Cloud,Containers","Yes, for DR","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,"Staging,Proof of Concept (PoC)","Video,Cloud,Archive Storage,Backup",3,20,"S3,Swift","Boto3,s3cmd / s3tools",Keystone,HAproxy,2,Use case: improve latency and prefetching,Yes,1,No,2,Global,ElasticSearch Sync Module,,Http,Https,,,,,,,,,,,8 (Passive),No (Please specify) - Dont use dashboard due to the distro,1,1,1,1,1,1,0,0,0,1,3,"Inaccuracy in grafana dashboards, especially with mirroring enabled. Many rbd level stats are not reported","Prometheus,Telegraf",Juju,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Commercial,5,5,0,1,0,0,4,Friends,3,1,3,4,3,4,5,5,"Mailing list,Ceph events / conferences",10 (Promoter),"Scalability, performance, community, open-source, reliability, features set.","S3 buckets metrics(I/O stats, etc..) RBD images size metrics Dashboard possibility use external prometheus, grafana and alertmanager for multi clusters","Lithuania,Netherlands,United Kingdom,United States","No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I knew they existed by have not used them",,10,2000,6000,2000,"Luminous (12.x),Nautilus (14.x)",Upstream packages,RHEL / CentOS / Fedora,No,Longer,"I apply upgrades quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about new functionality,I delay upgrades because there are too many updates,I only upgrade as needed to address issues",Within half a year of release,"I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,diskprediction,iostat",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,5,3,1,4,1,2,1,,5,4,4,3,4,4,4,4,"Reporting issues via the bug tracker,Ceph events / conferences",8 (Passive),,,"Finland,Germany","No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations",,,"No, I didn't realize they existed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Personal,5,2,0,0,0,0,0,,,,,,,,,,,2 (Detractor),"It's fun & edgy to have a ""scale out"" (as EMC pitched their Isilon clusters at some stage) personal NAS, and to play the ""no downtime"" rolling maintenance / upgrade game, but something much simpler (openmediavault?) would work just as well for home use.",,France,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,Enabling telemetry would require review by my organization's security team and I have chosen not request a review",,,"No, I didn't realize they existed",,1,22,22,11,Nautilus (14.x),Distribution packages,Debian,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Longer,"I delay upgrades due to the effort required to upgrade,Other (Please specify) - Migration from Luminous on Debian has been a pain due to lack of packages. Took a covid lockdown to have time to rehearse what has in fact been a rolling reinstall of all OSDs.",I don't know / default set,Other (Please specify) - InWin MS04 + ASRock J3455-ITX,Intel (other),"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",No,"1000 Mb/s Ethernet (GigE, gigabit),RJ45 (Copper)",IPv4 only,,Single,Only type available at mainstream prices.,10 or fewer,BlueStore,"Replication (2x),Replication (3x)",Other (Please specify) - Direct cephfs mount on Linux clients,No,No,Yes,,,,,,,,,,,,,,,,,,,,,,,,,Production,"Video,Cloud,Home Directories,Archive Storage",Linux kernel CephFS mount,1 - 5,1G,1,Unknown,No,No,,0 (Detractor),No (Please specify) - Is there a dashboard somewhere?,0,0,0,0,0,0,0,0,0,0,0,Show / edit configuration as CLI does not quite cut it... ceph> config dump b'WHO MASK LEVEL OPTION VALUE RO \n' ceph>,Collectd,"Other (Please specify) - by hand, but it's a pain",
More than 5 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Other (Please specify) - IT Consulting,5,4,1,4,1,1,5,,5,3,5,3,5,4,5,3,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Contributing code,Ceph events / conferences,Member of the Ceph Foundation",10 (Promoter),,Improve flash performance Deduplication Async replication,"Canada,Denmark,France,Germany,Hungary,Netherlands,Poland,Singapore,South Africa,United Kingdom","Yes, telemetry is enabled on some of my clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,I enabled 'ident' and configured a cluster description and/or email address,Yes,To inform my decisions about when or whether to upgrade,20,8000,30000,10000,"Octopus (15.x),Master,Nautilus (14.x)","Upstream packages,Distribution packages,We build our own packages,We built a custom version","Ubuntu,Debian,RHEL / CentOS / Fedora,SUSE",No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to concerns about performance regressions,I upgrade quickly to get any available bug fixes or security fixes,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,iostat,restful","Lenovo,Dell,HPE (Hewlett Packard),Supermicro,Intel","ARM,Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device),dm-cache",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet,RJ45 (Copper),AOC / Fiber",Dual stack,"M.2,U.2 (2.5-inch)",Single,,1000 to 3000,BlueStore,"Replication (3x),Erasure coding","OpenStack,Cloudstack,OpenNebula,Kubernetes,VMware,Proxmox,KVM / QEMU",Yes,Yes,Yes,Production,"Build,Big Data & Analytics,Cloud,Containers,Virtualization","No, it is incompatible with my performance requirements","librbd (e.g., in combination with qemu),Linux kernel RBD,iSCSI (tcmu-runner),iSCSI (LIO with /dev/rbd)",Yes,Production,"Build,Log,Video,Big Data & Analytics,Cloud,Containers,Virtualization,Archive Storage,Backup",5,500,S3,"Boto3,s3cmd / s3tools","RGW (built-in),Keystone,LDAP","HAproxy,Nginx,DNS round-robin",0,Use case:,No,,,,,,,"Kafka,Http",,Production,"Cloud,Containers,Virtualization,Home Directories","Linux kernel CephFS mount,ceph-fuse,libcephfs,NFS (nfs-ganesha)",51 - 100,32G - 64G,3,Yes,Yes,No,,5 (Detractor),No (Please specify),0,0,0,0,0,0,0,0,0,0,0,Integration into Others systems,"Prometheus,Grafana (custom),node_exporter,ceph_exporter","Ansible (ceph-ansible),Salt / DeepSea,ceph-deploy,Rook,cephadm",
Between 2-5 years,"Open source,Cost,Data durability / reliability / integrity",Commercial,4,2,4,0,1,0,2,,5,5,3,4,1,2,5,5,,4 (Detractor),"Ceph is really hard to operate and has very low visibility in to its inner workings. In addition, many error messages are obtuse and/or non-google-able.","better error messages, less obtuse operability.",United States,"No, telemetry is not enabled on any clusters","My organization's security policy does not allow storage systems to communicate with vendors or external organizations,Enabling telemetry would require review by my organization's security team and I have chosen not request a review,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,I do not believe that the Ceph community should be collecting any of this information",,,"No, I didn't realize they existed",,7,55,250,90,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Never,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues,Other (Please specify) - We delay upgrades because we've lost entire clusters. Rather leave the cluster how it is if it's working.",Never (clusters remain on major release they were deployed with),"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability,Other (Please specify) - We delay upgrades because we've lost entire clusters. Rather leave the cluster how it is if it's working.",,Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Containerized daemons",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,,Dual,,10 to 100,"Filestore (XFS, EXT4, BTRFS)",Replication (3x),Kubernetes,No,Yes,No,,,,,,"Staging,Production","Cloud,Containers",100,0,S3,"Boto,Boto3,s3cmd / s3tools",RGW (built-in),Nginx,0,Use case:,No,,,,,,,,,,,,,,,,,,,6 (Detractor),Yes,4,3,3,5,4,0,0,0,0,5,4,.,Prometheus,"Rook,Other (Please specify) - helm",
Between 1-2 years,"Open source,Scalability,Cost",Commercial,5,3,0,0,0,0,1,Telegram Group,4,1,3,2,2,3,4,5,"IRC / Slack / etc,Mailing list",7 (Passive),,,Russian Federation,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,Enabling telemetry would require review by my organization's security team and I have chosen not request a review",,,"No, I didn't realize they existed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Scalability,Cost,Integration with adjacent technologies",Commercial,3,2,1,1,1,4,1,google,2,3,2,1,2,5,5,3,"IRC / Slack / etc,Reporting issues via the bug tracker",5 (Detractor),just one reason to use ceph - you already know why you need it,normal maintenance mode without affect on clients dynamic bucket resharding for multisite clear conclusion about the status of asynchronous replication,Russian Federation,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations",,,"No, I knew they existed by have not used them",,30,6600,100000,35000,"Luminous (12.x),Nautilus (14.x)",Vendor packages,RHEL / CentOS / Fedora,No,Longer,I delay upgrades due to concerns about regression,Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability",restful,"Lenovo,Dell,HPE (Hewlett Packard),Huawei",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet",IPv4 only,U.2 (2.5-inch),Dual,cost,1000 to 3000,Both Filestore and BlueStore,"Replication (3x),Erasure coding","OpenStack,OpenShift",Yes,Yes,No,"Development / Testing,Production","Big Data & Analytics,Containers,Virtualization","No, it is incompatible with my performance requirements","librbd (e.g., in combination with qemu)",Yes,"Development / Testing,Production","Log,Video,AI/ML,Cloud,Containers,Archive Storage,Backup",2,1048576,"S3,RGW admin API","Amazon SDK,Boto3,s3cmd / s3tools",RGW (built-in),HAproxy,2,no,Yes,1,No,2,Global,,,,,,,,,,,,,,,0 (Detractor),No (Please specify) - no,0,1,0,0,0,0,0,0,0,0,0,many clusters in one dashboard,"Grafana (custom),Telegraf",Ansible (ceph-ansible),
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,3,2,2,,,2,1,,2,1,2,1,2,5,1,3,IRC / Slack / etc,10 (Promoter),,,Estonia,"No, telemetry is not enabled on any clusters","Enabling telemetry would require review by my organization's security team and I have chosen not request a review,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,I do not believe that the Ceph community should be collecting any of this information",,,"No, I didn't realize they existed",,0,0,0,0,"Mimic (13.x),Nautilus (14.x)","Upstream packages,Vendor packages",RHEL / CentOS / Fedora,No,Longer,I apply upgrades quickly to get any available bug fixes or security fixes,Within a year,I delay upgrades due to concerns about stability,,"Lenovo,HPE (Hewlett Packard)",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Yes,10 Gb/s Ethernet,IPv4 only,,Dual,,100 to 500,"Filestore (XFS, EXT4, BTRFS),BlueStore","Replication (3x),Erasure coding","Proxmox,KVM / QEMU",Yes,Yes,No,Production,"Cloud,Virtualization","No, it is incompatible with my performance requirements","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,"Log,Video,CDN,Big Data & Analytics,AI/ML,Cloud,Home Directories,Archive Storage,Backup",6,10000,S3,"Amazon SDK,s3cmd / s3tools",RGW (built-in),HAproxy,0,Use case:no,No,,,,,,,,,,,,,,,,,,,3 (Detractor),No (Please specify) - cli faster,0,0,0,0,0,0,0,0,0,0,0,no need,Zabbix,"Ansible (ceph-ansible),Ansible (other / homebrew),Other (Please specify) - pure cli/bash is the best way",
Between 2-5 years,"Open source,Scalability",Commercial,5,1,5,1,1,0,1,,5,5,5,3,1,5,5,3,"IRC / Slack / etc,Reporting issues via the bug tracker,Contributing / enhancing documentation",7 (Passive),,"Rook-Ceph - Volume Snapshots and integrations with Restic to back up snapshot data to an external filestore (s3,nfs, etc) Rook-Ceph - Volume Snapshots and integrations with Restic to back up snapshot data to an external filestore (s3,nfs, etc) Rook-Ceph - Volume Snapshots and integrations with Restic to back up snapshot data to an external filestore (s3,nfs, etc)",United States,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,5,5,3,Octopus (15.x),Upstream packages,Ubuntu,No,Within a week after release,I apply upgrades quickly because they require little effort,Within a month of release,I upgrade quickly because it is easy,"crash,pg_autoscaler",Intel,"ARM,Intel Xeon,Intel (other)","SSD (SATA, SAS),NVMe","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Mix of yes and no,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,M.2,Single,Less hassle,10 or fewer,"Filestore (XFS, EXT4, BTRFS)",Replication (2x),Kubernetes,Yes,No,No,"Development / Testing,Staging",Containers,"Yes, for DR",Linux kernel RBD,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8 (Passive),Yes,5,0,5,5,5,5,0,0,0,0,0,"Configuring backups, snapshots and sending data to an external store for backup","Ceph Dashboard,Prometheus,Grafana (custom)",Rook,
Less than 1 year,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,4,0,5,0,0,0,2,,,5,,,5,,5,,IRC / Slack / etc,9 (Promoter),Ceph is very flexible and provide a lot of functionality out of the box without too much hassle,Monitoring / metrics out of the box Improved 'like for like' with S3 object store,United Kingdom,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,120,120,40,Octopus (15.x),Upstream packages,Ubuntu,No,Within a month after release,I delay upgrades because of the effort involved,Within half a year of release,I delay upgrades due to the effort required to upgrade,I don't know / default set,"Other (Please specify) - Unknown, Provided by hosting company",AMD Epyc,"HDD (SATA, SAS)",Containerized daemons,Yes,10 Gb/s Ethernet,IPv4 only,,No preference / beats me,,10 to 100,BlueStore,Replication (3x),"Kubernetes,Other (Please specify) - rook-ceph on kubernetes",Yes,No,No,Staging,Containers,"No, it is not needed",Linux kernel RBD,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9 (Promoter),Yes,5,3,3,3,3,3,0,0,0,0,0,in depth metrics / Grafana dashboard. Unsure how to set it up.,"Ceph Dashboard,Grafana (custom),node_exporter",Rook,
Between 2-5 years,"Open source,Cost,High availability,Other (Please specify) - works well with openstack",Commercial,5,0,0,0,0,0,3,googling, stackoverflow etc.,5,1,5,1,1,4,0,1,Other (Please specify) - sorry. not participate in any way,8 (Passive),for commercial use with low skilled admin team there is some reasons to choose ScaleIO instead of Ceph,"for all three - reliable, tested, production ready support for RDMA over Infiniband","Czech Republic,Poland,Ukraine","No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations",,,"No, I knew they existed by have not used them",,7,12000,56000,20000,Nautilus (14.x),We build our own packages,Debian,No,Never,"I delay upgrades due to concerns about new functionality,I only upgrade as needed to address issues",Never (clusters remain on major release they were deployed with),"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,pg_autoscaler","Dell,HPE (Hewlett Packard)",Intel Xeon,"SSD (SATA, SAS),NVMe",Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Yes,InfiniBand,IPv4 only,,Dual,there are no need to have more CPU for storage node,500 to 1000,BlueStore,Replication (3x),"OpenStack,KVM / QEMU",Yes,No,Yes,Production,"Cloud,Virtualization","Yes, for DR","librbd (e.g., in combination with qemu)",No,,,,,,,,,,,,,,,,,,,,Production,Other (Please specify) - backup data storage,"Linux kernel CephFS mount,NFS (Kernel NFS server)",101 - 500,65G - 127G,3,Unknown,Yes,No,,0 (Detractor),No (Please specify) - CLI is more preferred,0,0,0,0,0,0,0,0,0,0,0,"prefer CLI, not using dashboard at all",Prometheus,"Puppet,cephadm"
Between 2-5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity","Commercial,Government,Academic",4,5,0,0,0,0,3,,,4,4,,,5,3,,"Mailing list,Ceph events / conferences",10 (Promoter),,"user, group policies (S3) deduplication",Poland,"Yes, telemetry is enabled on some of my clusters","I haven't gotten around to it yet,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.",,,"No, I didn't realize they existed",,6,2000,8000,4200,"Mimic (13.x),Luminous (12.x),Nautilus (14.x)","Upstream packages,Distribution packages",Ubuntu,"Yes, for performance reasons",Longer,I delay upgrades due to concerns about regression,Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,crash,diskprediction,iostat,pg_autoscaler,restful","Dell,Supermicro",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Mix of yes and no,"10 Gb/s Ethernet,40 Gb/s Ethernet,DAC (Copper)",IPv4 only,,Dual,,100 to 500,BlueStore,"Replication (3x),Erasure coding","OpenStack,Proxmox",Yes,Yes,No,Production,"Cloud,Virtualization,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,"Staging,Production","Big Data & Analytics,Archive Storage,Backup",8,0,"S3,RGW admin API","Amazon SDK,Boto3,s3cmd / s3tools","RGW (built-in),Keystone,LDAP","HAproxy,Nginx,DNS round-robin",0,Use case:,No,,,,,,,,,,,,,,,,,,,8 (Passive),No (Please specify) - I use it only to see the graphs.,5,5,0,0,0,0,0,0,0,,5,-,"Ceph Dashboard,Prometheus,Grafana (custom),node_exporter","Ansible (ceph-ansible),Proxmox",
Between 2-5 years,"Open source,Cost",Commercial,4,1,2,1,1,1,4,,4,4,5,2,2,2,3,3,Contributing / enhancing documentation,6 (Detractor),,,Korea South,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My cluster(s) are running a Ceph version older than Luminous",,,"No, I knew they existed by have not used them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,Scalability,Cost",Commercial,4,4,0,0,0,5,3,,5,0,2,2,3,5,2,3,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",10 (Promoter),,"more performance, much more",Germany,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I didn't realize they existed",,2,2500,3000,1200,"Octopus (15.x),Nautilus (14.x)","Distribution packages,Vendor packages",Debian,No,Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a year,I delay upgrades due to concerns about stability,balancer,Supermicro,AMD Epyc,"HDD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt),All-in-one OSD (default, colocated, no separate metadata device)",No,"10 Gb/s Ethernet,40 Gb/s Ethernet,DAC (Copper)",IPv4 only,U.2 (2.5-inch),Single,no numa problem,500 to 1000,BlueStore,"Replication (3x),Erasure coding",Proxmox,Yes,Yes,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",No,Production,"Cloud,Archive Storage",5,0,S3,"Amazon SDK,s3cmd / s3tools",RGW (built-in),DNS round-robin,0,Use case:,No,,,,,,,,,Production,"Build,Cloud,Containers,Home Directories","Linux kernel CephFS mount,ceph-fuse",51 - 100,16G - 31G,3,No,Yes,No,,0 (Detractor),No (Please specify) - cli is ok,1,1,1,1,1,1,0,0,1,1,1,I don't need it at all,croit,Croit,
More than 5 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity","Commercial,Academic",4,4,0,2,0,0,2,,4,4,4,1,,5,4,4,Reporting issues via the bug tracker,6 (Detractor),"Because CEPH is open source (no vendor lock-in), endless scalable, feature rich storage.",1. Support of RDMA 2. Seamless integration in Rancher kubernetes distribution 3. Suppport all features in dashboard without orchestrator and docker,Georgia,"Yes, telemetry is enabled on some of my clusters",My cluster(s) are on a protected network that does not have access to the internet,,I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,3,220,300,150,"Octopus (15.x),Nautilus (14.x)","Upstream packages,Vendor packages","Ubuntu,Debian","Yes, for performance reasons",Within a week after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes","balancer,crash,devicehealth,iostat,pg_autoscaler",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet",IPv4 only,U.2 (2.5-inch),Dual,,100 to 500,BlueStore,"Replication (2x),Erasure coding","OpenStack,Kubernetes,Proxmox,KRBD directly on Linux systems",Yes,Yes,Yes,Production,"Cloud,Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD,iSCSI (tcmu-runner)",Yes,Production,"Build,CDN,Cloud,Backup",2,1500,"S3,RGW admin API","Boto3,s3cmd / s3tools",RGW (built-in),HAproxy,0,no,No,,,,,,,,,Production,"Cloud,Containers,Virtualization,Archive Storage",Linux kernel CephFS mount,1 - 5,1G,2,No,Yes,Yes,Multiple File Systems within a Ceph Cluster,3 (Detractor),No (Please specify) - Because it requires to run CEPH inside docker and wo do not like this,0,0,0,0,0,0,0,0,0,0,0,Dashboard is not usable for us until it depends on CEH deployment insede docker,Proxmox,"ceph-deploy,Proxmox",
More than 5 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Commercial,5,4,0,2,0,5,4,,5,3,3,4,5,4,4,5,"Mailing list,Ceph events / conferences",10 (Promoter),Excellent open source with continuous improvement.,Deduplication Improvements on NVME performance,Taiwan,"Yes, telemetry is enabled on some of my clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,3,120,250,85,"Octopus (15.x),Nautilus (14.x)",Upstream packages,"RHEL / CentOS / Fedora,SUSE",No,Longer,I apply upgrades quickly because they require little effort,Within half a year of release,I delay upgrades due to concerns about stability,"crash,devicehealth,iostat,restful",Other (Please specify) - Ambedded,ARM,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Mix of yes and no,10 Gb/s Ethernet,IPv4 only,"M.2,U.2 (2.5-inch)",Single,simple,10 to 100,BlueStore,"Replication (3x),Erasure coding","OpenStack,Kubernetes,Hyper-v,Proxmox,KVM / QEMU",Yes,Yes,Yes,Production,"CDN,Big Data & Analytics,AI/ML,Cloud,Containers,Virtualization,Home Directories,Archive Storage","Yes, for DR","librbd (e.g., in combination with qemu),Linux kernel RBD,iSCSI (LIO with /dev/rbd)",Yes,Production,"Build,Archive Storage",2,10,S3,"Amazon SDK,s3cmd / s3tools",RGW (built-in),HAproxy,0,Use case:YES,No,,,,,,,,,Production,"Build,Big Data & Analytics","Linux kernel CephFS mount,NFS (Kernel NFS server)",1 - 5,2G - 3G,2,No,No,Yes,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,5,5,,5,5,5,,,,,,none,"Ceph Dashboard,Ceph-Dash,Prometheus,Grafana (custom),node_exporter","Ansible (ceph-ansible),cephadm,Other (Please specify) - Ambedded",
More than 5 years,"Open source,Cost,Feature set,High availability,Integration with adjacent technologies",Commercial,5,5,0,4,0,0,4,Telegram,3,1,3,1,1,5,3,5,"Mailing list,Reporting issues via the bug tracker,Contributing code",10 (Promoter),Ceph - this my work on full time,"performance restart ceph-osd from ""admin host"" more documentation about rocksdb, especially from Igor",Russian Federation,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,20,4000,30,10,"Luminous (12.x),Nautilus (14.x)",Upstream packages,RHEL / CentOS / Fedora,No,Longer,I delay upgrades due to concerns about regression,Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability",I don't know / default set,"Dell,Supermicro","Intel Xeon,AMD Epyc","HDD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,10 Gb/s Ethernet,IPv4 only,"U.2 (2.5-inch),Add In Cards",Dual,,1000 to 3000,BlueStore,"Replication (3x),Erasure coding","OpenStack,Kubernetes,KVM / QEMU,KRBD directly on Linux systems",Yes,Yes,Yes,Production,"Cloud,Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,"Build,Log,Video,CDN,Big Data & Analytics,HPC,Cloud,Containers,Archive Storage,Backup",3,10000,"S3,RGW admin API","Amazon SDK,Boto,Boto3,s3cmd / s3tools,custom / in-house,Other (Please specify) - s5cmd","RGW (built-in),Keystone",None,0,Use case:,No,,,,,,,,,Production,"Build,Log,Big Data & Analytics,Cloud,Containers,Virtualization,Home Directories,Archive Storage","Linux kernel CephFS mount,CIFS (SMB, Samba)",101 - 500,32G - 64G,2,Yes,Yes,Yes,,0 (Detractor),No (Please specify) - we use cli and previous years community instruments,,,,,,,,,,,,--,"Prometheus,Grafana (custom),Nagios/icinga,Other (Please specify) - netdata",Ansible (other / homebrew),
More than 5 years,"Scalability,Cost,Security,High availability,Data durability / reliability / integrity,Performance",Academic,5,4,0,0,0,0,0,,5,5,3,3,2,5,4,4,,10 (Promoter),,,France,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,3,1500,3000,1000,"Octopus (15.x),Nautilus (14.x)","Upstream packages,Distribution packages",Debian,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Within half a year of release,"I upgrade quickly to get new features,I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability","balancer,iostat,pg_autoscaler",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe",LVM (ceph-volume),Yes,10 Gb/s Ethernet,IPv4 only,M.2,Dual,,100 to 500,BlueStore,Replication (3x),"Proxmox,KRBD directly on Linux systems",Yes,No,Yes,Production,Virtualization,"No, it is not needed",Linux kernel RBD,No,,,,,,,,,,,,,,,,,,,,Production,"Home Directories,Archive Storage",Linux kernel CephFS mount,6 - 50,16G - 31G,3,No,No,No,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,5,2,2,2,2,2,0,0,2,0,3,aze,"Ceph Dashboard,Proxmox,InfluxDB,Prometheus,ceph_exporter,Telegraf",ceph-deploy,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance",Commercial,3,4,0,3,0,3,0,,5,3,4,4,5,3,4,2,"IRC / Slack / etc,Mailing list",9 (Promoter),,Better NFS integration (Way) more fault-tolerant CephFS kernel module,"Germany,Netherlands,Romania,Sweden,Switzerland,United Kingdom","Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,5,100,500,300,Mimic (13.x),Upstream packages,Ubuntu,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades because of the effort involved,Other (Please specify) - Maintenance needs to be announced to customers and could cause impact",Longer,"I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability,Other (Please specify) - Cross-compability with OpenStack needs to be checked, maintenance window needs to be announced, potential customer impact",I don't know / default set,"Dell,Supermicro","Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe",Defaults / I don't know,Yes,"40 Gb/s Ethernet,100 Gb/s Ethernet",IPv4 only,U.2 (2.5-inch),Single,CPU was not an issue in the past,10 to 100,BlueStore,"Replication (3x),Erasure coding","OpenStack,Kubernetes,Proxmox,KVM / QEMU",Yes,Yes,Yes,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,"Archive Storage,Backup",5,250,"S3,Swift","Amazon SDK,s3cmd / s3tools","RGW (built-in),Keystone","HAproxy,Varnish",1,No,No,,,,,,,,,Production,"Other (Please specify) - Millions of small files (Not a good idea, I know)",Linux kernel CephFS mount,51 - 100,128G+,6,No,No,Yes,,5 (Detractor),No (Please specify) - Do not use,0,0,0,0,0,0,0,0,0,0,0,Do not use,"croit,Prometheus,Grafana (custom),node_exporter,Other (Please specify) - Check_MK (Nagios)","Ansible (ceph-ansible),Croit",
Between 1-2 years,"Open source,Cost",Commercial,4,1,,,,,,,5,5,3,3,0,1,2,4,"Mailing list,Reporting issues via the bug tracker",2 (Detractor),"Too complex, poor interface, too many bugs, bugs not being worked on.",,That would be Telling :p,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Commercial,5,5,0,3,0,0,5,,4,0,4,4,4,5,5,5,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",10 (Promoter),"Reliability, features set, open-source, High availability, Scalability, performance","S3 buckets metrics(I/O, stats, etc) RBD images size metrics Dashboard possibility use external Prometheus, Grafana and Alertmanager when exist multi clusters","Lithuania,Netherlands,United Kingdom,United States","No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,Yes,"To make decisions about which vendor(s) or model(s) of storage devices to buy,Other",10,1000,6000,2000,"Luminous (12.x),Nautilus (14.x)",Upstream packages,RHEL / CentOS / Fedora,"Yes, for performance reasons",Longer,"I apply upgrades quickly to get any available bug fixes or security fixes,I only upgrade as needed to address issues",Within a year,"I delay upgrades due to the effort required to upgrade,I only upgrade as needed to address specific bugs or needed features","balancer,crash,devicehealth,diskprediction,iostat,Other (Please specify) - prometheus","Dell,HPE (Hewlett Packard),Supermicro",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),lvmcache,Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device),dm-cache,Containerized daemons",No,10 Gb/s Ethernet,Dual stack,U.2 (2.5-inch),Single,"Costs, no context switching",100 to 500,BlueStore,Replication (3x),"OpenStack,KVM / QEMU",Yes,Yes,No,Production,"Cloud,Virtualization","No, it is incompatible with my performance requirements","librbd (e.g., in combination with qemu)",Yes,Production,"Cloud,Virtualization,Archive Storage,Backup",3,30,"S3,Swift","Amazon SDK,Boto,Boto3,s3cmd / s3tools","RGW (built-in),Keystone",HAproxy,0,Use case:,No,,,,,,,,,,,,,,,,,,,5 (Detractor),No (Please specify) - Configure management do it,0,0,0,0,0,0,0,0,0,0,0,"Compatibility to connect dashboard to centralized external Prometheus, Grafana and Alertmanager. When exist multi clusters.","Prometheus,Grafana (custom),node_exporter",Ansible (ceph-ansible),
Less than 1 year,"Open source,Scalability,Cost,High availability","Academic,Non-profit",5,4,0,0,1,0,3,,5,4,4,2,3,4,4,5,"Mailing list,Reporting issues via the bug tracker,Contributing code,Contributing / enhancing documentation",9 (Promoter),,More flexible crush rule,China,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,"No, I didn't realize they existed",,1,68,68,30,Octopus (15.x),"Upstream packages,Distribution packages",Ubuntu,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within half a year of release,"I delay upgrades due to concerns about performance regressions,I upgrade quickly because it is easy,I delay upgrades due to concerns about stability","crash,devicehealth,diskprediction,pg_autoscaler,restful",Other (Please specify) - Tyan,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",No,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",Dual stack,,No preference / beats me,,10 to 100,BlueStore,"Replication (2x),Replication (3x)",KRBD directly on Linux systems,Yes,Yes,Yes,"Development / Testing,Proof of Concept (PoC)","Big Data & Analytics,AI/ML","No, it is not needed",Linux kernel RBD,No,Proof of Concept (PoC),Containers,2,4,S3,"custom / in-house,Other (Please specify) - Docker registry",RGW (built-in),"HAproxy,Nginx,DNS round-robin",0,No,No,,,,,,,,,Production,"Big Data & Analytics,AI/ML,Home Directories",Linux kernel CephFS mount,6 - 50,16G - 31G,2,Yes,No,Yes,,8 (Passive),Yes,5,5,3,4,4,2,0,0,4,3,5,Landing page,"Ceph Dashboard,Prometheus,Grafana (custom),node_exporter",cephadm,
Between 1-2 years,"Open source,Scalability,Feature set,Data durability / reliability / integrity",Commercial,5,4,0,4,0,2,0,,5,3,3,3,2,5,4,3,"IRC / Slack / etc,Mailing list",6 (Detractor),,"speed improvement reliability improvement (faster recoveries, more even data balacing etc) better monitoring (for example, a way to find why random osds are eating lot of cpu sometimes)",Poland,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.",,,"No, I didn't realize they existed",,1,256,256,73,Octopus (15.x),Distribution packages,Ubuntu,No,Within a month after release,"I delay upgrades due to concerns about new functionality,I delay upgrades because of the effort involved",Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability",pg_autoscaler,Supermicro,Intel Xeon,"HDD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),bcache",Yes,10 Gb/s Ethernet,IPv4 only,M.2,Dual,,10 to 100,BlueStore,Replication (3x),"Proxmox,KVM / QEMU,KRBD directly on Linux systems",Yes,No,Yes,Staging,"Virtualization,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,Production,CDN,Linux kernel CephFS mount,1 - 5,4G - 15G,4,No,No,No,"Inline Data,LazyIO",8 (Passive),Yes,3,1,1,5,3,2,0,0,1,0,2,RBD images listing - it lasts forever and ends with an error,"Ceph Dashboard,Zabbix,ceph_exporter","ceph-deploy,Ansible (other / homebrew)",
Between 1-2 years,"Scalability,Feature set,Integration with adjacent technologies",Commercial,5,4,0,0,0,0,0,,5,4,4,3,3,5,4,5,Mailing list,5 (Detractor),updates have lead to catastrophic performance changes and non-functionality which you woulkd expect to be tested first move to container only version is a negative. containers are great but not for everything in every situation,non-container option better performance better reliability between updates,"Czech Republic,Italy","Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,"No, I didn't realize they existed",,4,280,816,608,Nautilus (14.x),Distribution packages,Ubuntu,No,Longer,I delay upgrades due to concerns about regression,Within half a year of release,I delay upgrades due to concerns about performance regressions,"balancer,crash,devicehealth","HPE (Hewlett Packard),Supermicro",Intel (other),"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Mix of yes and no,"10 Gb/s Ethernet,RJ45 (Copper),DAC (Copper)",Dual stack,M.2,No preference / beats me,,10 to 100,BlueStore,"Replication (3x),Erasure coding",OpenStack,Yes,Yes,Yes,"Development / Testing,Production",Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,"Development / Testing,Production",Backup,1,100,S3,Other (Please specify) - triliovault,RGW (built-in),None,0,Use case:,No,,,,,,,,,"Development / Testing,Production","Virtualization,Archive Storage","Linux kernel CephFS mount,NFS (nfs-ganesha)",6 - 50,4G - 15G,1,Unknown,No,No,LazyIO,10 (Promoter),Yes,5,3,4,5,4,3,0,0,3,3,3,additional abilities only available by cli,"Ceph Dashboard,Prometheus,Grafana (custom)",ceph-deploy,
Less than 1 year,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,3,3,1,1,2,0,1,,2,2,2,2,2,2,2,5,"IRC / Slack / etc,Mailing list",4 (Detractor),Too little expirence in my short time of usage.,"The documentation is good but it could be better. Manually triggered handover of CEPHFS MDS Rank 0 without service downtime. Sizing guides (Nodes, RAM, ...) for different usecases.",That would be Telling :p,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.",,,"No, I didn't realize they existed",,4,56,0,0,"Octopus (15.x),Nautilus (14.x)",Upstream packages,Ubuntu,No,Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues",Within a year,"I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,diskprediction,iostat,pg_autoscaler,restful",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe",Defaults / I don't know,Mix of yes and no,"100 Mb/s Ethernet,1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,U.2 (2.5-inch),No preference / beats me,,10 to 100,BlueStore,Replication (3x),"Kubernetes,KVM / QEMU,OpenShift",Yes,Yes,Yes,"Development / Testing,Production","Containers,Virtualization","No, it is not needed",Linux kernel RBD,Yes,"Development / Testing,Production","Cloud,Containers,Virtualization,Home Directories",1,0,S3,Other (Please specify) - idk,"RGW (built-in),Keystone,LDAP",Other (Please specify) - idk,0,Use case:,No,,,,,,,,,"Development / Testing,Production","Cloud,Containers,Virtualization","Linux kernel CephFS mount,ceph-fuse",101 - 500,16G - 31G,1,No,Yes,Yes,Mantle (Programmable Metadata Balancer),10 (Promoter),No (Please specify) - We are using puppet to manage the cluster but it gives an good/quck overview of the CEPH cluster.,5,5,0,0,2,0,0,0,5,0,1,Logs should be correctly listed by time! We often encounter that the listed lines are not correctly listed by time.  2021-02-17 09:29:05.986274 [WRN] 2021-02-17 09:29:12.617681 [INF] 2021-02-17 09:29:12.617623 [INF] 2021-02-17 09:29:11.558152 [WRN] 2021-02-17 09:29:07.002716 [WRN],"Ceph Dashboard,InfluxDB,Prometheus,Grafana (custom),Nagios/icinga","Puppet,ceph-deploy,cephadm",
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Commercial,5,4,0,3,0,1,0,,5,4,5,2,3,5,3,5,"Mailing list,Reporting issues via the bug tracker,Contributing / enhancing documentation,Ceph events / conferences,Other (Please specify) - Consulting",8 (Passive),,,Germany,"No, telemetry is not enabled on any clusters",Other (Please specify) - Personally I only run test clusters,,,"No, I didn't realize they existed",,1,5,5,2,Octopus (15.x),Upstream packages,Ubuntu,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort,Other (Please specify) - It's a test cluster",Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy,Other (Please specify) - It's a test cluster","balancer,crash,devicehealth,iostat,pg_autoscaler,restful,Other (Please specify) - cephadm, dashboard, prometheus","Other (Please specify) - The Ceph nodes are VMs on Proxmox, it's a test cluster",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",No,10 Gb/s Ethernet,IPv4 only,,No preference / beats me,,10 to 100,BlueStore,"Replication (3x),Erasure coding","Proxmox,KVM / QEMU,KRBD directly on Linux systems",Yes,Yes,Yes,Proof of Concept (PoC),"Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Proof of Concept (PoC),Archive Storage,1,1,"S3,RGW admin API","Amazon SDK,Boto3,s3cmd / s3tools",RGW (built-in),None,0,Use case:,No,,,,,,,,,Proof of Concept (PoC),"Scratch,Home Directories,Other (Please specify) - Unstructured Data (aka Office files)","Linux kernel CephFS mount,ceph-fuse,libcephfs,NFS (nfs-ganesha),NFS (Kernel NFS server),CIFS (SMB, Samba)",6 - 50,16G - 31G,2,No,No,Yes,,10 (Promoter),Yes,5,1,1,4,5,4,5,5,4,5,4,None I can think of.,"Ceph Dashboard,Proxmox,Prometheus,Other (Please specify) - CheckMK with https://github.com/HeinleinSupport/check_mk_extensions/tree/master/ceph","cephadm,Proxmox",
Between 1-2 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies","Commercial,Personal",5,4,0,0,0,0,1,,4,5,3,4,3,3,4,5,,8 (Passive),,,France,"Yes, telemetry is enabled on some of my clusters",I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 1-2 years,"Open source,Scalability,Security,High availability,Data durability / reliability / integrity,Performance",Commercial,4,4,0,0,0,0,1,,5,2,3,4,3,4,3,3,Mailing list,9 (Promoter),,,Germany,"No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I didn't realize they existed",,1,8,8,2,Octopus (15.x),Upstream packages,Ubuntu,No,Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues",Within a year,I only upgrade as needed to address specific bugs or needed features,I don't know / default set,Other (Please specify) - The one the hoster provides,AMD Epyc,NVMe,LVM (ceph-volume),No,10 Gb/s Ethernet,Dual stack,U.2 (2.5-inch),No preference / beats me,,10 to 100,BlueStore,Replication (4 or more copies),Kubernetes,Yes,No,No,Production,Containers,"No, it is not needed",Linux kernel RBD,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0 (Detractor),No (Please specify) - I never used it,0,0,0,0,0,0,0,0,0,0,0,Never used,"Prometheus,Grafana (custom)",Rook,
More than 5 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Commercial,5,5,0,0,0,0,3,,2,4,2,3,3,4,4,2,Mailing list,10 (Promoter),ceph is the final point on storage,,France,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,7,60,150,50,Octopus (15.x),Upstream packages,Debian,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort,I delay upgrades due to concerns about regression",Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy","crash,devicehealth,diskprediction,pg_autoscaler",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Yes,10 Gb/s Ethernet,IPv4 only,"M.2,U.2 (2.5-inch)",Single,,10 to 100,BlueStore,Replication (3x),Kubernetes,Yes,Yes,Yes,Production,"Big Data & Analytics,Containers","No, it is not needed",Linux kernel RBD,Yes,Production,"Build,Containers,Archive Storage,Backup",3,1,S3,Boto3,RGW (built-in),HAproxy,0,Use case:,No,,,,,,,,,Production,"Containers,Home Directories",Linux kernel CephFS mount,101 - 500,4G - 15G,3,No,Yes,No,,6 (Detractor),No (Please specify) - I got my own monitoring tools,1,1,1,1,1,1,1,1,1,4,1,no one,"Prometheus,Other (Please specify) - xymon",Rook,
Between 2-5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity",Academic,5,4,0,0,0,0,1,,5,5,2,2,2,5,5,5,Mailing list,10 (Promoter),good experience so far,"Set protection level on directory basis, just like it is possible for EMC Isilon OneFS. http://doc.isilon.com/onefs/6.5.4/webhelp/en-us/Content/dto1288983749046.html",Austria,"Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted),crash: information about daemon crashes, including the type of daemon, ceph version, and stack trace,device: anonymized device health metrics (e.g., SMART hard disk metrics)",I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,1,1807,1807,1200,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,"I delay upgrades because of the effort involved,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features","balancer,devicehealth,pg_autoscaler,restful,Other (Please specify) - dashboard",HPE (Hewlett Packard),Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","Partitions,Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,10 Gb/s Ethernet,IPv4 only,"U.2 (2.5-inch),Add In Cards",Dual,,100 to 500,BlueStore,"Replication (3x),Erasure coding",None,No,No,Yes,,,,,,,,,,,,,,,,,,,,,,,,,Production,"HPC,Home Directories",Linux kernel CephFS mount,1 - 5,128G+,1,No,Yes,Yes,Multiple File Systems within a Ceph Cluster,5 (Detractor),Yes,0,0,0,0,0,0,0,0,0,0,0,"Don't know, do not use it",Grafana (custom),ceph-deploy,
Between 2-5 years,"Open source,Cost,High availability,Data durability / reliability / integrity","Government,Academic",1,2,1,0,0,0,0,google is our best friend,5,3,3,4,5,4,4,4,"Mailing list,Ceph events / conferences",8 (Passive),,,Netherlands,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,84,84,84,Luminous (12.x),Distribution packages,Debian,No,Within a month after release,I delay upgrades due to concerns about regression,Within a year,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability",I don't know / default set,Supermicro,Intel Xeon,"SSD (SATA, SAS)","Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Mix of yes and no,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,,10 to 100,BlueStore,Replication (3x),Proxmox,Yes,No,No,Production,Virtualization,Other (Please specify) - didn't know about it,"librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5 (Detractor),"No (Please specify) - don't know it, we use proxmox GUI for such things",,,,,,,,,,,,none,"Proxmox,Other (Please specify) - graylog",Proxmox,
Between 2-5 years,"Open source,Cost,Integration with adjacent technologies","Commercial,Academic",5,4,0,2,5,0,3,,3,5,4,4,5,4,1,3,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",8 (Passive),,,"Italy,United States","Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted),crash: information about daemon crashes, including the type of daemon, ceph version, and stack trace",I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,2,1000,600,200,"Luminous (12.x),Nautilus (14.x)",Vendor packages,RHEL / CentOS / Fedora,"Yes, for performance reasons",Never,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues,I delay upgrades due to concerns about regression,Other (Please specify) - I use ceph with OpenStack and I used to upgrade the environments together",Longer,"I delay upgrades due to the effort required to upgrade,I only upgrade as needed to address specific bugs or needed features",restful,"HPE (Hewlett Packard),Cisco",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe",LVM (ceph-volume),Yes,"100 Mb/s Ethernet,10 Gb/s Ethernet",IPv4 only,,"Dual,Quad",,100 to 500,"Filestore (XFS, EXT4, BTRFS),BlueStore","Replication (3x),Erasure coding","OpenStack,Kubernetes",Yes,Yes,Yes,Production,"AI/ML,Cloud,Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",No,Production,"Video,Backup",3,10000,S3,s3cmd / s3tools,RGW (built-in),HAproxy,0,not yet,No,,,,,,,Http,,Production,Virtualization,"libcephfs,NFS (nfs-ganesha)",1 - 5,1G,3,No,No,No,,5 (Detractor),No (Please specify),3,4,4,0,0,0,0,0,0,0,0,nothing,Ceph Dashboard,"Ansible (ceph-ansible),Rook",
Less than 1 year,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Academic,5,3,0,0,0,0,1,,5,3,3,2,1,4,2,5,Mailing list,8 (Passive),"Seems to be the only platform to tick all the boxes. However, still some issues and - for some use cases - incomplete documentation",CephFS kernel performance with many snapshots Better documentation of snapshots w.r.t. CephFS,Germany,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,Enabling telemetry would require review by my organization's security team and I have chosen not request a review",,,"No, I didn't realize they existed",,1,524,524,200,Octopus (15.x),Distribution packages,RHEL / CentOS / Fedora,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I only upgrade as needed to address issues,I apply upgrades quickly because they require little effort,I delay upgrades due to concerns about regression",Within a year,"I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","crash,devicehealth,diskprediction,pg_autoscaler",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Partitions,All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",Yes,"10 Gb/s Ethernet,RJ45 (Copper)",IPv4 only,,Single,Avoid NUMA issues,10 to 100,BlueStore,"Replication (2x),Replication (3x),Erasure coding",Proxmox,Yes,No,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,Production,"Scratch,Log,Big Data & Analytics,HPC,Virtualization,Home Directories,Archive Storage","Linux kernel CephFS mount,ceph-fuse,NFS (Kernel NFS server),CIFS (SMB, Samba)",6 - 50,16G - 31G,1,No,Yes,Yes,,10 (Promoter),Yes,5,3,2,3,2,1,0,0,1,0,1,"RBD Overview is not really working (takes long to gather data, shows warning about cached results)","Ceph Dashboard,Nagios/icinga",cephadm,
Between 2-5 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Non-profit,4,3,0,0,1,2,2,,3,3,2,2,3,3,4,4,"IRC / Slack / etc,Mailing list,Ceph events / conferences",7 (Passive),,Better solid murti-region support Integrated backups Simplify heterogeneous infrastructure setups,United States,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.",,,"No, I didn't realize they existed",,2,230,300,90,Nautilus (14.x),Upstream packages,Debian,No,Longer,"I delay upgrades because of the effort involved,Other (Please specify) - lack of time (trying to change that to more frequent upgrades)",Longer,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability,Other (Please specify) - There's a lot of fear in the team (trying to change it)","crash,pg_autoscaler,I don't know / default set","Dell,Intel",Intel Xeon,"SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device),Defaults / I don't know",Yes,"10 Gb/s Ethernet,RJ45 (Copper)",Dual stack,M.2,Dual,price/availability,100 to 500,BlueStore,Replication (3x),"OpenStack,KVM / QEMU",Yes,No,No,Production,"Cloud,Virtualization","No, it is incompatible with my performance requirements","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5 (Detractor),"No (Please specify) - Not currently using the dashboard (I want to set it up, but there's no time)",,,,,,,,,,,,.,"Prometheus,Grafana (custom),Nagios/icinga","Puppet,Other (Please specify) - We want to improve here, most of the setup/maintenance is actually manual",
More than 5 years,"Open source,Cost,Feature set,Security,High availability,Data durability / reliability / integrity",Commercial,5,4,0,0,0,0,0,,5,3,3,3,5,4,3,4,Mailing list,10 (Promoter),"Works so well for us, no reason to consider another option.",Improve journal/DB handling for old installations (partition on boot disk),"Andorra,Spain","Yes, telemetry is enabled on some of my clusters",I haven't gotten around to it yet,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,10,36,100,40,"Luminous (12.x),Nautilus (14.x)",Vendor packages,"Debian,Other OS",No,Longer,I delay upgrades due to concerns about regression,Longer,"I delay upgrades due to concerns about stability,Other (Please specify) - We wait for a new Proxmox VE major version",I don't know / default set,"Dell,We build our own","Intel Xeon,AMD Epyc,AMD (other)","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Partitions,Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Mix of yes and no,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,M.2,"Single,Dual","Cost (Clusters are small, usually 3-node)",10 to 100,Both Filestore and BlueStore,"Replication (2x),Replication (3x)",Proxmox,Yes,No,No,"Development / Testing,Production",Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5 (Detractor),No (Please specify) - We use Proxmox VE WUI,0,0,0,0,0,0,0,0,0,0,0,"None, as we don't use it","Proxmox,Nagios/icinga",Proxmox,
Between 1-2 years,"Open source,Cost",Commercial,5,0,1,0,0,0,0,,5,5,4,3,2,5,3,5,,8 (Passive),,,Germany,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,Enabling telemetry would require review by my organization's security team and I have chosen not request a review",,,"No, I didn't realize they existed",,2,5,6,5,"Octopus (15.x),Nautilus (14.x)",Distribution packages,RHEL / CentOS / Fedora,No,Longer,I only upgrade as needed to address issues,Within a year,I only upgrade as needed to address specific bugs or needed features,I don't know / default set,IBM,AMD (other),SanDisk InfiniFlash or other AFA (All Flash Arrays),Defaults / I don't know,No,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",Dual stack,,No preference / beats me,,10 or fewer,Not sure,Replication (2x),"Kubernetes,OpenShift",Yes,No,Yes,Development / Testing,Containers,"No, it is not needed",Other (Please specify) - ceph-csi,Yes,,,,,,,,,,,,,,,,,,,,Development / Testing,Containers,Other (Please specify) - ceph-csi,1 - 5,1G,4,Unknown,Yes,No,,10 (Promoter),Yes,5,4,1,3,2,4,0,0,4,0,0,n/a,Ceph-Dash,"ceph-deploy,cephadm",
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies","Government,Academic,Non-profit",5,2,1,3,0,0,3,,5,2,4,1,3,4,1,1,"Reporting issues via the bug tracker,Ceph events / conferences,Member of the Ceph Foundation",9 (Promoter),,,South Africa,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,"No, I knew they existed by have not used them",,4,21000,24000,9000,"Octopus (15.x),Luminous (12.x),Nautilus (14.x)",Upstream packages,Ubuntu,No,Within a month after release,"I delay upgrades due to concerns about new functionality,I only upgrade as needed to address issues,I delay upgrades due to concerns about regression,Other (Please specify) - Limited scheduled upgrade windows. On secondary clusters upgrade asap.",Never (clusters remain on major release they were deployed with),"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,Other (Please specify) - Limited scheduled upgrade windows. On secondary clusters upgrade asap.","balancer,crash,devicehealth,diskprediction,pg_autoscaler,Other (Please specify) - prometheus, dashboard","Dell,Supermicro,We build our own","Intel Xeon,Intel (other),AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",No,"40 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet,RJ45 (Copper),DAC (Copper),AOC / Fiber",IPv4 only,"M.2,U.2 (2.5-inch)",Single,"Single for AMD systems, enough PCIe lanes available to get a non-blocking system. Unfortunately dual on Intel.  Cost & simplicity. A bit of Numa concern but have not looked into this yet.",1000 to 3000,BlueStore,"Replication (3x),Erasure coding","Kubernetes,Proxmox",Yes,Yes,Yes,"Development / Testing,Staging,Production,Proof of Concept (PoC)","Scratch,HPC,Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",No,"Development / Testing,Production","Big Data & Analytics,Archive Storage",5,10000,S3,"Boto,s3cmd / s3tools",RGW (built-in),"HAproxy,DNS round-robin",1,Use case:no,No,,,,,,,,,"Development / Testing,Staging","Scratch,Big Data & Analytics,HPC",Linux kernel CephFS mount,6 - 50,4G - 15G,2,No,No,No,"Multiple File Systems within a Ceph Cluster,LazyIO",8 (Passive),Yes,5,4,4,4,2,2,0,0,3,3,4,not sure,"Ceph Dashboard,Proxmox,Prometheus,Grafana (custom)","Ansible (ceph-ansible),cephadm,Proxmox",
Less than 1 year,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,5,4,0,0,0,0,0,,5,4,4,4,4,5,4,3,Mailing list,8 (Passive),,,France,"No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I didn't realize they existed",,1,12,12,4,Nautilus (14.x),"Distribution packages,Vendor packages",Debian,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a year,I delay upgrades to avoid a changed user experience due to new features,"balancer,crash,devicehealth,pg_autoscaler,I don't know / default set,Other (Please specify) - dashboard",Prefer not to say,Intel (other),"HDD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Yes,"1000 Mb/s Ethernet (GigE, gigabit)",IPv4 only,M.2,"Single,Dual",,10 to 100,BlueStore,Replication (3x),Proxmox,Yes,No,No,"Staging,Proof of Concept (PoC)",Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity","Academic,Non-profit",5,4,0,0,1,0,0,,5,5,5,3,2,2,3,5,"Mailing list,Ceph events / conferences",8 (Passive),OpenSource scalability,* support for bare metal installation (WAL+DB on SSD shared with partitions for OS) * hide more of the difficult to understand options under automatisms * enhance the dashboard with algorithms (step by step tutorial) to do standard maintenance (replacement of failed hdds),Germany,"Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted),device: anonymized device health metrics (e.g., SMART hard disk metrics)",I left 'ident' disabled (the default),Yes,Out of curiosity,1,300,300,200,Nautilus (14.x),Vendor packages,Debian,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features",,"Supermicro,Fujitsu,Intel",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Partitions",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet,RJ45 (Copper),DAC (Copper)",IPv4 only,U.2 (2.5-inch),Single,,10 to 100,BlueStore,Erasure coding,None,No,No,Yes,,,,,,,,,,,,,,,,,,,,,,,,,Production,"Virtualization,Home Directories,Archive Storage","Linux kernel CephFS mount,NFS (Kernel NFS server)",1 - 5,65G - 127G,1,No,Yes,No,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,5,0,2,2,2,0,0,0,0,0,0,I don't know,"Ceph Dashboard,Grafana (custom),Nagios/icinga",ceph-deploy,
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Commercial,5,5,0,0,0,0,0,,5,3,5,3,2,5,3,4,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",10 (Promoter),Works great - we have 3 clusters and it just works. Switching from HDD to nvme SSD solved alot of ceph headaches for us though.,NVMe Over Fabrics / RDMA Faster OSDs / more IOPS Dovecot Integration (project seems dead?),Sweden,"Yes, telemetry is enabled on some of my clusters",I haven't gotten around to it yet,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,3,768,994,331,Nautilus (14.x),Upstream packages,Debian,No,Longer,"I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Within a year,"I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,iostat,pg_autoscaler","Supermicro,Intel","Intel Xeon,AMD Epyc","SSD (SATA, SAS),NVMe","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",No,"25 Gb/s Ethernet,AOC / Fiber",IPv4 only,U.2 (2.5-inch),Single,Numa - we dont have to pin osds to specific cores/cpus. We use a single EPYC in newer servers.,100 to 500,BlueStore,Replication (3x),Proxmox,Yes,No,No,Production,"Cloud,Virtualization",Other (Please specify) - Havent gotten around to it yet,"librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10 (Promoter),No (Please specify) - We're used to CLI,5,0,0,0,0,0,0,0,0,0,0,Being able to connect several clusters to one dashboard.,"Ceph Dashboard,Proxmox,Prometheus,Grafana (custom),Telegraf","ceph-deploy,Other (Please specify) - Our own, based on git configs",
Between 2-5 years,"Open source,Cost,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,4,4,0,0,0,0,1,Google.,3,5,1,2,2,5,3,3,Mailing list,8 (Passive),,Seamless LTS>LTS upgrade.,Russian Federation,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,40,120,40,Nautilus (14.x),"Upstream packages,Distribution packages",Ubuntu,No,Longer,"I delay upgrades because of the effort involved,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability","balancer,restful",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,10 Gb/s Ethernet,IPv4 only,"M.2,U.2 (2.5-inch)",Single,No NUMA.,10 to 100,BlueStore,Replication (3x),"OpenStack,KVM / QEMU",Yes,Yes,No,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,Production,Cloud,3,2,Swift,Other (Please specify) - swift,Keystone,HAproxy,0,Not yet.,No,,,,,,,,,,,,,,,,,,,2 (Detractor),No (Please specify) - CLI is more familiar and allows automation.,2,0,0,0,0,0,0,0,0,0,0,Mostly do not use at all.,"Ceph Dashboard,Zabbix",ceph-deploy,
Less than 1 year,"Open source,Scalability,High availability",Personal,5,4,0,0,0,0,0,,5,4,4,2,2,2,5,4,,9 (Promoter),,,Norway,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,84,84,41,Nautilus (14.x),Distribution packages,RHEL / CentOS / Fedora,No,Longer,Other (Please specify) - Generaly delay 2-4weeks on anything not marked as urgent/critical patch,Never (clusters remain on major release they were deployed with),Other (Please specify) - New user,crash,"Raspberry Pi,We build our own","ARM,Intel (other)","HDD (SATA, SAS),SSD (SATA, SAS),NVMe",Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Yes,"1000 Mb/s Ethernet (GigE, gigabit),RJ45 (Copper)",IPv4 only,,Single,Very thin low end nodes (5-10w systems without drives),10 to 100,BlueStore,"Replication (4 or more copies),Erasure coding","Kubernetes,VMware,Microsoft Windows",Yes,Yes,Yes,Development / Testing,Virtualization,"No, it is not needed",iSCSI (tcmu-runner),Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Commercial,4,4,0,2,2,0,0,,5,4,5,4,4,5,4,4,"IRC / Slack / etc,Mailing list",9 (Promoter),Most usable software-defined storage with huge user base and very active community,Higher performance for SSD/NVME Auto-tuning performance,Indonesia,"Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted),crash: information about daemon crashes, including the type of daemon, ceph version, and stack trace",I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,400,400,100,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,iostat","Dell,Supermicro",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Yes,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,,10 to 100,BlueStore,"Replication (2x),Replication (3x)","OpenStack,Kubernetes",Yes,Yes,Yes,"Development / Testing,Production","Cloud,Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Staging,"Cloud,Archive Storage",3,1,S3,"Amazon SDK,s3cmd / s3tools",RGW (built-in),HAproxy,0,Use case:,No,,,,,,,,,Production,"Big Data & Analytics,Cloud,Virtualization","Linux kernel CephFS mount,NFS (Kernel NFS server)",6 - 50,1G,2,Unknown,No,Yes,,6 (Detractor),Yes,2,2,2,2,2,3,0,1,1,2,0,No,"Prometheus,Grafana (custom),node_exporter",ceph-deploy,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Commercial,4,3,0,0,0,0,0,,3,4,4,4,3,5,4,4,IRC / Slack / etc,10 (Promoter),,need more perfomance,"Netherlands,Russian Federation","No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My cluster(s) are running a Ceph version older than Luminous",,,"No, I didn't realize they existed",,3,1500,2000,900,"Luminous (12.x),Jewel (10.x),Nautilus (14.x)",Upstream packages,"Ubuntu,Debian",No,Longer,"I apply upgrades quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about new functionality",Within a year,"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features",balancer,Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,,100 to 500,"Filestore (XFS, EXT4, BTRFS),BlueStore",Replication (3x),"Proxmox,KVM / QEMU,KRBD directly on Linux systems",Yes,No,Yes,Production,"Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,Production,Video,ceph-fuse,51 - 100,16G - 31G,1,No,No,No,Multiple File Systems within a Ceph Cluster,8 (Passive),Yes,5,4,4,2,2,2,2,0,2,2,4,i dont know,"Ceph Dashboard,Proxmox,Prometheus,ceph_exporter","ceph-deploy,Proxmox",
Between 2-5 years,"Scalability,High availability,Data durability / reliability / integrity",Commercial,5,2,0,0,0,0,0,,5,4,4,2,2,5,4,5,,8 (Passive),,,"Germany,Russian Federation","No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,Yes,Out of curiosity,3,2,6,800,"Mimic (13.x),Luminous (12.x)",Upstream packages,Ubuntu,No,Longer,I only upgrade as needed to address issues,Longer,"I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability",,Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","All-in-one OSD (default, colocated, no separate metadata device)",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,,No preference / beats me,,10 to 100,"Filestore (XFS, EXT4, BTRFS),BlueStore",Replication (3x),"Kubernetes,Proxmox,KVM / QEMU",Yes,No,No,"Development / Testing,Staging,Production","Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5 (Detractor),Yes,2,2,2,2,2,2,2,2,2,2,2,CLI,"Prometheus,Grafana (custom)",ceph-deploy,
Between 2-5 years,"Open source,Scalability,High availability",Academic,3,1,0,0,0,0,1,Twitter, blog postings.,3,4,3,3,3,4,4,2,"Other (Please specify) - I regretfully do not participate that much. Ceph is not my primary field of work, but I want to be kept informed.",9 (Promoter),Reliability. We have next to no problems with Ceph.,"I don't really have a wishlist. Although, I would like to see a 'Ceph cluster out of the box' setup. By that, I mean that it would be nice if I could spin-up a virtual cluster with, for example, Vagrant and Virtualbox/kvm, for testing and training purposes. And now that I think of it, a Ceph course is welcome too. Especially one with which you can learn what to do in case an OSD fails, etc.",Netherlands,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,My cluster(s) are on a protected network that does not have access to the internet",,,"No, I didn't realize they existed",,2,300,500,165,Nautilus (14.x),Upstream packages,Ubuntu,"Yes, for performance reasons",Longer,"I only upgrade as needed to address issues,Other (Please specify) - Lack of time.",Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability,Other (Please specify) - Lack of time.",I don't know / default set,Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),Yes,"10 Gb/s Ethernet,RJ45 (Copper),DAC (Copper)",IPv4 only,U.2 (2.5-inch),Single,"One processor is sufficient for us. But since we're in the academic field, the cost factor is also important.",10 to 100,BlueStore,Replication (3x),OpenNebula,Yes,No,No,"Development / Testing,Staging,Production",Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8 (Passive),Yes,5,3,3,3,3,3,0,0,0,0,4,Don't know.,,
More than 5 years,"Open source,Scalability,Cost,High availability,Integration with adjacent technologies",Non-profit,4,4,0,0,1,0,1,,5,4,4,3,2,4,3,4,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",9 (Promoter),,,Brazil,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I didn't realize they existed",,3,600,800,2400,Luminous (12.x),Upstream packages,Ubuntu,"Yes, for performance reasons",Longer,I only upgrade as needed to address issues,Never (clusters remain on major release they were deployed with),"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability",,Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","Partitions,Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Single,,100 to 500,BlueStore,Replication (3x),"OpenStack,KRBD directly on Linux systems,rbd-nbd",Yes,No,No,Production,Archive Storage,"Yes, for DR","librbd (e.g., in combination with qemu),Linux kernel RBD,rbd-nbd",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5 (Detractor),No (Please specify) - prefer cli,0,0,1,0,0,0,0,0,0,0,0,dont use it,"Ceph-metrics,Grafana (custom),Nagios/icinga,Graphite",Salt / DeepSea,
Between 1-2 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Commercial,4,5,0,0,0,4,3,,3,5,2,2,2,5,4,5,Reporting issues via the bug tracker,10 (Promoter),,,United States,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I didn't realize they existed",,2,2430,2867,2867,"Octopus (15.x),Nautilus (14.x)",Vendor packages,Debian,"Yes, in front of EC pools for functionality",Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a month of release,"I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy","balancer,diskprediction,iostat,pg_autoscaler,restful","Dell,Supermicro,We build our own",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Yes,"10 Gb/s Ethernet,40 Gb/s Ethernet,RJ45 (Copper),AOC / Fiber",IPv4 only,"M.2,U.2 (2.5-inch),E1.L (EDSFF, ruler form factor),Add In Cards",Dual,All of our available motherboards are all dual socket.,100 to 500,BlueStore,"Replication (3x),Erasure coding","OpenStack,Kubernetes,VMware,Proxmox,OpenShift",Yes,Yes,No,Production,Archive Storage,"No, it is not needed",iSCSI (tcmu-runner),No,Production,"Build,Log,Video,Cloud,Containers,Virtualization,Archive Storage,Backup",3,2000,"S3,RGW admin API","Amazon SDK,Boto3,s3cmd / s3tools,custom / in-house",RGW (built-in),HAproxy,0,Use case:,No,,,,,,,"Kafka,Amqp",,,,,,,,,,,,10 (Promoter),Yes,5,4,3,4,4,2,3,0,0,4,5,None,"Ceph Dashboard,croit,Proxmox,Zabbix,Nagios/icinga,Telegraf","Ansible (ceph-ansible),ceph-deploy,Croit,Ansible (other / homebrew),Proxmox",
Less than 1 year,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,4,2,1,0,0,0,2,,5,3,3,4,3,5,3,5,IRC / Slack / etc,6 (Detractor),"It provides many features, but it's hard to configure.",,Russian Federation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Less than 1 year,"Open source,Scalability,Cost,Integration with adjacent technologies","Commercial,Personal",4,2,3,4,0,0,0,,4,5,4,5,5,5,4,5,"IRC / Slack / etc,Ceph events / conferences",9 (Promoter),,Easy provisioning for rgw from ceph dashboard More installation and best practices video Community meetup and sharing uses Case,Indonesia,"Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted)",I enabled 'ident' and configured a cluster description and/or email address,Yes,Out of curiosity,3,500,500,200,"Octopus (15.x),Nautilus (14.x)","Upstream packages,Distribution packages","Ubuntu,RHEL / CentOS / Fedora",No,Longer,"I delay upgrades because of the effort involved,I apply upgrades quickly because they require little effort",Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,pg_autoscaler,I don't know / default set","Dell,Supermicro,Huawei,We build our own",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",Mix of yes and no,"10 Gb/s Ethernet,100 Gb/s Ethernet,RJ45 (Copper),DAC (Copper)",IPv4 only,U.2 (2.5-inch),Dual,Gives more performance,10 to 100,BlueStore,"Replication (2x),Replication (3x)","Proxmox,Other (Please specify) - warren.io",Yes,Yes,No,Proof of Concept (PoC),"Cloud,Containers,Virtualization",Other (Please specify) - Not yet,"librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Commercial,4,2,0,0,2,0,4,,5,4,4,4,4,4,4,4,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Contributing code",9 (Promoter),,,That would be Telling :p,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,1,3,1,1,Master,Upstream packages,Other OS,No,Within a week after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a month of release,I upgrade quickly to get new features,,Prefer not to say,ARM,"SSD (SATA, SAS)",dm-cache,Yes,40 Gb/s Ethernet,IPv4 only,M.2,Single,,10 or fewer,Both Filestore and BlueStore,Replication (2x),OpenStack,Yes,No,Yes,Development / Testing,Virtualization,"Yes, for DR","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,Development / Testing,Virtualization,ceph-fuse,6 - 50,32G - 64G,5,No,Yes,Yes,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost",Academic,5,4,0,0,0,0,3,,4,1,2,1,0,5,3,2,Mailing list,2 (Detractor),Perforamnce issues with CephFS and overall flacky stability.,Better tools for performance tracking and tuning.,Austria,"Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted),crash: information about daemon crashes, including the type of daemon, ceph version, and stack trace,device: anonymized device health metrics (e.g., SMART hard disk metrics)",I left 'ident' disabled (the default),"No, I didn't realize they existed",,2,5000,10000,4000,Octopus (15.x),Upstream packages,Debian,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes","balancer,devicehealth,diskprediction,iostat,pg_autoscaler",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,"10 Gb/s Ethernet,40 Gb/s Ethernet,25 Gb/s Ethernet",Dual stack,U.2 (2.5-inch),Dual,Best price/performance ratio.,500 to 1000,BlueStore,"Replication (3x),Erasure coding",None,No,No,Yes,,,,,,,,,,,,,,,,,,,,,,,,,Production,"Video,Big Data & Analytics,HPC,Home Directories,Archive Storage",Linux kernel CephFS mount,51 - 100,32G - 64G,1,No,Yes,Yes,,5 (Detractor),No (Please specify) - CLI can be scripted,2,3,4,2,3,0,0,0,5,0,0,N/A,"Ceph Dashboard,Prometheus,Grafana (custom)",Salt / DeepSea,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Commercial,4,5,0,0,0,0,0,,5,3,4,4,4,5,4,5,Mailing list,10 (Promoter),"Ceph has run very very well for us for the last year and more.  While the community understandably focuses on large deployments, our organization has shown that Ceph scales well to miniature deployments as well.","Mixed physical / virtual deployments.  We don't plan to deploy Octopus, because we don't feel virtualizing OSDs is appropriate.",United States,"No, telemetry is not enabled on any clusters","I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,I do not believe that the Ceph community should be collecting any of this information",,,"No, I didn't realize they existed",,2,137,273,40,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Longer,I delay upgrades due to the effort required to upgrade,"balancer,crash,devicehealth,iostat,I don't know / default set",Supermicro,Intel (other),"HDD (SATA, SAS),SSD (SATA, SAS),NVMe",LVM (ceph-volume),Yes,"1000 Mb/s Ethernet (GigE, gigabit)",IPv4 only,M.2,Single,"Cost, ease of maintenance.",10 to 100,BlueStore,Replication (3x),"OpenStack,Microsoft Windows,OpenShift",Yes,Yes,Yes,Staging,Cloud,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,Production,Archive Storage,1,11162,"S3,Swift",Boto3,RGW (built-in),None,1,No,Yes,1,No,2,Global,,,,,Production,Other (Please specify) - Backup,Linux kernel CephFS mount,1 - 5,1G,6,No,No,No,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,5,0,3,3,4,4,0,0,4,5,0,Can't miss what you've never had.,Ceph Dashboard,Other (Please specify) - Manual,
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity,Performance","Commercial,Personal",5,4,4,0,0,0,3,,4,1,3,5,3,4,2,3,"IRC / Slack / etc,Mailing list",9 (Promoter),"Ceph is amazing. It requires a PhD in everything to operate, but it's resilient as hell, has all the right connections via CSI and is just getting better every day. So my only caution to new users is to plan on spending six months learning it before depending on it and being able to deal with incidents within SLA set by stakeholders.","1. Multitenancy up through Rook (namespaces are sufficient) 2. Stronger NFS support 3. Stronger SSO integrations, maybe similar to k8s AAA with Oauth",United States,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,2,20,40,0,"Octopus (15.x),Nautilus (14.x)",Distribution packages,"Ubuntu,RHEL / CentOS / Fedora",No,Longer,I delay upgrades because of the effort involved,Never (clusters remain on major release they were deployed with),"I delay upgrades due to concerns about stability,Other (Please specify) - I trust that Ceph is stable, rather I don't trust my own ability to execute the upgrade flawlessly and deal with the repercussions of screwing it up in a time-boxed manner. So it's really a problem with time management.",pg_autoscaler,Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","All-in-one OSD (default, colocated, no separate metadata device)",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,U.2 (2.5-inch),Dual,I use Supermicro fat twin mobos and they don't usually come with more than two sockets,10 to 100,BlueStore,Replication (3x),Kubernetes,Yes,Yes,Yes,"Staging,Production","Containers,Virtualization",Other (Please specify) - I saw it once but didn't really grok it at the time. May use it soon.,"librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Development / Testing,Backup,1,50,"S3,NFS re-export of S3 buckets","Amazon SDK,Boto3,s3cmd / s3tools","RGW (built-in),Other (Please specify) - Would like to see Oauth tokens",None,2,Use case:,No,,,,,,,,I didn't even know these existed. Great stuff!,Production,Other (Please specify) - I really only use it for RWX storage,"Linux kernel CephFS mount,NFS (nfs-ganesha)",6 - 50,1G,2,No,Yes,Yes,Multiple File Systems within a Ceph Cluster,7 (Passive),No (Please specify) - I'm running under Rook and the integration seems to be weak so I don't usually bother to install it.,,,,,,,,,,,,"Lack of Rook integration means I don't use it much. It's eye candy, but rarely a go-to.","Prometheus,node_exporter",Rook,
Between 2-5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity",Commercial,4,4,0,0,0,0,0,,4,4,4,2,3,5,5,4,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",9 (Promoter),,,"Austria,Croatia,Czech Republic,Germany,Greece,Hungary,Macedonia,Montenegro,Poland,Romania,Slovakia","No, telemetry is not enabled on any clusters","My organization's security policy does not allow storage systems to communicate with vendors or external organizations,My cluster(s) are running a Ceph version older than Luminous",,,"No, I knew they existed by have not used them",,24,360,2700,900,"Mimic (13.x),Jewel (10.x)","Distribution packages,Vendor packages",Ubuntu,No,Within a month after release,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Never (clusters remain on major release they were deployed with),Other (Please specify) - Vendor support,,"Lenovo,HPE (Hewlett Packard)","Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device),bcache",Yes,"10 Gb/s Ethernet,40 Gb/s Ethernet,25 Gb/s Ethernet,DAC (Copper)",IPv4 only,U.2 (2.5-inch),Single,"numa, costs",500 to 1000,"Filestore (XFS, EXT4, BTRFS),BlueStore",Replication (3x),OpenStack,Yes,No,No,"Development / Testing,Staging,Production",Cloud,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8 (Passive),Yes,4,4,5,4,1,2,0,0,0,0,4,non,"Ceph Dashboard,Prometheus,Grafana (custom),node_exporter,ceph_exporter",Juju,
Between 2-5 years,"Open source,Scalability,Feature set,Security,High availability,Data durability / reliability / integrity,Performance",Commercial,4,5,0,2,1,0,0,,2,4,2,2,3,5,4,5,"Mailing list,Ceph events / conferences",8 (Passive),,better performance of bluestore better documentation of tools around bluestore easier scaling of MDS,Poland,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I didn't realize they existed",,5,1024,2560,1500,"Mimic (13.x),Nautilus (14.x)",Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,I delay upgrades due to concerns about regression,Within half a year of release,I delay upgrades due to concerns about performance regressions,"balancer,devicehealth,diskprediction",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Mix of yes and no,"10 Gb/s Ethernet,AOC / Fiber",IPv4 only,U.2 (2.5-inch),Single,Cost,100 to 500,Both Filestore and BlueStore,Replication (3x),"Kubernetes,KVM / QEMU",Yes,Yes,No,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu),iSCSI (tcmu-runner)",No,Production,CDN,20,5,S3,"Amazon SDK,Boto,Boto3,s3cmd / s3tools",RGW (built-in),Nginx,4,Use case: Did not know it existed,Yes,1,No,1,Global,,,,,,,,,,,,,,,9 (Promoter),No (Please specify) - Prefer CLI,5,3,2,0,0,0,0,0,0,0,0,replace disk,"Ceph Dashboard,Ceph-Dash,Prometheus,Grafana (custom),Zabbix,Graphite,node_exporter,ceph_exporter","Chef,Rook,Ansible (other / homebrew)",
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability","Commercial,Personal",4,3,0,2,0,0,3,,3,4,4,2,,4,4,3,"Mailing list,Reporting issues via the bug tracker,Contributing / enhancing documentation",10 (Promoter),,,Germany,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,High availability,Integration with adjacent technologies",Commercial,5,5,0,1,3,0,3,,5,5,5,4,4,4,4,4,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",10 (Promoter),,Good performances and storage usage on small configs (3 nodes) VMware RBD support Windows RBD support,France,"No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I didn't realize they existed",,3,480,600,250,Octopus (15.x),Upstream packages,Ubuntu,No,Within a month after release,Other (Please specify) - 3 months MCO,Within a year,"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,devicehealth,diskprediction,pg_autoscaler,Other (Please specify) - prometheus",Dell,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Yes,"10 Gb/s Ethernet,DAC (Copper),AOC / Fiber",IPv4 only,U.2 (2.5-inch),No preference / beats me,,10 to 100,BlueStore,"Replication (3x),Erasure coding","OpenStack,KRBD directly on Linux systems",Yes,Yes,No,Production,"Cloud,Archive Storage",Other (Please specify) - soon,"librbd (e.g., in combination with qemu),Linux kernel RBD,rbd-nbd",Yes,Production,"Cloud,Archive Storage",4,1,"S3,Swift","Amazon SDK,Boto3,s3cmd / s3tools,Other (Please specify) - rclone","RGW (built-in),Keystone",HAproxy,0,Use case:,No,,,,,,,,,,,,,,,,,,,9 (Promoter),Yes,4,2,,5,5,5,0,0,0,4,5,S3 keys S3 policies S3 Lifecycle,"Ceph Dashboard,Prometheus,Grafana (custom),node_exporter",Ansible (ceph-ansible),
Between 2-5 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity,Performance",Academic,5,5,0,4,1,0,3,,5,5,5,3,5,5,3,5,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",10 (Promoter),"Very active and helpful community, also usually fast reaction to questions or bug reprts.","1. SSH orchestrator (for bare metal operation, without containers) to allow good interoperability with separate configuration management systems.  2. Live migration between different erasure coding profiles.  3. Support for an open RedHat rebuild once the ""winner"" becomes clear (AlmaLinux, RockyOS etc.).",Germany,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,Other (Please specify) - will do once we are on Octopus (better telemetry configurability)",,,"No, I knew they existed by have not used them",,3,1000,1200,600,"Mimic (13.x),Nautilus (14.x)",Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,I delay upgrades due to concerns about regression,Longer,I delay upgrades due to concerns about stability,"balancer,crash,devicehealth,iostat,pg_autoscaler",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",No,"1000 Mb/s Ethernet (GigE, gigabit),InfiniBand",IPv4 only,"U.2 (2.5-inch),Add In Cards","Single,Dual",single or dual depending on the number of OSDs (we typically use compression),100 to 500,BlueStore,"Replication (3x),Erasure coding",KVM / QEMU,Yes,Yes,Yes,Production,Virtualization,"Yes, for DR","librbd (e.g., in combination with qemu)",Yes,Production,"Archive Storage,Backup",3,1,"S3,RGW admin API","Boto3,s3cmd / s3tools",RGW (built-in),DNS round-robin,0,currently not,No,,,,,,,,,Production,"Big Data & Analytics,HPC,Other (Please specify) - shared configuratio files","ceph-fuse,NFS (nfs-ganesha)",101 - 500,4G - 15G,1,No,No,Yes,LazyIO,9 (Promoter),Yes,4,2,3,1,1,2,0,0,0,0,0,"Executing commands bare-metal (when no containers are used), e.g. ceph-volume for disk replacement.","Ceph Dashboard,Zabbix","Puppet,ceph-deploy",
Less than 1 year,"Open source,Scalability,Cost,Security,High availability,Data durability / reliability / integrity,Performance,Other (Please specify) - also runs well on k8s, thanks to rook",Personal,5,2,0,0,0,0,0,man pages,5,3,4,4,2,3,4,3,,10 (Promoter),"it's open source, reliable, scalable, and there's the rook operator",,"Germany,Italy","No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,3,8,11,5,"Octopus (15.x),Mimic (13.x)",Upstream packages,Ubuntu,No,Longer,I only upgrade as needed to address issues,Longer,I delay upgrades due to the effort required to upgrade,"balancer,pg_autoscaler,I don't know / default set,Other (Please specify) - prometheus","Raspberry Pi,Intel","ARM,Intel (other)","HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),No,"100 Mb/s Ethernet,1000 Mb/s Ethernet (GigE, gigabit)",IPv4 only,"M.2,U.2 (2.5-inch)",No preference / beats me,,10 or fewer,BlueStore,Replication (3x),"OpenStack,Kubernetes",Yes,No,Yes,"Development / Testing,Production","Cloud,Containers,Virtualization",Other (Please specify) - Not yet,"Linux kernel RBD,Other (Please specify) - CSI Node plugins",Yes,,,,,,,,,,,,,,,,,,,,"Development / Testing,Production",Containers,Linux kernel CephFS mount,1 - 5,1G,1,No,Yes,No,Multiple File Systems within a Ceph Cluster,5 (Detractor),"No (Please specify) - i don't use it, i prefer monitoring through external grafana",0,0,0,0,0,0,0,0,0,0,0,i would know if i used it,"Prometheus,Grafana (custom),node_exporter","ceph-deploy,Rook",
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Commercial,3,2,0,1,1,4,2,,3,3,4,3,3,5,4,3,Ceph events / conferences,7 (Passive),,,Vietnam,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I knew they existed by have not used them",,15,253,500,158,"Octopus (15.x),Luminous (12.x),Nautilus (14.x)",Upstream packages,RHEL / CentOS / Fedora,No,Never,I apply upgrades quickly to get any available bug fixes or security fixes,Never (clusters remain on major release they were deployed with),"I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability","balancer,iostat,pg_autoscaler,restful","Dell,HPE (Hewlett Packard),Cisco","Intel (other),AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),Yes,"10 Gb/s Ethernet,40 Gb/s Ethernet",IPv4 only,U.2 (2.5-inch),Dual,Money,100 to 500,BlueStore,"Replication (2x),Erasure coding","OpenStack,Proxmox,KVM / QEMU",Yes,Yes,Yes,Production,"Cloud,Virtualization","Yes, for DR","librbd (e.g., in combination with qemu)",Yes,"Development / Testing,Production,Proof of Concept (PoC)","Cloud,Virtualization",3,50,"S3,RGW admin API","Boto3,s3cmd / s3tools","RGW (built-in),LDAP",Nginx,6,Use case:no,Yes,1,No,1,Global,,,Http,,"Development / Testing,Production,Proof of Concept (PoC)","Cloud,Home Directories","ceph-fuse,NFS (nfs-ganesha)",51 - 100,1G,3,No,No,No,,7 (Passive),Yes,3,3,3,3,3,3,3,3,3,3,3,Create bucket with Object locking,"Ceph Dashboard,Graphite,ceph_exporter,Telegraf","Ansible (ceph-ansible),ceph-deploy,Proxmox",
Between 2-5 years,"Open source,Scalability,Feature set,High availability,Performance",Commercial,5,0,5,0,0,0,0,,,,,5,,,5,4,"IRC / Slack / etc,Reporting issues via the bug tracker",10 (Promoter),,"* Out of the box backup and restore capabilities for object and block storage and ability to backup the same to S3/GCS/Azure blob storage and nfs server, please take a look at backup functionalities offered by longhorn * Ability to encrypt Ceph PVC's and ceph object store and it should be as easy as this https://docs.portworx.com/key-management/kubernetes-secrets/pvc-encryption-using-storageclass/ * Dashboarding / Monitoring UI  * Debugging ceph related issues is also quite a pain many a times so some debugging utility through which all known issues can be surfaced to the user , for example like the way istioctl analyze command , analyzes the issues and report the same to end user","Australia,India,United Kingdom,United States","No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,300,1,150,150,Nautilus (14.x),Upstream packages,"Ubuntu,Debian,RHEL / CentOS / Fedora,SUSE",No,Longer,I delay upgrades due to concerns about regression,Within a year,"I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability",I don't know / default set,"Dell,HPE (Hewlett Packard),Intel","ARM,Intel (other),AMD (other)","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","All-in-one OSD (default, colocated, no separate metadata device)",No,100 Mb/s Ethernet,Dual stack,,No preference / beats me,,10 or fewer,BlueStore,Replication (2x),"Kubernetes,OpenShift",Yes,Yes,No,Production,"Log,Big Data & Analytics,AI/ML,Archive Storage","No, it is not needed",Linux kernel RBD,No,Production,"Log,Big Data & Analytics,AI/ML,Archive Storage",2,500,"S3,RGW admin API","Amazon SDK,Boto3,s3cmd / s3tools",RGW (built-in),"Nginx,Other (Please specify) - istio",1,Use case:,No,,,,,,,,,,,,,,,,,,,3 (Detractor),Yes,,,,,,,,,,,,aggregated view or single pane of glass through which all possible issues can be detected from one single page,"Ceph Dashboard,Prometheus",Rook,
Less than 1 year,"Scalability,Cost,High availability,Data durability / reliability / integrity",Commercial,5,1,0,3,0,0,0,,2,4,1,1,3,4,2,5,IRC / Slack / etc,7 (Passive),,,Ukraine,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),Yes,Out of curiosity,1,135,135,86,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Longer,I only upgrade as needed to address issues,Never (clusters remain on major release they were deployed with),"I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,devicehealth,diskprediction,iostat,pg_autoscaler",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Yes,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),"Single,No preference / beats me",,10 to 100,BlueStore,Erasure coding,Other (Please specify) - cephfs directly on Linux systems,No,No,Yes,,,,,,,,,,,,,,,,,,,,,,,,,Production,"Cloud,Archive Storage",Linux kernel CephFS mount,1 - 5,1G,1,No,No,Yes,,8 (Passive),Yes,5,2,1,2,3,1,3,1,2,0,5,daemons logs,Ceph Dashboard,cephadm,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability",Commercial,5,5,2,0,5,5,0,,5,3,5,5,1,4,3,3,IRC / Slack / etc,10 (Promoter),I think it's awsome,Replication for cephFS,Sweden,"Yes, telemetry is enabled on some of my clusters",My cluster(s) are on a protected network that does not have access to the internet,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,5,350,400,280,"Mimic (13.x),Luminous (12.x),Nautilus (14.x)","Upstream packages,Vendor packages","Ubuntu,RHEL / CentOS / Fedora",No,Never,I delay upgrades because of the effort involved,Longer,I delay upgrades due to concerns about stability,"balancer,iostat","Dell,HPE (Hewlett Packard)",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Mix of yes and no,"100 Mb/s Ethernet,25 Gb/s Ethernet",IPv4 only,U.2 (2.5-inch),Dual,Performance,10 to 100,BlueStore,Replication (3x),"OpenStack,VMware,KRBD directly on Linux systems",Yes,No,No,Production,Home Directories,"No, it is not compatible with my RBD feature requirements","librbd (e.g., in combination with qemu),Linux kernel RBD,iSCSI (tcmu-runner)",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9 (Promoter),No (Please specify) - It gives overview but I rather manage with CLI,3,1,1,0,0,0,0,0,0,4,1,troubleshooting,"Ceph Dashboard,Prometheus,Grafana (custom),Nagios/icinga,node_exporter",Ansible (ceph-ansible),
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Personal,4,4,0,0,0,0,0,,4,4,3,3,3,4,3,5,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),,"Cephfs for Windows More self tuning (like the balancer, autoscaler) cephx permission management in dashboard",Netherlands,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,30,30,10,Nautilus (14.x),Vendor packages,Debian,No,Within a month after release,"I delay upgrades because of the effort involved,I apply upgrades quickly because they require little effort,Other (Please specify)",Within a year,"I upgrade quickly to get new features,I upgrade quickly because it is easy,I only upgrade as needed to address specific bugs or needed features","balancer,crash,devicehealth,diskprediction,pg_autoscaler","Dell,HPE (Hewlett Packard)",Intel (other),"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),At-rest encryption (dmcrypt)",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet,RJ45 (Copper)",IPv4 only,,No preference / beats me,,10 to 100,BlueStore,Replication (3x),"Microsoft Windows,Proxmox,KVM / QEMU",Yes,No,Yes,"Development / Testing,Production","Video,Home Directories,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,"Development / Testing,Production","Video,Home Directories,Archive Storage","Linux kernel CephFS mount,ceph-fuse",6 - 50,2G - 3G,2,No,No,No,Inline Data,7 (Passive),Yes,4,3,4,3,4,1,0,0,2,0,0,Deploy iscsi and manage cephx permissions,"Ceph Dashboard,Proxmox,Grafana (custom),Zabbix","ceph-deploy,Proxmox",
Between 2-5 years,"Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Commercial,2,4,0,0,0,0,2,,4,1,1,3,2,5,4,4,,8 (Passive),,,Russian Federation,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,4,500,1150,500,"Mimic (13.x),Nautilus (14.x)",Upstream packages,RHEL / CentOS / Fedora,No,Longer,I delay upgrades due to concerns about new functionality,Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability",balancer,Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","Partitions,Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),bcache",Yes,10 Gb/s Ethernet,IPv4 only,"U.2 (2.5-inch),Add In Cards",Dual,,100 to 500,Both Filestore and BlueStore,"Replication (2x),Replication (3x),Erasure coding",Proxmox,Yes,Yes,No,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,Production,"Log,Archive Storage,Backup",2,2000,S3,"Amazon SDK,Boto3,s3cmd / s3tools,custom / in-house",RGW (built-in),"DNS round-robin,Other (Please specify) - gobetween",0,No,No,,,,,,,,,,,,,,,,,,,0 (Detractor),"No (Please specify) - I love CLI, dashboard for noobs",0,0,0,0,0,0,0,0,0,0,0,-,"Zabbix,node_exporter,ceph_exporter",Ansible (other / homebrew),
Between 2-5 years,"Open source,Scalability,Cost,Feature set,Data durability / reliability / integrity,Other (Please specify)","Government,Other (Please specify)",4,3,0,0,0,0,3,,5,3,,,3,5,3,4,"IRC / Slack / etc,Reporting issues via the bug tracker,Other (Please specify)",8 (Passive),,cephfs multidc replication,Russian Federation,"Yes, telemetry is enabled on some of my clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,2,7300,7600,2000,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,I delay upgrades due to concerns about regression,Within a year,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability","balancer,iostat",Dell,Intel Xeon,"HDD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),bcache",Yes,10 Gb/s Ethernet,IPv4 only,,Dual,,500 to 1000,BlueStore,"Replication (4 or more copies),Erasure coding",VMware,No,Yes,Yes,,,,,,Production,Archive Storage,4,1,"S3,RGW admin API","Amazon SDK,Boto3,s3cmd / s3tools",RGW (built-in),HAproxy,0,Use case:,No,,,,,,,,,Production,"Scratch,Archive Storage","Linux kernel CephFS mount,NFS (nfs-ganesha),CIFS (SMB, Samba)",51 - 100,65G - 127G,1,No,No,Yes,,8 (Passive),Yes,3,3,1,1,2,0,0,0,3,4,0,cephfs creds and more feature for rgw,"Ceph Dashboard,Prometheus,Grafana (custom),node_exporter,ceph_exporter","ceph-deploy,Ansible (other / homebrew)",
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Commercial,5,1,2,3,0,5,0,,5,5,4,4,4,4,4,5,"Contributing / enhancing documentation,Ceph events / conferences",10 (Promoter),"It's an amazing system. I wouldn't recommend it for every use case offcourse, but if the requirements match Ceph it's usecase I will definitly recommend it.","Documentation enhancement, like for the ansible project.",Netherlands,"Yes, telemetry is enabled on some of my clusters","I haven't gotten around to it yet,My cluster(s) are on a protected network that does not have access to the internet,Enabling telemetry would require review by my organization's security team and I have chosen not request a review",,I left 'ident' disabled (the default),"No, I didn't realize they existed",,2,600,610,203,Nautilus (14.x),Upstream packages,Ubuntu,"Yes, for performance reasons",Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues,Other (Please specify) - we rollout fixes as needed or in a 6 months cycle",Within a year,"I delay upgrades due to the effort required to upgrade,I only upgrade as needed to address specific bugs or needed features,Other (Please specify) - we rollout fixes as needed or in a 6 months cycle",I don't know / default set,Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,"10 Gb/s Ethernet,DAC (Copper)",IPv4 only,U.2 (2.5-inch),No preference / beats me,,10 or fewer,BlueStore,"Replication (3x),Erasure coding","Kubernetes,KVM / QEMU",Yes,Yes,No,Production,"Containers,Virtualization","Other (Please specify) - No, not needed (YET)","librbd (e.g., in combination with qemu),Other (Please specify) - ceph-csi",Yes,Production,Containers,0,0,S3,"s3cmd / s3tools,custom / in-house",RGW (built-in),Other (Please specify) - keepalive,0,"Use case: yes, I think this might be usefull as we have lots of user files like pictures in our s3 storage. But I have to look into it more to see if it's actually usefull",No,,,,,,,,,,,,,,,,,,,10 (Promoter),Yes,2,0,2,4,4,3,0,0,0,2,3,"not sure, sorry.","Ceph Dashboard,Prometheus,Grafana (custom)","Ansible (ceph-ansible),Rook",
More than 5 years,"Open source,Scalability,High availability",Academic,1,4,0,0,0,5,0,,4,5,4,4,3,5,3,3,,6 (Detractor),"Ceph is a feature beast in a positive way. But that makes it also very difficult to find the best setup, as it is so flexible. And works in so many hardware setups as well.",More easy setup/configuration  see 1 see 2 :),Germany,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I knew they existed by have not used them",,1,2,3,2,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,"Yes, for performance reasons",Within a month after release,"I delay upgrades due to concerns about new functionality,I delay upgrades because of the effort involved,I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability",I don't know / default set,Supermicro,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe",Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),No,"10 Gb/s Ethernet,40 Gb/s Ethernet,RJ45 (Copper),Other (Please specify) - QSFP+",IPv4 only,U.2 (2.5-inch),"Single,Dual",Depends on the usecase .... and suggestion by our hardware reseller.,500 to 1000,BlueStore,"Replication (3x),Erasure coding","VMware,Microsoft Windows,Xen",Yes,No,Yes,Production,Archive Storage,Other (Please specify) - dont know.,Linux kernel RBD,No,,,,,,,,,,,,,,,,,,,,Production,"Video,Virtualization","Linux kernel CephFS mount,NFS (Kernel NFS server),CIFS (SMB, Samba)",1 - 5,1G,5,Unknown,No,No,Multiple File Systems within a Ceph Cluster,9 (Promoter),No (Please specify) - Dont use that yet.,1,1,1,1,1,1,1,1,1,1,1,"Non yet, as we dont use it much.","Ceph Dashboard,Other (Please specify) - check_MK",ceph-deploy,
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance","Academic,Non-profit",4,3,0,0,0,0,1,,4,4,3,2,3,5,4,4,Mailing list,9 (Promoter),,,United States,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,3,1200,3000,1000,"Luminous (12.x),Nautilus (14.x)",Upstream packages,Debian,No,Longer,I delay upgrades due to concerns about regression,Within half a year of release,"I delay upgrades due to concerns about performance regressions,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy",I don't know / default set,"Lenovo,Supermicro,We build our own","Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Mix of yes and no,"10 Gb/s Ethernet,RJ45 (Copper)",IPv4 only,U.2 (2.5-inch),Single,Avoids NUMA concerns,100 to 500,BlueStore,Replication (3x),KRBD directly on Linux systems,Yes,No,Yes,Production,"Video,HPC,Home Directories,Archive Storage","No, it is incompatible with my performance requirements",Linux kernel RBD,No,,,,,,,,,,,,,,,,,,,,Production,"Scratch,HPC",Linux kernel CephFS mount,101 - 500,32G - 64G,3,Unknown,No,No,Multiple File Systems within a Ceph Cluster,3 (Detractor),No (Please specify) - Major clusters still luminous due to GCC migration that broke upgrade process for Debian 9,0,0,0,0,0,0,0,0,0,0,0,Do not use yet,"InfluxDB,Grafana (custom),Nagios/icinga,Telegraf","Salt / DeepSea,ceph-deploy",
Between 2-5 years,"Open source,Scalability,High availability",Commercial,5,3,0,0,0,0,2,,4,2,4,3,3,5,5,4,Mailing list,10 (Promoter),,,"Luxembourg,Russian Federation,Singapore,United States","Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted)",I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Less than 1 year,"Open source,Scalability,Data durability / reliability / integrity",Commercial,4,1,0,0,0,3,2,,5,3,4,5,2,4,3,4,Reporting issues via the bug tracker,7 (Passive),,,Latvia,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,1000,1000,1000,Nautilus (14.x),Vendor packages,RHEL / CentOS / Fedora,No,Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a year,I only upgrade as needed to address specific bugs or needed features,pg_autoscaler,Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet",Dual stack,,Dual,NUMA,100 to 500,BlueStore,"Replication (3x),Erasure coding",None,No,Yes,No,,,,,,Production,"Log,Video,Cloud,Containers,Backup",3,500,S3,"Amazon SDK,s3cmd / s3tools",RGW (built-in),HAproxy,0,Use case: no,No,,,,,,,"Kafka,Http",,,,,,,,,,,,7 (Passive),Yes,4,3,4,4,4,0,0,0,0,5,5,s3 subuser,"Ceph Dashboard,Zabbix",Ansible (ceph-ansible),
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance","Commercial,Non-profit,Personal",4,4,1,0,0,2,2,,3,3,3,4,3,5,3,3,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",9 (Promoter),Because ceph is the answer to a lot of storage challenges,"- NVMEoF - Cephfs: Built-in mechanism for easy movement (archival) of data/files between pools, ie when moving a file from nvme-based pool to a dir that is pinned to hdd, the data should be moved automatically to hdd-pool. - cephfs: native webdav-client with ad/ldap auth and locking",Norway,"No, telemetry is not enabled on any clusters",Other (Please specify) - Enabling telemetry gives HEALTH_ERR...,,,"No, I didn't realize they existed",,1,1500,1500,500,Nautilus (14.x),"Distribution packages,Vendor packages",Debian,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a year,"I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,crash,devicehealth",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe,Optane","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Yes,40 Gb/s Ethernet,IPv4 only,Add In Cards,Dual,,100 to 500,BlueStore,"Replication (3x),Erasure coding","Kubernetes,Microsoft Windows,Proxmox,KVM / QEMU,Other (Please specify) - macOS",Yes,No,Yes,Production,"Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,Production,"Video,CDN,HPC,Containers,Virtualization,Home Directories,Archive Storage,Other (Please specify) - Video/media production","Linux kernel CephFS mount,CIFS (SMB, Samba)",51 - 100,4G - 15G,1,No,Yes,Yes,"Inline Data,Mantle (Programmable Metadata Balancer),Multiple File Systems within a Ceph Cluster",9 (Promoter),Yes,4,3,3,0,1,1,0,0,1,0,0,Timeseries graphs,"Ceph Dashboard,Proxmox,Prometheus,Grafana (custom),node_exporter,ceph_exporter","Ansible (other / homebrew),Proxmox",
Between 1-2 years,"Open source,Scalability,Cost,Feature set",Commercial,2,3,0,0,4,5,1,,5,2,2,5,5,5,5,2,IRC / Slack / etc,3 (Detractor),"It is still a massively complex system to operate at scale. Depends on the use case, but certainly not useful below 2-5 PB.",Better visibility for troubleshooting high latency. Better automatic tuning during rebuilds and rebalancing to avoid service disruption. Higher performance for RGW.,Denmark,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,2,4600,8000,5000,Octopus (15.x),Upstream packages,"Ubuntu,Debian",No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability",balancer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 1-2 years,"Open source,High availability,Data durability / reliability / integrity",Commercial,4,4,1,2,1,0,3,,3,5,5,3,4,4,4,,"Contributing code,Ceph events / conferences",7 (Passive),,,Korea South,"No, telemetry is not enabled on any clusters",My cluster(s) are running a Ceph version older than Luminous,,,"No, I didn't realize they existed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Academic,5,4,0,0,0,4,3,,5,5,3,3,4,4,4,4,"Mailing list,Reporting issues via the bug tracker,Contributing code,Ceph events / conferences",8 (Passive),Really solid scaleable open source storage,Better debug tools for e.g. unhappy bluestores,United Kingdom,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,My cluster(s) are running a Ceph version older than Luminous",,,Yes,Out of curiosity,3,16300,18000,6000,"Mimic (13.x),Luminous (12.x)",Distribution packages,Ubuntu,No,Within a month after release,Other (Please specify) - Wait for Canonical to release packages based on the new version,Longer,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability,Other (Please specify) - Hard to carve out time for investigating new versions' stability",I don't know / default set,Supermicro,Intel Xeon,"HDD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,100 Gb/s Ethernet,IPv4 only,M.2,Quad,"We have 60 OSDs per server, and need enough CPU to drive them all especially during recovery",3000 to 5000,BlueStore,Replication (3x),OpenStack,Yes,Yes,No,"Development / Testing,Production","Big Data & Analytics,HPC,Cloud","No, it is not needed","librbd (e.g., in combination with qemu)",Unknown,"Development / Testing,Production","Scratch,Log,CDN,Big Data & Analytics,HPC,Cloud,Containers,Archive Storage,Backup,Other (Please specify) - web services",6,0,"S3,RGW admin API","Amazon SDK,Boto,Boto3,s3cmd / s3tools,custom / in-house",RGW (built-in),"HAproxy,DNS round-robin,Other (Please specify) - RIP",0,Use case:,No,,,,,,,"Kafka,Amqp",,,,,,,,,,,,2 (Detractor),"No (Please specify) - cli more scriptable, infrastructure-as-code approach",0,0,0,0,0,0,0,0,0,0,0,https and user management both difficult to configure and hard to integrate.,"Grafana (custom),Graphite,Collectd","Ansible (ceph-ansible),Ansible (other / homebrew)",
More than 5 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity",Commercial,5,5,0,1,2,0,5,,5,,5,3,1,4,2,4,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Contributing code,Member of the Ceph Foundation",10 (Promoter),,- More focus on testing and regressions,"Canada,Germany,India,Netherlands,Singapore,United Kingdom,United States","No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,36,7859,52679,0,"Luminous (12.x),Nautilus (14.x)","We build our own packages,We built a custom version","Ubuntu,RHEL / CentOS / Fedora",No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression,Other (Please specify) - regressions and bugs are too frequent for us to apply updates frequently. We usually backport the patches needed",Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability,Other (Please specify) - Undocumented changes are a pain to discover. We're building automation to catch them all but until we do we cannot upgrades without a massive amount of manual effort","balancer,crash,restful","Dell,HPE (Hewlett Packard),Supermicro","Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",Mix of yes and no,"40 Gb/s Ethernet,100 Gb/s Ethernet",IPv4 only,,"Single,Dual",Switching from dual (intel) to single amd,1000 to 3000,Both Filestore and BlueStore,"Replication (3x),Erasure coding","Kubernetes,KVM / QEMU",Yes,Yes,No,Production,Cloud,"Yes, to migrate volumes between clusters","librbd (e.g., in combination with qemu)",Yes,Production,Cloud,12,-1,"S3,RGW admin API",Other (Please specify) - we're a cloud. Customers can use any library,RGW (built-in),"HAproxy,Varnish",0,"Use case:I believe it would, especially if it allows to cache non-public objects. We have not updated to Octopus yet so it hasn't been evaluated",No,,,,,,,,,,,,,,,,,,,0 (Detractor),No (Please specify) - we don't use the dashboard. This section shouldn't mandatory,,,,,,,,,,,,we don't use the dashboard. This section shouldn't mandatory,"Prometheus,Grafana (custom),node_exporter,ceph_exporter","Chef,Ansible (other / homebrew)",
Between 1-2 years,"Open source,Scalability,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,5,4,0,0,4,0,2,,5,5,5,3,3,5,4,4,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",8 (Passive),,,Germany,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),Yes,"Out of curiosity,To inform my decisions about when or whether to upgrade",1,265,265,151,Nautilus (14.x),Upstream packages,Ubuntu,No,Longer,I delay upgrades due to concerns about regression,Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","iostat,pg_autoscaler,Other (Please specify) - dashboard, prometheus",Supermicro,AMD Epyc,"SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,AOC / Fiber,IPv4 only,Add In Cards,Single,Bad experience with NUMA configurations in the past in multi socket systems,100 to 500,BlueStore,"Replication (3x),Erasure coding","OpenStack,KVM / QEMU",Yes,No,Yes,Production,"Cloud,Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",No,,,,,,,,,,,,,,,,,,,,Production,"Cloud,Containers,Virtualization,Other (Please specify) - Hosting of website data","Linux kernel CephFS mount,ceph-fuse,libcephfs",1 - 5,4G - 15G,1,Unknown,No,Yes,,8 (Passive),No (Please specify) - We do not make changes on the fly but use ansible to make them to guarantee review and audit of changes.,5,4,4,0,0,0,0,0,0,0,0,.,"Ceph Dashboard,Prometheus,Grafana (custom),ceph_exporter",Ansible (ceph-ansible),
More than 5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity,Performance",Commercial,4,3,3,0,0,0,4,,4,4,4,4,3,4,4,3,"Mailing list,Reporting issues via the bug tracker",9 (Promoter),it works,nativ QoS feature like limit requests per second/minute etc. per Uses for S3 / RGW,Germany,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.",,,"No, I didn't realize they existed",,3,180,360,120,"Luminous (12.x),Jewel (10.x),Nautilus (14.x)","Upstream packages,Distribution packages",Debian,"Yes, for performance reasons",Within a month after release,I only upgrade as needed to address issues,Longer,I delay upgrades due to the effort required to upgrade,I don't know / default set,Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe",Defaults / I don't know,No,10 Gb/s Ethernet,IPv6 only,"M.2,U.2 (2.5-inch)","Single,Dual",,10 to 100,Both Filestore and BlueStore,Replication (3x),"OpenNebula,KVM / QEMU",Yes,Yes,No,Production,Cloud,"No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Commercial,5,5,1,1,5,1,5,,3,3,2,2,2,5,3,5,"IRC / Slack / etc,Mailing list,Contributing code,Ceph events / conferences,Member of the Ceph Foundation",10 (Promoter),,,That would be Telling :p,"Yes, telemetry is enabled on some of my clusters",,,,,,20,5000,1,1,"Luminous (12.x),Nautilus (14.x)","Upstream packages,Distribution packages,We build our own packages",Debian,"Yes, in front of EC pools for functionality",Within a month after release,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Within a year,I delay upgrades due to concerns about stability,"balancer,crash,diskprediction,iostat,pg_autoscaler",SoftIron,"ARM,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Mix of yes and no,"10 Gb/s Ethernet,40 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet,RJ45 (Copper)",IPv4 only,"M.2,U.2 (2.5-inch)","Single,Dual",,1000 to 3000,"Filestore (XFS, EXT4, BTRFS),BlueStore","Replication (2x),Replication (3x),Erasure coding","OpenStack,Kubernetes,VMware,Microsoft Windows,KVM / QEMU,KRBD directly on Linux systems",Yes,Yes,Yes,Production,"HPC,Cloud,Containers,Virtualization","Yes, for DR","librbd (e.g., in combination with qemu),Linux kernel RBD,Other (Please specify)",Yes,Production,"Cloud,Backup",5,1,"S3,Swift,RGW admin API,RGW data caching and CDN","Boto3,s3cmd / s3tools,custom / in-house","RGW (built-in),Keystone","HAproxy,Nginx,DNS round-robin,Varnish",2,Use case:,Yes,1,Yes,1,Global,"ElasticSearch Sync Module,Cloud Sync Module,PubSub Module",,Amqp,,Production,"Scratch,Build,Log,Big Data & Analytics,HPC,Home Directories,Archive Storage","Linux kernel CephFS mount,ceph-fuse,libcephfs,NFS (nfs-ganesha),CIFS (SMB, Samba)",6 - 50,16G - 31G,5,Yes,Yes,Yes,"Inline Data,Multiple File Systems within a Ceph Cluster,LazyIO",8 (Passive),No (Please specify) - cli warrior,3,3,3,3,4,4,4,3,4,4,3,1,"Prometheus,Grafana (custom),ceph_exporter","Ansible (ceph-ansible),Salt / DeepSea,ceph-deploy,Rook,SoftIron Storage Manager,cephadm",
More than 5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity","Commercial,Government",5,4,0,0,0,0,3,,5,3,5,3,0,3,4,4,Mailing list,10 (Promoter),,,Turkey,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I didn't realize they existed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Less than 1 year,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Academic,5,5,0,0,0,0,0,,5,5,5,5,5,5,5,5,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),"Great piece of software, great community around it!",,Spain,"No, telemetry is not enabled on any clusters",I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,,,"No, I knew they existed by have not used them",,1,1400,1400,600,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Never (clusters remain on major release they were deployed with),Other (Please specify) - Interoperability with the rest of our environment,"balancer,crash,devicehealth,diskprediction,iostat,pg_autoscaler,restful",Dell,Intel Xeon,"SSD (SATA, SAS)",Containerized daemons,Yes,100 Gb/s Ethernet,IPv4 only,,Dual,Best balance between cost and performance,100 to 500,BlueStore,Replication (2x),OpenStack,Yes,No,Yes,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",No,,,,,,,,,,,,,,,,,,,,Production,"Scratch,Cloud,Virtualization","Linux kernel CephFS mount,libcephfs",51 - 100,2G - 3G,1,No,No,Yes,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,5,5,5,5,5,5,,,5,,,None,"Ceph Dashboard,Prometheus,Grafana (custom),node_exporter",cephadm,
Between 1-2 years,"Open source,Integration with adjacent technologies,Other (Please specify)",Commercial,5,4,3,3,3,2,4,,4,4,4,5,5,5,4,4,Mailing list,4 (Detractor),,,United States,"Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted)",I enabled 'ident' and configured a cluster description and/or email address,"No, I didn't realize they existed",,5,45,56,78,Octopus (15.x),Distribution packages,SUSE,"Yes, for performance reasons",Within a week after release,I apply upgrades quickly because they require little effort,Within half a year of release,I upgrade quickly to get any available bug fixes or security fixes,devicehealth,"Supermicro,We build our own","Intel Xeon,Intel (other)","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt)",Mix of yes and no,"1000 Mb/s Ethernet (GigE, gigabit)",IPv4 only,"M.2,Add In Cards",Quad,,100 to 500,"Filestore (XFS, EXT4, BTRFS)",Replication (2x),"Kubernetes,VMware,Microsoft Windows",No,No,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4 (Detractor),Yes,4,3,4,3,3,4,4,3,3,3,4,N,"Ceph Dashboard,Other (Please specify) - Homemade","Chef,Ansible (other / homebrew)",
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Performance",Commercial,4,3,0,0,2,2,3,,5,0,0,0,0,4,2,0,,10 (Promoter),Debuggability.,"Event-stream for cephfs (inotify-ish, but without tagging individual files) Robust automatic-ish OSD balancing That stat on a file in cephfs doesn't block while another process is writing to it (makes ls -l in a busy directory hang)","Denmark,India,Japan,United States","No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I didn't realize they existed",,5,3000,4000,1333,"Octopus (15.x),Nautilus (14.x)","Upstream packages,Distribution packages",Ubuntu,No,Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues",Longer,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability",I don't know / default set,"Dell,Supermicro","Intel Xeon,AMD Epyc,AMD (other)","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",No,"40 Gb/s Ethernet,25 Gb/s Ethernet",IPv4 only,"U.2 (2.5-inch),E1.L (EDSFF, ruler form factor)","Dual,Quad","I/O throughput (more disks per node), servers dual/quad socket servers can be swapped from HPC nodes to OSD nodes in an emergency.",500 to 1000,BlueStore,"Replication (3x),Erasure coding","KVM / QEMU,KRBD directly on Linux systems",Yes,Yes,Yes,Production,"Build,Log,Virtualization,Home Directories","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,Big Data & Analytics,16,1,"S3,NFS re-export of S3 buckets",Boto3,RGW (built-in),None,0,Use case:No,No,,,,,,,,,Production,"Scratch,Log,Big Data & Analytics,HPC,Home Directories","Linux kernel CephFS mount,NFS (nfs-ganesha)",2500+,128G+,2,No,Yes,No,,3 (Detractor),"No (Please specify) - Data for long-term trending goes into a dedicated system for that. For debugging, error-finding, CLI tools are good enough, and thus preferred.",1,1,1,0,0,0,0,0,0,0,0,Nothing,"Prometheus,Grafana (custom),Nagios/icinga,Other (Please specify) - munin","Ansible (ceph-ansible),Ansible (other / homebrew)",
Less than 1 year,"Open source,Feature set,High availability,Data durability / reliability / integrity","Commercial,Personal",5,2,0,0,3,0,4,,5,3,3,4,2,3,3,3,IRC / Slack / etc,8 (Passive),,,United States,"No, telemetry is not enabled on any clusters",I do not believe that the Ceph community should be collecting any of this information,,,"No, I didn't realize they existed",,1,48,48,16,Nautilus (14.x),Distribution packages,Debian,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a year,I only upgrade as needed to address specific bugs or needed features,I don't know / default set,"Supermicro,Raspberry Pi,We build our own","ARM,Intel Xeon,AMD (other)","HDD (SATA, SAS)","All-in-one OSD (default, colocated, no separate metadata device)",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet,RJ45 (Copper),DAC (Copper)",Dual stack,,Single,,10 or fewer,BlueStore,"Replication (2x),Replication (3x)","KVM / QEMU,KRBD directly on Linux systems",No,No,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0 (Detractor),No (Please specify) - I don't use it,0,0,0,0,0,0,0,0,0,0,0,I don't use it,"InfluxDB,Grafana (custom),Telegraf",Other (Please specify) - shell scripts,
Between 2-5 years,"Open source,Scalability,Cost,Security,High availability,Data durability / reliability / integrity,Performance",Commercial,3,3,1,0,1,4,5,,5,1,4,4,2,3,3,3,,8 (Passive),"It's mostly smooth sailing but when bugs pop up at scale they can be terrifying for weeks and months on end, we have 4PB of other people's data in it :) that said, I don't know of anything that does better at the petabyte scale","Better handling of billions of small objects without the need to use nvmes, though I don't know how... something like a SeaweedFS-like storage layer? Reducing bluefs minimum allocation per-object size on HDDs would be great if possible, great job on getting that down to 4k on SSDs :) More insight/documentation into the deep internals / inner workings",United States,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,Yes,Out of curiosity,2,4100,4130,1964,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Longer,"I delay upgrades due to concerns about regression,Other (Please specify) - Seems like we're always expanding/backfilling",Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,pg_autoscaler",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,10 Gb/s Ethernet,IPv4 only,,No preference / beats me,Ends up being influenced by market considerations,100 to 500,BlueStore,"Replication (3x),Erasure coding","OpenStack,KVM / QEMU",Yes,Yes,Yes,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,Production,Cloud,28,5000,"S3,RGW admin API","Boto,Boto3,s3cmd / s3tools,Other (Please specify) - minio-client",RGW (built-in),"Nginx,Other (Please specify) - BGP load balancing with exabgp (ECMP)",0,"Use case: Yes, feeding larger CDNs. Haven't seen this yet, might use it.",No,,,,,,,,,Production,Cloud,"ceph-fuse,NFS (nfs-ganesha),CIFS (SMB, Samba)",6 - 50,65G - 127G,1,No,No,Yes,LazyIO,5 (Detractor),"No (Please specify) - All I use it for is Ganesha, but I'm on Nautilus",0,0,0,0,0,0,0,5,1,0,0,"I kinda miss the read-only dashboard from Luminous, I'd check that from my phone over VPN when I got an alert. Didn't need authentication because it was read-only, now I usually just do ceph -s instead","Prometheus,Nagios/icinga,node_exporter","ceph-deploy,Ansible (other / homebrew)",
Between 2-5 years,"Open source,High availability",Commercial,5,5,5,5,5,0,5,,0,0,0,0,0,5,0,0,"IRC / Slack / etc,Mailing list,Contributing code,Ceph events / conferences",10 (Promoter),,,China,"No, telemetry is not enabled on any clusters",Other (Please specify),,,"No, I didn't realize they existed",,3,500,2000,1000,"Luminous (12.x),Nautilus (14.x)","Distribution packages,We build our own packages","Ubuntu,RHEL / CentOS / Fedora,Other OS","Yes, for performance reasons",Longer,I only upgrade as needed to address issues,Within a year,I upgrade quickly to get new features,balancer,"Huawei,Intel","ARM,Intel Xeon,AMD Epyc","HDD (SATA, SAS),NVMe","LVM (ceph-volume),bcache",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet",IPv4 only,"U.2 (2.5-inch),Add In Cards",Dual,,100 to 500,BlueStore,"Replication (3x),Erasure coding",None,Yes,Yes,Yes,"Development / Testing,Proof of Concept (PoC)","Cloud,Virtualization","No, it is not compatible with my RBD feature requirements","librbd (e.g., in combination with qemu),Linux kernel RBD",Unknown,"Development / Testing,Proof of Concept (PoC)","Video,Big Data & Analytics",8,4,S3,"Boto3,s3cmd / s3tools",RGW (built-in),None,0,Use case:,No,,,,,,,,,"Development / Testing,Proof of Concept (PoC)","Build,Big Data & Analytics,HPC","Linux kernel CephFS mount,NFS (Kernel NFS server)",51 - 100,4G - 15G,60,Yes,No,No,"Inline Data,Mantle (Programmable Metadata Balancer),LazyIO",10 (Promoter),Yes,5,5,5,5,5,5,5,5,5,5,5,-,"Ceph Dashboard,Prometheus","Ansible (ceph-ansible),ceph-deploy,Rook,cephadm",
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,3,4,0,0,2,1,1,,5,3,4,4,4,5,5,3,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",9 (Promoter),,,United States,"No, telemetry is not enabled on any clusters",My cluster(s) are running a Ceph version older than Luminous,,,"No, I didn't realize they existed",,1,324,324,108,Infernalis (9.x) or older,Distribution packages,Ubuntu,No,Never,I delay upgrades because of the effort involved,Never (clusters remain on major release they were deployed with),I delay upgrades due to the effort required to upgrade,,Dell,Intel Xeon,"HDD (SATA, SAS)",Partitions,No,"1000 Mb/s Ethernet (GigE, gigabit)",IPv4 only,U.2 (2.5-inch),"Single,Dual",,100 to 500,"Filestore (XFS, EXT4, BTRFS)",Replication (3x),"OpenStack,KVM / QEMU",Yes,No,No,Production,"Video,Cloud,Virtualization,Archive Storage","Other (Please specify) - No, my Ceph version is too old","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5 (Detractor),No (Please specify) - My ceph version is too old,0,0,0,0,0,0,0,0,0,0,0,My Ceph version is too old to have the dashboard.,"Graphite,Other (Please specify) - Sensu",Other (Please specify) - CLI / homebrew,
More than 5 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies","Commercial,Government,Military,Academic,Non-profit,Personal",5,5,0,0,5,0,5,,,5,,,,5,5,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Contributing code,Contributing / enhancing documentation",10 (Promoter),I'm all in.,"External KMS for OSD at rest, oppose to config-key or RGWs implementation Self-Healing of common scenarios Better way to combat io impact when primaries are busy(e.g. apply temp overrides to heartbeats if we can explain slowness/deep-scrub... fallback after a period to mark it down)","United Arab Emirates,United States,That would be Telling :p","No, telemetry is not enabled on any clusters",,,,Yes,Other,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Data durability / reliability / integrity",Personal,5,4,0,1,2,2,2,,5,3,5,4,,,,4,"Reporting issues via the bug tracker,Contributing / enhancing documentation",10 (Promoter),,,Germany,"No, telemetry is not enabled on any clusters",I do not believe that the Ceph community should be collecting any of this information,,,,,1,20,20,5,Nautilus (14.x),Vendor packages,RHEL / CentOS / Fedora,No,Longer,I apply upgrades quickly to get any available bug fixes or security fixes,Longer,Other (Please specify) - I currently use Red Hat's container images,pg_autoscaler,Prefer not to say,"Intel (other),AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS)","Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",Dual stack,M.2,Single,price,10 to 100,BlueStore,Replication (3x),"KVM / QEMU,KRBD directly on Linux systems,OpenShift,Other (Please specify) - fuse.ceph",Yes,Yes,Yes,"Development / Testing,Staging,Production,Proof of Concept (PoC)",Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Unknown,"Development / Testing,Staging,Production,Proof of Concept (PoC)",Backup,1,10,"S3,Swift,RGW admin API",s3cmd / s3tools,RGW (built-in),None,0,Use case:,No,,,,,,,,,"Development / Testing,Staging,Production,Proof of Concept (PoC)","Home Directories,Other (Please specify) - OpenShift","Linux kernel CephFS mount,ceph-fuse,NFS (nfs-ganesha)",1 - 5,2G - 3G,1,Unknown,Unknown,Unknown,Multiple File Systems within a Ceph Cluster,9 (Promoter),Yes,5,5,4,3,4,,,,,,2,better view of what OpenShift is doing,"Ceph Dashboard,Other (Please specify) - https://checkmk.com/",Ansible (ceph-ansible),
Between 2-5 years,"Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Commercial,4,5,0,0,0,0,3,,5,5,3,4,4,5,4,4,"Mailing list,Ceph events / conferences",10 (Promoter),"FOSS, block/object/file, kernel support","improved balancer performance/stability dedication to non-cephadm exclusive path some kind of official Zabbix plugin, either mgr or API templat in Zabbix",United States,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),Yes,Out of curiosity,1,682,682,423,Octopus (15.x),Upstream packages,Ubuntu,No,Within a month after release,I delay upgrades due to concerns about regression,Within a year,"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,diskprediction,iostat,pg_autoscaler,restful,Other (Please specify) - rbd_support,influx,zabbix,prometheusdashboard",Supermicro,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",No,"10 Gb/s Ethernet,RJ45 (Copper)",IPv4 only,"M.2,U.2 (2.5-inch),Add In Cards",No preference / beats me,"dual is what I typically have, because standard BOM, but a single AMD Epyc would be preferred if the config works",100 to 500,BlueStore,"Replication (3x),Erasure coding",KRBD directly on Linux systems,Yes,No,Yes,Production,"Scratch,Other (Please specify) - Database volumes","No, it is incompatible with my performance requirements",Linux kernel RBD,Yes,,,,,,,,,,,,,,,,,,,,Production,"Scratch,Build,Log,Big Data & Analytics,HPC,Home Directories,Archive Storage","Linux kernel CephFS mount,ceph-fuse",51 - 100,32G - 64G,1,No,No,Yes,"Multiple File Systems within a Ceph Cluster,LazyIO",6 (Detractor),"No (Please specify) - I've been using the CLI for 5 years for management, config, visibility. Dashboard is having to relearn all of the tooling/locations.",2,2,1,1,1,1,0,0,3,0,0,"I think being able to manipulate the balancer better in the dashboard would be great. You could see what its doing/going to do, and how it will improve things visually?  I think making crush ruleset changes and being able to simulate what changes will do would be helpful for making that less scary.","Ceph Dashboard,InfluxDB,Prometheus,Grafana (custom),Zabbix,Collectd,Telegraf","ceph-deploy,Other (Please specify) - scripts handle most day two operations, which are mostly just adding osd-nodes and ceph-volume in a script handles that pretty painlessly. I plan to use salt to help speed up at least part of this process in the future.",
Between 1-2 years,"Open source,Scalability,Cost,Feature set,Data durability / reliability / integrity",Academic,5,1,0,0,0,0,1,,3,4,3,4,5,4,4,4,,10 (Promoter),,,United States,"No, telemetry is not enabled on any clusters",Enabling telemetry would require review by my organization's security team and I have chosen not request a review,,,"No, I didn't realize they existed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,Scalability,Cost,Data durability / reliability / integrity",Other (Please specify) - Finance,4,5,2,1,5,2,4,,5,0,5,2,2,3,0,5,Mailing list,8 (Passive),,,Sweden,"No, telemetry is not enabled on any clusters","My organization's security policy has reviewed the information reported by telemetry and judged that it should not be shared,I do not believe that the Ceph community should be collecting any of this information",,,"No, I knew they existed by have not used them",,4,2000,8000,1000,Nautilus (14.x),We build our own packages,RHEL / CentOS / Fedora,"Yes, for performance reasons",Longer,"I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Longer,I delay upgrades due to concerns about performance regressions,pg_autoscaler,HPE (Hewlett Packard),Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet,InfiniBand",Dual stack,"U.2 (2.5-inch),E1.L (EDSFF, ruler form factor)",Dual,,100 to 500,Both Filestore and BlueStore,Erasure coding,None,Yes,Yes,No,Staging,"Log,CDN,Cloud,Archive Storage","No, it is not needed",Linux kernel RBD,No,Production,"CDN,Cloud,Backup",20,2,"S3,RGW data caching and CDN","Amazon SDK,s3cmd / s3tools",Keystone,Varnish,3,backup,Yes,6,Yes,3,Per bucket,,,,,,,,,,,,,,,2 (Detractor),No (Please specify) - logs give me all info. Cli faster to work with. Gui only needed for demo/noc use.,0,0,0,0,0,0,0,0,0,0,0,do not use dashboad.,Performance co-pilot,Ansible (other / homebrew),
Between 2-5 years,"Open source,High availability,Performance",Commercial,5,5,0,0,0,0,2,,5,,,,,5,,,,8 (Passive),,,United States,"No, telemetry is not enabled on any clusters",,,,"No, I knew they existed by have not used them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,High availability,Data durability / reliability / integrity",Commercial,4,0,0,0,0,0,2,,2,4,3,2,4,,2,3,,8 (Passive),,,Spain,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I knew they existed by have not used them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,High availability,Integration with adjacent technologies,Other (Please specify) - integration with OpenNebula",Commercial,4,3,1,2,2,3,4,,3,4,3,2,4,3,2,3,"IRC / Slack / etc,Mailing list,Contributing / enhancing documentation",8 (Passive),great community support,diagnosis tools simpler setup dimensioning tools,"Spain,United Kingdom,United States","No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I knew they existed by have not used them",,20,100,610,400,"Octopus (15.x),Mimic (13.x),Luminous (12.x),Nautilus (14.x)",Upstream packages,"Ubuntu,Debian,RHEL / CentOS / Fedora","Yes, in front of EC pools for functionality",Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a month of release,I upgrade quickly to get new features,"balancer,crash,iostat,restful","IBM,HPE (Hewlett Packard),Supermicro,Intel","Intel Xeon,Intel (other),AMD Epyc,AMD (other)","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),bcache",Yes,"100 Mb/s Ethernet,1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet,40 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet",Dual stack,M.2,Quad,more availability,100 to 500,"Filestore (XFS, EXT4, BTRFS)",Replication (2x),OpenNebula,Yes,No,Yes,Production,"Video,Cloud,Containers,Virtualization","Yes, for DR",Linux kernel RBD,Yes,,,,,,,,,,,,,,,,,,,,Production,"Cloud,Containers,Virtualization",Linux kernel CephFS mount,6 - 50,32G - 64G,5,No,Yes,No,Inline Data,8 (Passive),Yes,4,3,4,3,4,3,3,4,3,4,3,integration with OpenNebula,"Ceph Dashboard,Calamari,Prometheus,Collectd",Ansible (ceph-ansible),
Less than 1 year,"Open source,Scalability,Cost,Feature set,High availability","Commercial,Non-profit,Personal",5,3,0,2,0,0,2,,5,4,4,4,3,4,4,4,,10 (Promoter),,,Poland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Scalability,High availability,Data durability / reliability / integrity",Commercial,4,2,0,0,1,5,1,,2,5,2,1,2,5,4,5,Reporting issues via the bug tracker,6 (Detractor),,,Germany,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,Yes,Out of curiosity,4,900,1200,400,Nautilus (14.x),"Distribution packages,Vendor packages",RHEL / CentOS / Fedora,No,Longer,Other (Please specify) - I delay upgrades because of the high risk of bugs within RHCS releases,Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability",crash,"Dell,HPE (Hewlett Packard)",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Yes,10 Gb/s Ethernet,Dual stack,"M.2,U.2 (2.5-inch)",Dual,,100 to 500,BlueStore,"Replication (3x),Erasure coding",None,No,Yes,No,,,,,,"Development / Testing,Staging,Production","Build,Log,Cloud,Containers,Archive Storage,Backup",6,2,"S3,RGW admin API","Amazon SDK,Boto,Boto3,s3cmd / s3tools,custom / in-house",RGW (built-in),"Dedicated custom hardware (Alteon, F5, etc)",0,Use case: didnt try yet,No,,,,,,,,,,,,,,,,,,,8 (Passive),Yes,3,3,3,2,0,0,0,0,0,1,0,none,"Ceph Dashboard,Grafana (custom),node_exporter",Ansible (ceph-ansible),
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,5,5,0,0,0,0,2,,5,2,5,2,2,5,2,4,Mailing list,8 (Passive),I would recommend Ceph to anybody that has the right needs and also has the right knowledge to make it work. That's the main reason I would not recommend it to everybody.,"- NVME or SSD based RBD cache for hypervisor as latency is becoming more and more problematic for high performance applications, like high volume MySQL databases  - Improved performance for RDB disks, for the same reasons. In synchronous transactions, it's almost impossible to go above 2000 read IOPS, which is not enough for high demanding scenarios.  - Ordered RBD Write cache: In the event of a ceph failure as the clients will be able to write to available OSDs but not to failed ones, it may happen that the clients will just write some data to disk but not other data, leading to an inconsistent disk.",Spain,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,3,1265,2697,900,Luminous (12.x),"Distribution packages,We build our own packages",Ubuntu,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability,Other (Please specify) - We have to go through a big QA testing procedure before upgrading",balancer,"Dell,Supermicro",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Yes,40 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,"We use high frecuency CPUs from Intel, to reduce latency",500 to 1000,BlueStore,"Replication (2x),Replication (3x)","OpenStack,Proxmox,KVM / QEMU",Yes,Yes,No,Production,"Cloud,Virtualization,Archive Storage","Other (Please specify) - We have developed a solution which works quite similar to snapshot based rbd-mirror, but we are really interested in rbd-mirror future","librbd (e.g., in combination with qemu)",Yes,Production,Archive Storage,4,1,S3,"s3cmd / s3tools,Other (Please specify) - Openstack cinder backup",RGW (built-in),None,0,"Use case:No, as actually we only use RGW for internal archiving of data, but we are planning on offering RGW services to our customers, and then we'll use caching.",No,,,,,,,,,,,,,,,,,,,5 (Detractor),No (Please specify) - We don't use it yet,,,,,,,,,,,,Cannot reply as we don't use the dashboard,"Prometheus,Zabbix","Ansible (ceph-ansible),Other (Please specify) - We have some custom developed scripts in ansible for deployment and management, as our clusters have evolved from really old versions of ceph and we cannot use the official ones.",
More than 5 years,"Open source,Cost,High availability,Data durability / reliability / integrity,Performance",Commercial,5,4,0,0,0,0,1,,,5,,,5,5,,,Reporting issues via the bug tracker,9 (Promoter),,,Czech Republic,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,3,12,18,36,Infernalis (9.x) or older,Distribution packages,RHEL / CentOS / Fedora,"Yes, for performance reasons",Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades because of the effort involved",Longer,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability",,"Supermicro,We build our own","AMD Epyc,AMD (other)","HDD (SATA, SAS),SSD (SATA, SAS)","Partitions,Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,"M.2,U.2 (2.5-inch)",No preference / beats me,,10 to 100,"Filestore (XFS, EXT4, BTRFS)",Replication (2x),"KVM / QEMU,Other (Please specify) - oVirt",Yes,No,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,Production,Virtualization,Linux kernel CephFS mount,6 - 50,1G,3,Unknown,Yes,Yes,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,5,2,5,3,3,5,,0,3,0,4,I dont know.. :-(,"Ceph Dashboard,Prometheus,Zabbix","Ansible (ceph-ansible),ceph-deploy,Ansible (other / homebrew),cephadm",
Less than 1 year,"Open source,Cost,High availability,Data durability / reliability / integrity,Performance",Commercial,5,1,0,0,0,2,3,,4,4,2,2,1,2,3,5,,8 (Passive),Featureset,,Germany,"No, telemetry is not enabled on any clusters","Enabling telemetry would require review by my organization's security team and I have chosen not request a review,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.",,,"No, I didn't realize they existed",,1,24,24,8,Octopus (15.x),Upstream packages,Ubuntu,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades because there are too many updates",Within a year,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability","devicehealth,pg_autoscaler",Supermicro,Intel Xeon,"SSD (SATA, SAS)","LVM (ceph-volume),At-rest encryption (dmcrypt)",No,"1000 Mb/s Ethernet (GigE, gigabit)",IPv4 only,U.2 (2.5-inch),Dual,price vs performance,10 or fewer,BlueStore,Replication (3x),KVM / QEMU,Yes,No,No,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10 (Promoter),Yes,5,3,4,2,2,1,0,0,0,0,5,Functioning links to the documentation,"Ceph Dashboard,Nagios/icinga",Ansible (ceph-ansible),
Less than 1 year,"Open source,Scalability,Cost,Feature set,High availability","Academic,Non-profit",4,3,0,0,0,0,1,,4,4,3,3,2,4,3,5,,6 (Detractor),,RichACL support Userquota for cephfs,Germany,"No, telemetry is not enabled on any clusters",I do not believe that the Ceph community should be collecting any of this information,,,"No, I didn't realize they existed",,1,140,140,70,Octopus (15.x),Upstream packages,Debian,No,Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy","balancer,devicehealth,pg_autoscaler,I don't know / default set","Dell,Supermicro","Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,"10 Gb/s Ethernet,RJ45 (Copper)",IPv4 only,"U.2 (2.5-inch),Add In Cards",Dual,,10 to 100,BlueStore,Replication (2x),"Proxmox,None",Yes,No,Yes,Production,"Video,Containers,Virtualization,Home Directories,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",No,,,,,,,,,,,,,,,,,,,,Production,"Scratch,Video,Home Directories,Archive Storage","Linux kernel CephFS mount,NFS (Kernel NFS server),CIFS (SMB, Samba)",1 - 5,32G - 64G,2,No,Yes,Yes,"Mantle (Programmable Metadata Balancer),Multiple File Systems within a Ceph Cluster,LazyIO",7 (Passive),No (Please specify) - Long search periods,5,2,2,3,4,1,0,0,4,0,3,Export Samba,Ceph Dashboard,"cephadm,Proxmox",
Between 2-5 years,"Open source,Scalability,High availability,Integration with adjacent technologies","Commercial,Government",4,4,0,0,0,2,0,,2,5,2,2,4,5,2,2,,10 (Promoter),,"SMB + iSCSI gateway as a packaged addon included in CEPH. Would make it easier to push towards ""legacy"" customers and environments.","Denmark,Faroe Islands","Yes, telemetry is enabled on some of my clusters",I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,,I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,4,160,1024,340,"Octopus (15.x),Luminous (12.x),Nautilus (14.x)",Distribution packages,"Debian,RHEL / CentOS / Fedora",No,Longer,I only upgrade as needed to address issues,Within a year,I delay upgrades due to concerns about stability,"balancer,iostat,pg_autoscaler","Dell,HPE (Hewlett Packard),Supermicro,Raspberry Pi","ARM,Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",Yes,"10 Gb/s Ethernet,40 Gb/s Ethernet,25 Gb/s Ethernet,DAC (Copper)",IPv4 only,"M.2,U.2 (2.5-inch)",Single,"Less Numa troubles. Scale out, not up.",100 to 500,BlueStore,"Replication (3x),Erasure coding","VMware,Microsoft Windows,Proxmox,KRBD directly on Linux systems",Yes,Yes,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD,iSCSI (tcmu-runner)",No,Development / Testing,Backup,0,0,"S3,NFS re-export of S3 buckets",custom / in-house,"RGW (built-in),LDAP","HAproxy,Nginx",0,No.,No,,,,,,,,,Production,"Virtualization,Archive Storage","Linux kernel CephFS mount,libcephfs",6 - 50,4G - 15G,3,No,No,No,,6 (Detractor),No (Please specify) - Workflows are today based on cli and ansible,1,0,0,0,,0,0,0,1,1,0,.,"Proxmox,InfluxDB,Prometheus,Ceph-metrics,Grafana (custom),Zabbix","Ansible (ceph-ansible),cephadm,Proxmox",
I don't use Ceph yet,"Open source,Scalability,Feature set,Data durability / reliability / integrity,Performance,Other (Please specify) - erasure coding","Academic,Personal",5,0,0,0,0,0,2,reddit, medium, habr,5,5,5,4,4,5,4,4,,6 (Detractor),,,"Estonia,Russian Federation","No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations",,,"No, I didn't realize they existed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Academic,4,5,2,1,0,2,0,,5,5,5,5,5,5,5,5,Mailing list,10 (Promoter),The Linux of Storage - more truer with every release,orchestration - cephadm / rook self-tuning better defaults,Australia,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,"No, I didn't realize they existed",,3,2500,4666,2358,"Octopus (15.x),Luminous (12.x)",Upstream packages,Ubuntu,No,Within a week after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within half a year of release,I upgrade quickly to get any available bug fixes or security fixes,"balancer,crash,devicehealth,iostat,pg_autoscaler",Dell,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",Mix of yes and no,"10 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet,RJ45 (Copper),DAC (Copper),AOC / Fiber",IPv4 only,U.2 (2.5-inch),Dual,"once high-speed NIC per socket, and to get enough cores for OSDs. AMD EPYC has made this easier with large core count single socket.",100 to 500,Both Filestore and BlueStore,"Replication (3x),Erasure coding","OpenStack,KVM / QEMU",Yes,Yes,Yes,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,Cloud,1,500,S3,s3cmd / s3tools,RGW (built-in),None,0,Use case:,No,,,,,,,,,Production,"Scratch,HPC,Home Directories","Linux kernel CephFS mount,NFS (Kernel NFS server)",101 - 500,16G - 31G,2,No,No,Yes,Multiple File Systems within a Ceph Cluster,10 (Promoter),No (Please specify) - only used for monitoring,5,2,0,0,1,0,0,0,0,0,0,"nice to look at, but mainly use CLI","Prometheus,node_exporter,ceph_exporter","ceph-deploy,cephadm",
More than 5 years,"Open source,Scalability,Cost,Data durability / reliability / integrity","Academic,Non-profit",4,4,,,,5,,,5,,4,4,5,5,5,4,Mailing list,7 (Passive),,,Canada,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,2000,2000,2000,Nautilus (14.x),Vendor packages,SUSE,No,Longer,"I delay upgrades because of the effort involved,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability",balancer,Dell,Intel Xeon,"HDD (SATA, SAS),NVMe",LVM (ceph-volume),No,40 Gb/s Ethernet,IPv4 only,"U.2 (2.5-inch),Add In Cards",Single,moving forward - Single AMD makes sense,100 to 500,BlueStore,Replication (3x),"VMware,Microsoft Windows",Yes,No,No,Production,"Big Data & Analytics,Virtualization,Home Directories,Archive Storage","No, it is not needed",Linux kernel RBD,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8 (Passive),No (Please specify) - still do a lot of command line - testing/tweeking,5,,,,,5,5,,,,,ceph pg ls active  sort -k18 -k19  head -n 20,Ceph Dashboard,Salt / DeepSea,
Less than 1 year,"Open source,Scalability,Cost,High availability,Performance",Commercial,5,1,1,0,3,0,1,,5,4,3,2,1,5,3,4,"IRC / Slack / etc,Reporting issues via the bug tracker",10 (Promoter),,,Germany,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,"No, I didn't realize they existed",,1,18,36,16,Octopus (15.x),Upstream packages,"Debian,Other OS",No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly because it is easy,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,iostat,pg_autoscaler",HPE (Hewlett Packard),Intel Xeon,"SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Yes,"10 Gb/s Ethernet,DAC (Copper)",IPv4 only,U.2 (2.5-inch),Dual,,10 to 100,BlueStore,Replication (2x),"Kubernetes,Proxmox",Yes,No,Yes,Production,Virtualization,"No, it is not needed",Linux kernel RBD,Yes,,,,,,,,,,,,,,,,,,,,Production,"Cloud,Archive Storage",Linux kernel CephFS mount,1 - 5,1G,3,No,No,Yes,,10 (Promoter),Yes,4,2,4,4,4,4,1,1,4,1,1,OSD creation,Ceph Dashboard,ceph-deploy,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Academic,5,4,0,0,3,0,3,,4,4,4,3,2,3,3,3,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",8 (Passive),It depends on their use case. Sometimes all someone needs is a single zfs server.,,United States,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,3,3124,3600,2300,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,Other (Please specify) - I delay because of they haven't been announced there are usually issues",Within a month of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy","balancer,crash,pg_autoscaler","IBM,Dell,Supermicro,QCT (Quanta)",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",No,"10 Gb/s Ethernet,40 Gb/s Ethernet",IPv4 only,"U.2 (2.5-inch),Add In Cards",Dual,,10 to 100,BlueStore,"Replication (3x),Erasure coding","KVM / QEMU,KRBD directly on Linux systems",Yes,No,Yes,Production,Virtualization,"No, it is not compatible with my RBD feature requirements","librbd (e.g., in combination with qemu),Linux kernel RBD",No,,,,,,,,,,,,,,,,,,,,Production,"Scratch,Big Data & Analytics,HPC,Home Directories,Archive Storage",Linux kernel CephFS mount,501 - 2500,32G - 64G,1,No,No,Yes,,7 (Passive),No (Please specify) - I've never tried to use it,0,0,0,0,0,0,0,0,0,0,0,I don't use it,Nagios/icinga,Other (Please specify) - manual,
Less than 1 year,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Academic,5,3,0,0,0,0,0,,5,5,3,4,,3,3,4,Mailing list,4 (Detractor),"Super opaque once you get past the basic deployment described in ""Getting Started"" pages: Difficult to know what to tune, what to do when there are errors, and there are 3-4 ways of administering it (cephadm, Rook, ceph-ansible, and ceph-deploy).","One, clearly-best, way of managing the cluster. Improve understanding/directions what to do when there are errors. Know how different tuning knobs affect the outcome.",Canada,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,Enabling telemetry would require review by my organization's security team and I have chosen not request a review,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.",,,"No, I didn't realize they existed",,1,672,672,500,Octopus (15.x),Upstream packages,Ubuntu,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades because of the effort involved,I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Within half a year of release,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability",I don't know / default set,Supermicro,AMD Epyc,"HDD (SATA, SAS),NVMe","Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Defaults / I don't know",Yes,10 Gb/s Ethernet,IPv4 only,M.2,Single,,100 to 500,BlueStore,Erasure coding,None,No,Yes,Yes,,,,,,Proof of Concept (PoC),"HPC,Archive Storage",3,500,"S3,Swift","s3cmd / s3tools,Other (Please specify) - Still exploring","RGW (built-in),Other (Please specify) - Want to do LDAP eventually",Other (Please specify) - None yet,0,Use case:,No,,,,,,,,,Proof of Concept (PoC),"HPC,Home Directories,Archive Storage",Linux kernel CephFS mount,51 - 100,4G - 15G,1,Unknown,Unknown,Unknown,,4 (Detractor),No (Please specify) - I'm still dependent on the getting started docs and they use the CLI,0,0,0,0,0,0,0,0,0,0,0,I don't know,"Ceph Dashboard,Prometheus,Other (Please specify) - I'd like to use the built-in dashboard eventually","Ansible (ceph-ansible),cephadm",
Between 2-5 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies","Commercial,Non-profit,Personal",5,5,4,3,3,1,3,,4,3,3,3,4,4,3,3,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",10 (Promoter),integration with k8s/openstack private s3 using rgw,rgw event push,"China,Singapore","Yes, telemetry is enabled on some of my clusters",My cluster(s) are on a protected network that does not have access to the internet,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,30,5000,30000,15000,"Mimic (13.x),Luminous (12.x),Nautilus (14.x)",Distribution packages,"Ubuntu,RHEL / CentOS / Fedora",No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades because there are too many updates,I delay upgrades due to concerns about regression",Never (clusters remain on major release they were deployed with),"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability","balancer,crash,iostat,restful","IBM,HPE (Hewlett Packard),Supermicro,Huawei,Other (Please specify) - inspur/ZTE","ARM,Power,Intel Xeon,Intel (other)","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Partitions,Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",Mix of yes and no,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,"M.2,U.2 (2.5-inch)",No preference / beats me,,500 to 1000,BlueStore,"Replication (2x),Replication (3x),Erasure coding","OpenStack,Kubernetes,KVM / QEMU,KRBD directly on Linux systems",Yes,Yes,Yes,"Development / Testing,Production,Proof of Concept (PoC)","Containers,Virtualization","No, it is not compatible with my RBD feature requirements","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,"Development / Testing,Production,Proof of Concept (PoC)","Build,Log,Video,CDN,Big Data & Analytics,AI/ML,Cloud,Archive Storage,Backup",10,3072,"S3,RGW admin API","Amazon SDK,Boto,Boto3,s3cmd / s3tools",RGW (built-in),"Nginx,Dedicated custom hardware (Alteon, F5, etc)",0,Use case:,No,,,,,,,,,Proof of Concept (PoC),"Containers,Virtualization,Archive Storage","Linux kernel CephFS mount,ceph-fuse,libcephfs,NFS (Kernel NFS server)",1 - 5,4G - 15G,1,No,No,No,"Multiple File Systems within a Ceph Cluster,LazyIO",10 (Promoter),Yes,5,3,4,4,4,4,0,1,1,3,3,none,"Ceph Dashboard,Prometheus,Grafana (custom),Zabbix,node_exporter,ceph_exporter","ceph-deploy,Ansible (other / homebrew)",
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Academic,5,3,0,0,2,0,2,debian community and debian bug tracker,5,4,5,5,1,5,1,3,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),Ceph perfectly fits all the very diverse use-cases and workloads of our university. Best thing there is.,"Samba VFS documentation/integration for ""restore previous versions"" thing with Windows clients, when Ceph is exported to them via Samba. It's the killer feature we need to retire the remaining NetApp storage (all the data except the user homes have been moved to Ceph 5+ years ago).",Switzerland,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,My cluster(s) are on a protected network that does not have access to the internet,Other (Please specify) - we'll do the necessary network changes soon, and then be able to send the telemetry.",,,"No, I knew they existed by have not used them",,1,4200,4200,1400,Octopus (15.x),We build our own packages,Debian,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a month of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy",pg_autoscaler,Supermicro,Intel Xeon,"HDD (SATA, SAS),NVMe",LVM (ceph-volume),Yes,10 Gb/s Ethernet,IPv6 only,U.2 (2.5-inch),Dual,"in Switzerland, rackspace is the most expensive cost. Therefore, overall it's cheaper do 2by systems allowing us to build denser systems",500 to 1000,BlueStore,Replication (3x),"OpenStack,KRBD directly on Linux systems",Yes,Yes,Yes,Production,"Big Data & Analytics,HPC,Containers,Virtualization,Home Directories,Archive Storage","No, it is not needed",Linux kernel RBD,Yes,Proof of Concept (PoC),"Cloud,Virtualization",0,0,"S3,Swift",s3cmd / s3tools,LDAP,HAproxy,0,"sounds usefull, but don't know yet",No,,,,,,,,,Production,"Big Data & Analytics,HPC,Containers,Home Directories,Archive Storage",Linux kernel CephFS mount,101 - 500,65G - 127G,9,Yes,Yes,No,Multiple File Systems within a Ceph Cluster,8 (Passive),"No (Please specify) - good for visualisation, but we operate by CLI",4,4,4,0,0,0,0,0,0,0,0,"just a note: we do monitoring seperatly, while I see the point of having it integrated, we have no use for it, so nothing important to have for us in ceph itself.","Ceph Dashboard,InfluxDB,Nagios/icinga,Telegraf","ceph-deploy,Ansible (other / homebrew),cephadm",
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies,Other (Please specify) - no vendor lockin for hardware","Commercial,Academic",5,2,0,0,3,0,4,,5,2,5,5,5,2,3,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Contributing / enhancing documentation,Member of the Ceph Foundation",10 (Promoter),,- QoS for client access - write operations to all OSDs - not just primary - public accessible DB with Telemetry / Device SMART data to provide cloud disk failure prediction / performance comparison / ...,"Germany,Singapore,Switzerland,United States","Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,Yes,Out of curiosity,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Less than 1 year,"Open source,Scalability,Cost,Feature set,Data durability / reliability / integrity",Academic,5,4,2,1,0,0,0,u-tube videos,5,4,4,4,4,4,4,5,"IRC / Slack / etc,Mailing list",9 (Promoter),,,United Kingdom,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,1600,1600,1000,Octopus (15.x),"Upstream packages,Distribution packages",RHEL / CentOS / Fedora,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about regression",Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about stability","balancer,devicehealth,Other (Please specify) - still playing","IBM,Lenovo,Dell,Raspberry Pi,Intel","ARM,Intel Xeon","HDD (SATA, SAS),SSD (SATA, SAS)",Containerized daemons,Mix of yes and no,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet,InfiniBand,RJ45 (Copper)",Dual stack,U.2 (2.5-inch),Dual,"Come from an HPC background,",100 to 500,BlueStore,"Replication (3x),Erasure coding","OpenShift,rbd-nbd,Other (Please specify) - Still Learning",Yes,Yes,Yes,Development / Testing,"Scratch,Virtualization","No, it is not needed","Linux kernel RBD,iSCSI (LIO with /dev/rbd)",Unknown,Development / Testing,Other (Please specify) - Still Playing,0,0,S3,"custom / in-house,Other (Please specify) - Spectrum Scale","RGW (built-in),LDAP","Nginx,Other (Please specify) - Still Playing",0,Use case:,No,,,,,,,,,"Development / Testing,Staging","Scratch,Video,Big Data & Analytics,HPC,Virtualization,Home Directories,Archive Storage","Linux kernel CephFS mount,ceph-fuse,NFS (Kernel NFS server),CIFS (SMB, Samba)",501 - 2500,4G - 15G,4,No,No,Yes,"Inline Data,Multiple File Systems within a Ceph Cluster,LazyIO",9 (Promoter),Yes,5,4,4,5,5,4,4,3,5,4,4,Stable Config control via Git which is possible via Commandline.,"Ceph Dashboard,Nagios/icinga,Other (Please specify) - Ganglia","Puppet,cephadm,Other (Please specify) - Manual / Git",
Less than 1 year,"Feature set,High availability",Commercial,5,0,0,0,0,0,0,stackoverflow.com serverfault.com,5,3,2,3,5,3,0,4,IRC / Slack / etc,7 (Passive),CEPH is quite hard to maintain and keep healthy.,,Russian Federation,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,3,10,30,10,"Octopus (15.x),Nautilus (14.x)",Distribution packages,Ubuntu,No,Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues",Within a year,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability",I don't know / default set,"Lenovo,Dell,Supermicro,Huawei,We build our own",AMD Epyc,"SSD (SATA, SAS),NVMe",Containerized daemons,No,10 Gb/s Ethernet,IPv4 only,M.2,Dual,We are using dual sockets because it's quite standard for our vendor.,10 or fewer,BlueStore,Replication (3x),"KVM / QEMU,Other (Please specify) - Docker Swarm",Yes,No,Yes,Development / Testing,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,Production,"Scratch,Cloud,Containers",Linux kernel CephFS mount,1 - 5,2G - 3G,5,No,No,No,LazyIO,2 (Detractor),Yes,,4,5,,,,,,,,,-,Ceph Dashboard,cephadm,
Between 2-5 years,"Open source,Scalability,Cost,Data durability / reliability / integrity",Commercial,4,2,0,4,1,0,3,,5,4,5,4,5,5,5,4,"IRC / Slack / etc,Reporting issues via the bug tracker",9 (Promoter),,,"Germany,Russian Federation","No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I knew they existed by have not used them",,2,300,200,400,Mimic (13.x),Distribution packages,"Ubuntu,Debian",No,Longer,I delay upgrades due to concerns about regression,Longer,"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability",balancer,"Dell,HPE (Hewlett Packard)",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),Mix of yes and no,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,,100 to 500,BlueStore,Replication (3x),KRBD directly on Linux systems,Yes,Yes,No,"Development / Testing,Production","Video,Archive Storage","No, it is not needed",Linux kernel RBD,No,"Development / Testing,Production",CDN,5,10,"S3,RGW admin API","Amazon SDK,s3cmd / s3tools,custom / in-house",RGW (built-in),Nginx,0,Use case:,No,,,,,,,,,,,,,,,,,,,5 (Detractor),No (Please specify) - don't using it,0,0,0,0,0,0,0,0,0,0,0,-,"Prometheus,Grafana (custom),node_exporter","Ansible (ceph-ansible),ceph-deploy",
Between 2-5 years,"Open source,Scalability,Feature set,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,3,4,0,0,0,4,4,,5,3,4,3,4,3,3,4,"Mailing list,Reporting issues via the bug tracker",9 (Promoter),,,Belgium,"No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I didn't realize they existed",,9,400,1600,440,Nautilus (14.x),Vendor packages,RHEL / CentOS / Fedora,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a month of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy","balancer,iostat,pg_autoscaler",Dell,AMD Epyc,"HDD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",No,25 Gb/s Ethernet,IPv4 only,Add In Cards,Single,avoid more than 1 NUMA node. Epyc rules,10 to 100,BlueStore,Replication (3x),"Kubernetes,KRBD directly on Linux systems,OpenShift",Yes,Yes,No,Production,Containers,"Other (Please specify) - not yet, will use in the future",Linux kernel RBD,Yes,"Development / Testing,Staging,Production","Cloud,Containers,Archive Storage,Backup",4,100,S3,"Amazon SDK,Boto3,s3cmd / s3tools,custom / in-house",RGW (built-in),"Dedicated custom hardware (Alteon, F5, etc),Other (Please specify) - LVS",2,Use case:,Yes,1,Yes,2,Global,,,"Amqp,Http",,,,,,,,,,,,6 (Detractor),No (Please specify) - We use GUI read only. Changes are being done via ansible + gitops.  We do however use the dashboard API to automate RGW things,5,0,0,0,0,0,0,0,0,4,0,N/A,"Ceph Dashboard,Prometheus,Grafana (custom),node_exporter,ceph_exporter",Ansible (other / homebrew),
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Commercial,5,0,0,0,0,0,4,,5,5,4,2,2,3,3,4,"Reporting issues via the bug tracker,Contributing code",8 (Passive),"Its the best scalable storage system that i've used, however its not for everyone due to its complexity.","Automatic user provisioning in the dashboard using LDAP, or allowing user creation via SAML if certain groups exist in the assertion.","Netherlands,Narnia","Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted),crash: information about daemon crashes, including the type of daemon, ceph version, and stack trace",I enabled 'ident' and configured a cluster description and/or email address,"No, I knew they existed by have not used them",,2,35,56,18,Octopus (15.x),Distribution packages,Debian,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort,I delay upgrades due to concerns about regression",Within half a year of release,I delay upgrades due to concerns about stability,"balancer,crash,devicehealth,diskprediction,pg_autoscaler",Supermicro,"Intel Xeon,AMD Epyc","SSD (SATA, SAS)","All-in-one OSD (default, colocated, no separate metadata device)",Yes,"25 Gb/s Ethernet,100 Gb/s Ethernet",IPv4 only,,Single,"A single socket CPU with 24 threads is used in our setup for a server with 24 bays, we expect this to be enough, but time will tell.",10 to 100,BlueStore,Replication (3x),"Proxmox,KVM / QEMU",Yes,No,Yes,Production,"Cloud,Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,Production,"Scratch,CDN,Home Directories","ceph-fuse,NFS (nfs-ganesha),CIFS (SMB, Samba)",101 - 500,1G,62,No,No,No,,7 (Passive),Yes,5,4,4,4,2,2,0,0,0,0,0,Automated keepalived / haproxy setup Automated user provisioning from LDAP / SAML Provisioning of new nodes and OSDs Creating CephFS volumes,"Ceph Dashboard,Nagios/icinga","Puppet,cephadm",
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Academic,2,0,0,0,0,2,0,google,5,4,5,1,1,2,3,2,,10 (Promoter),Scales very well,"Alerting when a node goes down, or osd fails.",New Zealand,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,120,120,60,"Octopus (15.x),Nautilus (14.x)",Vendor packages,Debian,No,Longer,"Other (Please specify) - I apply them when I get around to it, unless I'm addressing a problem, then I apply them quickly",Within half a year of release,I delay upgrades due to concerns about stability,balancer,Other (Please specify) - Generic pc,"Intel (other),AMD (other)","HDD (SATA, SAS)",Defaults / I don't know,Yes,"1000 Mb/s Ethernet (GigE, gigabit)",IPv4 only,,No preference / beats me,"Just use old desktops as osd nodes, so use whatever is in them",10 to 100,BlueStore,Replication (3x),KRBD directly on Linux systems,Yes,No,No,Production,Other (Please specify) - Backup,"No, it is not needed",Linux kernel RBD,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5 (Detractor),Yes,5,1,1,2,1,1,0,0,1,0,1,Nothing,croit,Croit,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Academic,5,4,1,5,0,0,0,,5,5,3,3,3,3,4,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",10 (Promoter),"As long as you have staff that can wrap their heads around Ceph, and know how to respect its quirks, it's the most amazing storage solution ever conceived. :)",accurate and authoritative documentation simplified deployments and management,New Zealand,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,3200,3200,1066,Nautilus (14.x),Upstream packages,Ubuntu,No,Longer,I delay upgrades because of the effort involved,Longer,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,pg_autoscaler,restful,Other (Please specify) - prometheus",Dell,Intel Xeon,"SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet",IPv4 only,Add In Cards,Single,numa crossover concerns/complications,100 to 500,BlueStore,"Replication (3x),Erasure coding",OpenStack,Yes,Yes,Yes,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",No,Production,"Cloud,Archive Storage",2,100,"S3,Swift,NFS re-export of S3 buckets","Boto,s3cmd / s3tools,custom / in-house","RGW (built-in),Keystone",HAproxy,0,Use case:,No,,,,,,,,,Production,"Cloud,Home Directories,Archive Storage","Linux kernel CephFS mount,NFS (nfs-ganesha)",6 - 50,128G+,1,Unknown,Yes,Yes,"Multiple File Systems within a Ceph Cluster,LazyIO",6 (Detractor),"No (Please specify) - on Nautilus at least, it's not that useful",5,2,4,0,0,0,0,0,0,0,0,"I'd like ONE dashboard for metrics and management, instead of switching back and forth between prometheus, Ceph dash, and ganglia (in our case).","Ceph Dashboard,Prometheus,Grafana (custom),Nagios/icinga","Puppet,ceph-deploy",
Between 2-5 years,"Open source,Scalability,High availability",Personal,5,3,0,0,0,0,0,,3,4,3,5,2,5,4,3,"Mailing list,Reporting issues via the bug tracker",5 (Detractor),scaleout commodity hardware,Parallel like Luster improve data loss prevention multi site replication compression and deduplication,Algeria,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I didn't realize they existed",,2,100,300,100,Nautilus (14.x),Distribution packages,"Ubuntu,RHEL / CentOS / Fedora",No,Longer,"I delay upgrades because there are too many updates,I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Longer,"I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability",I don't know / default set,HPE (Hewlett Packard),Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Containerized daemons",Yes,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,24 OSDs per server need more CPUs cores,10 to 100,BlueStore,"Replication (3x),Erasure coding",OpenStack,Yes,Yes,Yes,Production,"Big Data & Analytics,HPC,Cloud,Virtualization","No, it is not needed",Linux kernel RBD,Yes,Production,"Archive Storage,Backup",1,10000,Swift,None,RGW (built-in),None,1,Use case: CDN,No,,,,,,,,,Production,"HPC,Home Directories","Linux kernel CephFS mount,NFS (nfs-ganesha)",1 - 5,1G,1,No,No,No,Multiple File Systems within a Ceph Cluster,5 (Detractor),Yes,5,4,4,5,5,5,0,3,3,3,2,Configuration,"Ceph Dashboard,Zabbix","Ansible (ceph-ansible),ceph-deploy,cephadm",
Between 2-5 years,"Open source,Cost,Feature set,High availability,Data durability / reliability / integrity","Commercial,Personal",5,3,3,0,4,0,2,,5,3,4,3,1,5,4,4,IRC / Slack / etc,10 (Promoter),The Rook Ceph Operator makes Ceph really compelling,,United Kingdom,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),Yes,Out of curiosity,2,5,10,3,Octopus (15.x),Upstream packages,Debian,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort,Other (Please specify) - When I get around to it",Within a month of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy","balancer,crash,devicehealth,iostat,pg_autoscaler,restful","Supermicro,QCT (Quanta)",Intel Xeon,"SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",No,10 Gb/s Ethernet,Dual stack,"M.2,U.2 (2.5-inch)",Dual,"More cores in a ""hyper-converged"" cluster, reasonably cost effective",10 or fewer,BlueStore,Replication (3x),"Kubernetes,KVM / QEMU",Yes,No,Yes,"Staging,Production","Build,Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,"Staging,Production",Containers,Linux kernel CephFS mount,6 - 50,1G,1,No,No,No,,8 (Passive),No (Please specify) - I'm comfortable with the CLI,1,0,1,1,1,1,0,0,1,0,0,N/A,"Prometheus,Grafana (custom),node_exporter",Rook,
More than 5 years,"Open source,Cost,Feature set,Data durability / reliability / integrity,Integration with adjacent technologies","Academic,Other (Please specify) - Research",4,5,0,1,1,2,0,,5,5,4,5,4,4,4,5,"Mailing list,Ceph events / conferences",10 (Promoter),,,Italy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity","Commercial,Government",5,4,0,0,0,0,3,stackoverflow,2,2,3,3,2,4,4,3,"Reporting issues via the bug tracker,Ceph events / conferences",10 (Promoter),,,Germany,"Yes, telemetry is enabled on some of my clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,I left 'ident' disabled (the default),Yes,Out of curiosity,3,988,1500,400,"Octopus (15.x),Nautilus (14.x)",Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a year,I delay upgrades due to the effort required to upgrade,"balancer,devicehealth,iostat,pg_autoscaler","Dell,HPE (Hewlett Packard),Huawei","Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet",IPv4 only,U.2 (2.5-inch),No preference / beats me,,100 to 500,BlueStore,"Replication (3x),Replication (4 or more copies)","VMware,KVM / QEMU,KRBD directly on Linux systems,OpenShift",Yes,Yes,Yes,Production,"Big Data & Analytics,Containers,Virtualization","No, it is not needed","Linux kernel RBD,iSCSI (tcmu-runner)",Yes,"Development / Testing,Production","Big Data & Analytics,Cloud,Backup",3,5,"S3,RGW admin API,NFS re-export of S3 buckets","Amazon SDK,s3cmd / s3tools",RGW (built-in),HAproxy,0,Use case:,No,,,,,,,,,Production,"Virtualization,Archive Storage","Linux kernel CephFS mount,NFS (nfs-ganesha)",6 - 50,16G - 31G,3,No,Yes,No,"Inline Data,Mantle (Programmable Metadata Balancer)",6 (Detractor),No (Please specify) - I'm unfamiliar with the dashboard. Using CLI out of habit,3,0,2,0,0,0,0,0,0,0,3,none,"Ceph Dashboard,Prometheus","ceph-deploy,cephadm",
Between 1-2 years,"Open source,High availability,Data durability / reliability / integrity",Commercial,5,3,0,0,2,0,3,,5,0,2,1,1,3,2,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",10 (Promoter),,"Ability to create more complex CRUSH rules like ""pick 3 hosts, then pick 2 disks from each of those hosts"" without having to edit the CRUSH map manually Better stretched cluster support (mons keeping track of other mons' view of the network)",Iceland,"No, telemetry is not enabled on any clusters","My organization's security policy does not allow storage systems to communicate with vendors or external organizations,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,Other (Please specify) - Accepting telemetry means accepting including whatever information future versions may decide to share. I don't check the data every version so the safe choice is to disable telemetry, as I do with all software.",,,"No, I didn't realize they existed",,3,271,336,150,Octopus (15.x),"Distribution packages,Vendor packages","Debian,FreeBSD/OpenBSD/NetBSD",No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a year,I delay upgrades due to concerns about stability,"balancer,crash,devicehealth,iostat,pg_autoscaler,restful,Other (Please specify) - prometheus,zabbix,alerts,diskprediction_local",Supermicro,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Mix of yes and no,"10 Gb/s Ethernet,DAC (Copper),AOC / Fiber",Dual stack,"M.2,U.2 (2.5-inch)",No preference / beats me,,10 to 100,BlueStore,"Replication (3x),Erasure coding","Proxmox,KVM / QEMU",Yes,No,Yes,"Development / Testing,Staging,Production","Scratch,Build,Log,Video,Virtualization,Home Directories,Archive Storage","Yes, for DR","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,Production,Other (Please specify) - Non-critical ISO storage for virtualization. We're still testing it out for more serious deployments.,ceph-fuse,1 - 5,2G - 3G,3,No,No,No,Multiple File Systems within a Ceph Cluster,0 (Detractor),No (Please specify) - I've already got a set of monitoring tools (grafana and zabbix) and don't want a ceph-specific monitoring interface.,,,,,,,,,,,,Don't use it.,"Proxmox,Prometheus,Grafana (custom),Zabbix,node_exporter,ceph_exporter","Ansible (ceph-ansible),Ansible (other / homebrew),Proxmox,Other (Please specify) - Nix and NixOS",
Between 2-5 years,"Open source,Cost,Feature set,High availability,Data durability / reliability / integrity","Commercial,Personal",5,3,0,0,0,0,2,Other blogs and writeups.,5,2,2,4,3,2,3,4,"Mailing list,Ceph events / conferences",10 (Promoter),A combination of capabilities and open source (i.e. no lock in) along with tremendous flexibility in deployment architecture.,,Canada,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),Yes,Out of curiosity,1,40,44,22,"Octopus (15.x),Nautilus (14.x)","Distribution packages,Vendor packages","Ubuntu,Debian",No,Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues,Other (Please specify) - If a release has a security issue that impacts me then I upgrade within a week.",Within a year,"I delay upgrades due to the effort required to upgrade,I only upgrade as needed to address specific bugs or needed features","balancer,crash,devicehealth,pg_autoscaler,restful","Dell,Supermicro,Raspberry Pi,We build our own","ARM,Intel Xeon,Intel (other)","HDD (SATA, SAS)",LVM (ceph-volume),Mix of yes and no,"1000 Mb/s Ethernet (GigE, gigabit),RJ45 (Copper)",IPv4 only,,Single,Primarily cost. I use consumer/low cost hardware.  I have one server class machine with dual sockets but that is next to be removed and spread across multiple Raspberry Pi's.,10 to 100,BlueStore,Replication (2x),"Proxmox,KVM / QEMU,KRBD directly on Linux systems,Other (Please specify) - ceph-fuse client on linux",Yes,No,Yes,Production,"Cloud,Virtualization,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",No,,,,,,,,,,,,,,,,,,,,Production,"Video,Cloud,Home Directories,Archive Storage","Linux kernel CephFS mount,ceph-fuse,CIFS (SMB, Samba)",6 - 50,4G - 15G,1,No,No,No,,9 (Promoter),Yes,4,0,2,5,1,1,0,0,2,0,2,"The level of detail available in octopus ""ceph -s"". When a recovery/rebalance is going on I tend to use:  watch -d ""ceph -s""  to keep an eye on the ongoing status and performance.","Ceph Dashboard,Proxmox,Other (Please specify) - ceph -s",Other (Please specify) - apt-get via Ubuntu or raspbian repositories or a PPA for upstream packages.  Homebrew with SSH and images and text files for configuration.,
More than 5 years,"Open source,Scalability,Feature set,Security,High availability,Data durability / reliability / integrity,Performance","Commercial,Government,Military,Academic",5,5,0,0,0,0,3,Reddit community is very helpful!,3,5,3,3,3,3,3,5,"Reporting issues via the bug tracker,Contributing / enhancing documentation,Ceph events / conferences",10 (Promoter),Ceph offers immense potential to the solutions built on top of it.,Auto-healing Auto-RCA,"Germany,India,United Kingdom,United States","No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations",,,"No, I didn't realize they existed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Integration with adjacent technologies",Commercial,5,4,0,0,0,3,0,,5,3,5,5,3,4,3,4,Mailing list,9 (Promoter),High availability.,,New Zealand,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,4,2000,4300,1960,"Luminous (12.x),Nautilus (14.x)",Distribution packages,Ubuntu,No,Longer,"I delay upgrades because of the effort involved,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade","balancer,pg_autoscaler",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Partitions,Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),bcache",Mix of yes and no,10 Gb/s Ethernet,IPv4 only,"U.2 (2.5-inch),Add In Cards","Single,Dual",Hi Core count on recent processors don't require dual socket. Used to prefer dual to increase core count.,500 to 1000,"Filestore (XFS, EXT4, BTRFS),BlueStore","Replication (3x),Erasure coding",Proxmox,Yes,Yes,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,Production,"Archive Storage,Backup",2,0,S3,"custom / in-house,Other (Please specify) - Veeam Backup",RGW (built-in),HAproxy,0,Use case:,No,,,,,,,,,Production,Other (Please specify) - Backup Data,"Linux kernel CephFS mount,NFS (Kernel NFS server)",1 - 5,4G - 15G,1,Yes,No,No,,6 (Detractor),No (Please specify) - Very familiar with command line.,1,1,0,0,0,0,0,0,0,0,0,Status Dashboard without login.,"Ceph Dashboard,InfluxDB,Prometheus,Grafana (custom),Collectd,node_exporter",ceph-deploy,
More than 5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity",Commercial,5,4,0,0,0,0,1,,4,3,3,2,2,5,4,3,Mailing list,8 (Passive),it has always been very reliable,"performance. ceph is so slow for single threaded IO. sure it's fast when you have 1000's of clients, but with a small client set it's terrible.","Germany,Netherlands,United States","Yes, telemetry is enabled on some of my clusters",I haven't gotten around to it yet,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,4,516,1320,440,"Mimic (13.x),Nautilus (14.x)","Upstream packages,Distribution packages","Debian,RHEL / CentOS / Fedora",No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades because of the effort involved,I only upgrade as needed to address issues",Longer,"I delay upgrades due to the effort required to upgrade,I only upgrade as needed to address specific bugs or needed features",pg_autoscaler,"Lenovo,Dell","Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device),bcache",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet,DAC (Copper)",IPv4 only,U.2 (2.5-inch),Single,"no numa overhead, so performance is better",100 to 500,BlueStore,Replication (3x),Proxmox,Yes,No,No,"Development / Testing,Staging,Production","Virtualization,Other (Please specify) - backups","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6 (Detractor),Yes,3,1,1,2,0,0,0,0,0,0,1,per OSD latency data,Zabbix,"Ansible (other / homebrew),Proxmox",
Between 2-5 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Personal,5,4,0,0,3,0,0,,5,3,5,4,2,2,2,2,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),,,Canada,"No, telemetry is not enabled on any clusters","My organization's security policy does not allow storage systems to communicate with vendors or external organizations,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.",,,"No, I didn't realize they existed",,1,178,178,59,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a month of release,I delay upgrades due to concerns about stability,"balancer,devicehealth,pg_autoscaler",We build our own,Intel Xeon,"HDD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",Dual stack,U.2 (2.5-inch),No preference / beats me,,10 to 100,BlueStore,Replication (3x),"OpenStack,KVM / QEMU",Yes,No,Yes,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,,"Home Directories,Archive Storage","Linux kernel CephFS mount,ceph-fuse,NFS (nfs-ganesha),CIFS (SMB, Samba)",1 - 5,1G,1,No,No,No,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Academic,5,3,3,3,2,1,2,,4,5,4,4,4,4,4,4,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),,,France,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 1-2 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity",Government,4,3,0,0,0,0,0,,5,4,4,3,3,4,4,4,"Mailing list,Ceph events / conferences",10 (Promoter),,,Turkey,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,2,15000,18000,11000,"Octopus (15.x),Nautilus (14.x)",Distribution packages,RHEL / CentOS / Fedora,No,Longer,"I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Within a year,I delay upgrades due to concerns about stability,balancer,"Dell,HPE (Hewlett Packard),Huawei","Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",No,25 Gb/s Ethernet,IPv4 only,"U.2 (2.5-inch),E1.L (EDSFF, ruler form factor)",Dual,,1000 to 3000,BlueStore,"Replication (3x),Erasure coding","KVM / QEMU,KRBD directly on Linux systems",Yes,Yes,Yes,Proof of Concept (PoC),Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",No,Production,"Archive Storage,Backup",5,2,"S3,RGW admin API","Boto3,s3cmd / s3tools",RGW (built-in),None,0,Use case:,No,,,,,,,,,Production,"Build,Big Data & Analytics",Linux kernel CephFS mount,6 - 50,32G - 64G,1,No,No,No,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,5,3,5,3,3,3,0,0,0,0,3,No interest,"Ceph Dashboard,Prometheus,Grafana (custom),node_exporter",Ansible (ceph-ansible),
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity","Commercial,Personal",5,5,3,1,3,2,1,,5,4,4,4,4,5,4,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",7 (Passive),,,United States,"Yes, telemetry is enabled on some of my clusters","My cluster(s) are on a protected network that does not have access to the internet,My cluster(s) are running a Ceph version older than Luminous,Other (Please specify) - bugs","device: anonymized device health metrics (e.g., SMART hard disk metrics)",I left 'ident' disabled (the default),"No, I didn't realize they existed",,3,100,80,240,"Mimic (13.x),Infernalis (9.x) or older,Nautilus (14.x)",Upstream packages,"Ubuntu,RHEL / CentOS / Fedora",No,Longer,"I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Never (clusters remain on major release they were deployed with),"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability","crash,iostat",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",Yes,"10 Gb/s Ethernet,RJ45 (Copper)",IPv4 only,"M.2,U.2 (2.5-inch)",Dual,,10 to 100,"Filestore (XFS, EXT4, BTRFS),BlueStore",Replication (3x),"OpenStack,OpenNebula,Proxmox,KRBD directly on Linux systems",Yes,No,Yes,"Development / Testing,Staging,Production","Containers,Virtualization,Archive Storage","No, it is not compatible with my RBD feature requirements","librbd (e.g., in combination with qemu),Linux kernel RBD,iSCSI (LIO with /dev/rbd)",Yes,,,,,,,,,,,,,,,,,,,,"Development / Testing,Staging,Production","Containers,Virtualization,Home Directories,Archive Storage","Linux kernel CephFS mount,libcephfs",6 - 50,32G - 64G,1,No,No,No,Multiple File Systems within a Ceph Cluster,7 (Passive),Yes,4,3,3,3,3,2,1,0,3,0,3,rgw configuration,"Ceph Dashboard,Prometheus,Nagios/icinga,Telegraf","Ansible (ceph-ansible),Puppet",
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity","Academic,Non-profit",5,5,0,3,1,0,4,,5,5,4,4,3,3,4,4,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",8 (Passive),,,Finland,"Yes, telemetry is enabled on some of my clusters",I haven't gotten around to it yet,,I enabled 'ident' and configured a cluster description and/or email address,Yes,"Out of curiosity,To inform my decisions about when or whether to upgrade",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Feature set,Data durability / reliability / integrity,Performance",Academic,3,4,0,0,0,0,2,,5,3,4,5,4,3,3,5,"Mailing list,Ceph events / conferences",10 (Promoter),,,Czech Republic,"No, telemetry is not enabled on any clusters",I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,,,"No, I didn't realize they existed",,3,25000,36000,12000,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a week after release,I apply upgrades quickly to get any available bug fixes or security fixes,Longer,Other (Please specify) - We have to to some testing before upgrade.,"iostat,restful,Other (Please specify) - prometheus",Supermicro,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt)",Mix of yes and no,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,,1000 to 3000,BlueStore,"Replication (3x),Erasure coding","OpenStack,rbd-nbd,Other (Please specify)",Yes,Yes,Yes,Production,"Virtualization,Other (Please specify) - Users get from us RBD and can do it with image what they want. Typicaly thes apply dm-crypt/LUKS and create file-system.",Other (Please specify) - We are in testing phase,Linux kernel RBD,Yes,,"Containers,Backup",2,5,S3,"Boto3,s3cmd / s3tools,Other (Please specify) - s5cmd because of speed",RGW (built-in),"HAproxy,DNS round-robin,Other (Please specify) - In out latest cluster we used exBGP for HA and load-balancer. This is probably the best approach.",0,Use case:,No,,,,,,,,,Production,"Home Directories,Other (Please specify) - Last resort backup of ownCloud","Linux kernel CephFS mount,ceph-fuse",1 - 5,16G - 31G,2,No,No,No,Multiple File Systems within a Ceph Cluster,6 (Detractor),"No (Please specify) - We are not using dashboard, only CLI :-)",0,0,0,0,0,0,0,0,0,0,0,I can't tell,"Prometheus,Grafana (custom),Nagios/icinga,node_exporter","Ansible (ceph-ansible),Puppet",
Between 2-5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity",Commercial,5,4,0,1,0,0,1,,5,4,3,3,2,4,2,3,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",7 (Passive),,,Spain,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,Yes,"Out of curiosity,To inform my decisions about when or whether to upgrade,To make decisions around cluster sizing (for new cluster(s))",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Commercial,5,4,0,1,0,0,1,,4,3,4,3,2,3,2,2,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",7 (Passive),,,Spain,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,Yes,"Out of curiosity,To inform my decisions about when or whether to upgrade,To make decisions around cluster sizing (for new cluster(s))",2,442,443,256,Octopus (15.x),Upstream packages,Ubuntu,No,Within a month after release,"I delay upgrades due to concerns about regression,Other (Please specify) - Depends on the update, I usually check the changelog and make and decision",Within half a year of release,"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,crash,diskprediction,iostat,pg_autoscaler,Other (Please specify) - prometheus",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,2 sockets for us are enough to cover osd requirements,10 to 100,BlueStore,Erasure coding,None,No,Yes,No,,,,,,"Development / Testing,Production","Log,Archive Storage,Backup",3,1,"S3,RGW admin API","Other (Please specify) - restic, minio client",RGW (built-in),"DNS round-robin,Other (Please specify) - powerdns lua records",0,Use case:,No,,,,,,,,,,,,,,,,,,,6 (Detractor),No (Please specify) - we do not have dashboard enabled,0,0,0,0,0,0,0,0,0,0,0,we do not have dashboard enabled,"Prometheus,Grafana (custom),node_exporter",Ansible (ceph-ansible),
Between 2-5 years,"Open source,Cost,High availability,Data durability / reliability / integrity",Commercial,5,4,0,0,0,0,0,I think I would IRC more if there would be a web interface that I can use without the need of creating an account.,4,0,,,,4,0,2,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),add to focus efforts: testing before releasing,Ceph is not going to AWS so doing the same as AWS does not make sense. Thus focus on what AWS cannot do but you can do because your environment is smaller. eg. renaming a radosgw object really does not need to result in a copy.,Netherlands,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,Other (Please specify)",,,"No, I didn't realize they existed",,1,150,150,0,Nautilus (14.x),Vendor packages,RHEL / CentOS / Fedora,No,Within a month after release,I delay upgrades due to concerns about regression,Longer,I delay upgrades due to concerns about performance regressions,"balancer,crash,devicehealth,iostat,Other (Please specify) - prometheus",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),No,10 Gb/s Ethernet,IPv4 only,,No preference / beats me,,10 to 100,BlueStore,Replication (3x),"KVM / QEMU,Other (Please specify) - mesos",Yes,Yes,Yes,Production,"Containers,Virtualization,Archive Storage","No, it is not needed",Linux kernel RBD,Yes,Production,"Archive Storage,Backup",2,0,S3,s3cmd / s3tools,RGW (built-in),HAproxy,0,Use case:,No,,,,,,,,,"Development / Testing,Staging",Archive Storage,"Linux kernel CephFS mount,NFS (nfs-ganesha),CIFS (SMB, Samba)",1 - 5,16G - 31G,1,No,Yes,No,,0 (Detractor),No (Please specify) - Use own grafana + cli,,,,,,,,,,,,not using dashboard,"InfluxDB,Prometheus,Grafana (custom),Nagios/icinga,Collectd,node_exporter","Other (Please specify) - none, because sooner or later they will not be supported",
More than 5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity",Commercial,5,2,1,0,0,0,3,,4,5,5,3,2,5,2,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",10 (Promoter),,Cephadm on baremetal deployment. Better documentation. Cephadm on baremetal deployment.,Poland,"No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I knew they existed by have not used them",,10,4000,15,5,"Octopus (15.x),Nautilus (14.x)",Upstream packages,Ubuntu,No,Within a month after release,I delay upgrades due to concerns about regression,Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,crash,restful,Other (Please specify) - zabbix dashboard",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Mix of yes and no,"10 Gb/s Ethernet,40 Gb/s Ethernet,25 Gb/s Ethernet",Dual stack,"U.2 (2.5-inch),Add In Cards","Single,Dual",I don't know :-).,1000 to 3000,BlueStore,Replication (3x),None,No,Yes,No,,,,,,"Development / Testing,Staging,Production,Proof of Concept (PoC)","Video,CDN,Big Data & Analytics,Backup",15,10,"S3,RGW admin API","Amazon SDK,Boto3,s3cmd / s3tools,Other (Please specify) - jets3t",RGW (built-in),"HAproxy,Nginx,Dedicated custom hardware (Alteon, F5, etc),Varnish",0,Use case:,No,,,,,,,,,,,,,,,,,,,5 (Detractor),No (Please specify) - No,1,1,1,0,0,0,0,0,0,0,1,"On big deployments this tool works very bad, very unstable...","Prometheus,Grafana (custom),Zabbix,Graphite,Collectd,node_exporter","Ansible (ceph-ansible),Puppet,ceph-deploy",
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Commercial,5,5,0,0,0,0,1,,5,5,5,5,5,5,5,5,Mailing list,10 (Promoter),,,United States,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance",Commercial,4,4,2,0,0,0,0,,5,5,4,2,4,3,2,4,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),It's nearly bullet proof and not hard to configure with ceph-ansible.,"ceph-deploy to come back Keep ceph bare metal, containerization just adds unnecessary complexity and adds a barrier to entry More stability in management interfaces.",United States,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,760,760,333,Luminous (12.x),Distribution packages,RHEL / CentOS / Fedora,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability",,Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Mix of yes and no,InfiniBand,IPv4 only,"U.2 (2.5-inch),E1.L (EDSFF, ruler form factor),Add In Cards",Dual,We like to have no less than 12 OSDs per chassis and having 2 CPUs ensures that we'll have a minimum of 12 physical cores so that each daemon can have a dedicated thread or more.,100 to 500,Both Filestore and BlueStore,Replication (3x),"Proxmox,KVM / QEMU,KRBD directly on Linux systems",Yes,Yes,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",No,Development / Testing,"Containers,Home Directories",3,0,"S3,NFS re-export of S3 buckets",None,"RGW (built-in),LDAP",HAproxy,0,Use case:,No,,,,,,,,,Development / Testing,"Containers,Virtualization,Home Directories","ceph-fuse,NFS (nfs-ganesha),NFS (Kernel NFS server)",6 - 50,2G - 3G,2,No,No,Yes,,10 (Promoter),Yes,4,3,3,3,3,3,0,0,2,3,4,Deployment (i'm on luminous),"Ceph Dashboard,Grafana (custom),Zabbix,Telegraf","Ansible (ceph-ansible),ceph-deploy",
Less than 1 year,"Open source,Other (Please specify) - Consulting other users",Personal,5,4,0,0,0,3,3,,3,4,3,0,1,3,3,5,Mailing list,1 (Detractor),"Too complicated, and I have seen enough unsolvable (for me) support requests involving crashes.","Troubleshooting and disaster recovery guides should be rewritten with orchestration in mind. Reflinks in cephfs. Official support for some cache (ssd over hdd) other than ""cache tiering"".",That would be Telling :p,"No, telemetry is not enabled on any clusters",Other (Please specify) - Mostly short-lived test clusters,,,Yes,Out of curiosity,1,122,122,61,"Octopus (15.x),Nautilus (14.x)",Upstream packages,Debian,No,Longer,I only upgrade as needed to address issues,Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability",I don't know / default set,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies,Other (Please specify)","Commercial,Other (Please specify)",5,4,0,0,3,5,1,,5,4,5,4,5,5,4,4,"IRC / Slack / etc,Mailing list,Other (Please specify)",10 (Promoter),Feature set,Improved performance Master/Master HA for NFS exporting CephFS Easier management,United States,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,6,200,1100,366,"Luminous (12.x),Infernalis (9.x) or older",Upstream packages,Ubuntu,No,Longer,I only upgrade as needed to address issues,Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,iostat,pg_autoscaler",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet,DAC (Copper)",IPv4 only,U.2 (2.5-inch),"Single,Dual",Dual for HDD clusters and Single for SSD/NVME clusters to reduce NUMA latency,100 to 500,Both Filestore and BlueStore,Replication (3x),OpenStack,Yes,Yes,No,Production,"Containers,Virtualization,Home Directories",Other (Please specify) - No good way to implement DR with OpenStack,"librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,"Log,Other (Please specify) - Time Series Storage",5,5,"S3,Swift","s3cmd / s3tools,Other (Please specify) - Minio",Keystone,HAproxy,0,Use case:Data replication between systems,No,,,,,,,,,,,,,,,,,,,10 (Promoter),No (Please specify) - Maybe in the future but we don't use it much currently,0,0,0,0,0,0,0,0,0,0,0,N/A,"Prometheus,Grafana (custom),node_exporter,ceph_exporter",Ansible (ceph-ansible),
Between 2-5 years,"Open source,Scalability,Cost,Feature set,Performance","Government,Academic",3,5,0,3,0,0,0,,4,4,3,1,3,4,4,5,Mailing list,10 (Promoter),it's the only way that our research lab can afford to build multi-PB storage clusters,Maintain easy (non-container) deployment tools Windows and Mac Native clients for Cephfs Cephfs georeplication / snapshots,United Kingdom,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,"No, I didn't realize they existed",,2,5000,10000,7300,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,I delay upgrades due to concerns about regression,Longer,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability","balancer,iostat","Supermicro,Other (Please specify) - 45drives",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",No,"10 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet",IPv4 only,"M.2,U.2 (2.5-inch)","Single,Dual",,500 to 1000,BlueStore,Erasure coding,Other (Please specify) - it's all cephfs,No,No,Yes,,,,,,,,,,,,,,,,,,,,,,,,,Production,"Big Data & Analytics,HPC,Archive Storage","Linux kernel CephFS mount,CIFS (SMB, Samba),Other (Please specify) - Windows alpha client....",1 - 5,65G - 127G,2,No,Yes,Yes,,10 (Promoter),Yes,3,0,1,4,2,0,0,0,4,0,5,more cephfs functionality - hot clients etc....,Ceph Dashboard,ceph-deploy,
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Academic,5,4,0,1,4,0,2,,5,4,3,3,3,3,4,5,"IRC / Slack / etc,Mailing list",7 (Passive),"Whilst ceph is generally great [so we'd be more likely to recommend it than not], the current direction of development is moving in places we're less interested in [an overweening obsession with containerised deployment over all things, for example, using its own in-house orchestrator].  We also have had more than one issue with poor releases of updates: regressions, unstable services etc.",Non-containerised deployment and orchestration.,United Kingdom,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,1,3500,3500,2800,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades because there are too many updates,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,pg_autoscaler",Dell,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Mix of yes and no,10 Gb/s Ethernet,IPv4 only,"M.2,U.2 (2.5-inch)",No preference / beats me,,100 to 500,BlueStore,Erasure coding,None,No,No,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8 (Passive),No (Please specify) - Mostly we use the Dashboard for information not to action changes.,4,2,4,4,3,0,0,0,0,0,0,None.,"Ceph Dashboard,Prometheus,node_exporter,ceph_exporter",Ansible (ceph-ansible),
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance",Commercial,5,2,0,0,0,0,1,,5,3,3,3,3,4,3,3,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),It's just awesome...,Better documentation about the old/ceph-deploy config path to the new cephadm management.,Germany,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,Enabling telemetry would require review by my organization's security team and I have chosen not request a review",,,"No, I didn't realize they existed",,1,257,257,257,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a month of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy","balancer,crash,devicehealth,iostat,pg_autoscaler,Other (Please specify) - ssh","Dell,Supermicro",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",No,10 Gb/s Ethernet,IPv4 only,"M.2,U.2 (2.5-inch),Add In Cards",Single,NUMA awareness,10 to 100,BlueStore,"Replication (3x),Erasure coding","OpenStack,Kubernetes,KVM / QEMU,KRBD directly on Linux systems",Yes,No,Yes,"Development / Testing,Staging,Proof of Concept (PoC)","Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",No,,,,,,,,,,,,,,,,,,,,"Development / Testing,Staging,Production,Proof of Concept (PoC)","Scratch,Big Data & Analytics,Cloud,Containers,Archive Storage","Linux kernel CephFS mount,libcephfs,NFS (Kernel NFS server),CIFS (SMB, Samba)",6 - 50,1G,5,Unknown,No,Yes,Multiple File Systems within a Ceph Cluster,10 (Promoter),No (Please specify) - Actually we do not have a dasboard available because MGR runs as systemd process on plain Centos7. Just CLI management...,0,0,0,0,0,0,0,0,0,0,0,Better upgrade path from Centos7 (mons & mgr) Our OSD meanwhile run on Centos8.,"Other (Please specify) - none, v15.9 mgr runs on Centos7",Other (Please specify) - Deployment from scatch like ceph-volume etc.,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Integration with adjacent technologies","Commercial,Academic,Non-profit",5,5,0,4,0,3,4,,5,5,5,5,5,5,5,5,"Mailing list,Reporting issues via the bug tracker,Contributing code,Contributing / enhancing documentation,Ceph events / conferences,Member of the Ceph Foundation",10 (Promoter),,,Switzerland,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,Enabling telemetry would require review by my organization's security team and I have chosen not request a review",,,"No, I knew they existed by have not used them",,5,1024,1024,384,"Luminous (12.x),Jewel (10.x)","We build our own packages,We built a custom version","Ubuntu,Debian,RHEL / CentOS / Fedora",No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,restful","Dell,Supermicro","ARM,Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe,Optane","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,10 Gb/s Ethernet,IPv4 only,"U.2 (2.5-inch),Add In Cards","Single,Dual",,10 to 100,BlueStore,"Replication (2x),Replication (3x),Erasure coding",OpenStack,Yes,Yes,Yes,Production,"Big Data & Analytics,HPC,Cloud,Containers,Virtualization,Home Directories,Archive Storage","Yes, to migrate volumes between clusters","librbd (e.g., in combination with qemu),Linux kernel RBD,iSCSI (LIO with /dev/rbd)",Yes,Production,"Big Data & Analytics,HPC,Cloud",4,1024000,S3,"Amazon SDK,s3cmd / s3tools","RGW (built-in),Keystone,LDAP","HAproxy,Nginx",2,No,No,,,,,,,Kafka,,Development / Testing,"Big Data & Analytics,Home Directories,Archive Storage","Linux kernel CephFS mount,ceph-fuse,NFS (Kernel NFS server)",6 - 50,16G - 31G,3,Yes,No,Yes,"Inline Data,Multiple File Systems within a Ceph Cluster",8 (Passive),Yes,4,4,4,4,4,4,4,4,4,4,4,None,"openATTIC,Prometheus,node_exporter,ceph_exporter","Salt / DeepSea,ceph-deploy",
Between 1-2 years,"Open source,Scalability,Feature set,High availability,Data durability / reliability / integrity",Academic,5,5,0,0,0,0,3,,5,5,4,3,3,5,5,5,Mailing list,10 (Promoter),,An enjoyable to use CLI,France,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,Yes,"Out of curiosity,To inform my decisions about when or whether to upgrade",1,29,29,13,Octopus (15.x),Upstream packages,Ubuntu,No,Longer,I delay upgrades due to concerns about regression,Within a year,I delay upgrades due to concerns about stability,"balancer,crash,devicehealth,diskprediction,iostat,pg_autoscaler",Intel,AMD Epyc,NVMe,"LVM (ceph-volume),Containerized daemons",Yes,"10 Gb/s Ethernet,RJ45 (Copper)",IPv4 only,U.2 (2.5-inch),"Dual,No preference / beats me",,10 to 100,BlueStore,"Replication (3x),Erasure coding","OpenStack,Kubernetes,KVM / QEMU",Yes,Yes,Yes,Production,"Build,Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,Production,"CDN,Cloud",5,0,"S3,Swift","Amazon SDK,Boto3,s3cmd / s3tools","RGW (built-in),Keystone",HAproxy,0,Use case:,No,,,,,,,,,Production,"Cloud,Archive Storage","Linux kernel CephFS mount,ceph-fuse,libcephfs",1 - 5,2G - 3G,5,No,Yes,No,Multiple File Systems within a Ceph Cluster,10 (Promoter),"No (Please specify) - CLI> clic clic: faster, easier to remember, scriptable. Wish the CLI was better though.",5,0,0,0,0,0,0,0,0,0,5,OIDC/LDAP integration.,"Ceph Dashboard,Prometheus,Grafana (custom),node_exporter",cephadm,
Between 1-2 years,"Scalability,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Non-profit,4,2,0,0,0,,2,,2,4,4,2,2,4,2,4,,8 (Passive),,,France,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I knew they existed by have not used them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Commercial,5,5,0,0,0,0,1,,3,5,2,4,4,4,4,4,"Mailing list,Ceph events / conferences",8 (Passive),,,France,"No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I didn't realize they existed",,3,1024,2020,1300,Luminous (12.x),Distribution packages,RHEL / CentOS / Fedora,"Yes, in front of EC pools for functionality",Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues",Longer,I delay upgrades due to the effort required to upgrade,balancer,Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),Yes,"10 Gb/s Ethernet,DAC (Copper)",IPv4 only,U.2 (2.5-inch),"Single,Dual","Depends on number of OSD. I try to have 1 core/OSD + 2 cores min. for the OS. If a single CPU has enough cores and reasonnable price I go with it, otherwise 2 CPUs",100 to 500,BlueStore,"Replication (3x),Erasure coding",KRBD directly on Linux systems,Yes,No,No,Production,"Archive Storage,Other (Please specify) - Backups","No, it is not needed",Linux kernel RBD,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8 (Passive),No (Please specify),0,0,0,0,0,0,0,0,0,0,0,We should use the dashboard more after a couple of version upgrades (we are in luminous),"Prometheus,Grafana (custom),node_exporter,ceph_exporter","Ansible (ceph-ansible),Puppet,ceph-deploy,Ansible (other / homebrew)",
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance",Personal,5,4,0,0,0,0,3,Bug tracker,3,4,4,3,3,4,4,4,Reporting issues via the bug tracker,8 (Passive),Excellent scalability - can easily grow over time as requirements change.,"Better documentation with acutal examples, explanations of trade-offs between various settings Better observability - why am I only getting this level of performance? What's using the cluster?",United Kingdom,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),Yes,"Out of curiosity,Other",1,78,78,50,Octopus (15.x),Upstream packages,Ubuntu,No,Within a month after release,"I delay upgrades because of the effort involved,I delay upgrades due to concerns about regression",Within half a year of release,"I upgrade quickly to get new features,I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,iostat,pg_autoscaler","Lenovo,Dell,HPE (Hewlett Packard),Supermicro,We build our own","Intel Xeon,Intel (other)","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Mix of yes and no,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet,RJ45 (Copper),DAC (Copper),AOC / Fiber",IPv4 only,"M.2,U.2 (2.5-inch)",No preference / beats me,Lab environment - whatever hardware I can get my hands on! Slight preference for lower numbers of sockets though in order to reduce power/heat issues.,10 to 100,BlueStore,"Replication (3x),Erasure coding","Kubernetes,rbd-nbd",Yes,Yes,Yes,Production,Containers,"No, it is not needed","Linux kernel RBD,rbd-nbd",No,Production,"Log,Video,CDN,Containers",2,0,"S3,RGW admin API","s3cmd / s3tools,custom / in-house",RGW (built-in),HAproxy,0,Use case:,No,,,,,,,,,Production,"Video,Containers,Home Directories,Archive Storage","Linux kernel CephFS mount,NFS (nfs-ganesha),CIFS (SMB, Samba)",51 - 100,4G - 15G,1,No,No,No,,8 (Passive),Yes,5,0,3,3,3,0,0,2,2,5,0,.,"Prometheus,Grafana (custom),node_exporter",Ansible (ceph-ansible),
Between 1-2 years,"Open source,Scalability,Cost,Feature set,Data durability / reliability / integrity,Performance",Academic,4,4,0,0,0,0,1,,5,5,5,4,4,5,5,5,Mailing list,9 (Promoter),I've looked at many distributed storage system for many years.  Ceph is the first to have everything I need.  9 because I'm opposed to container deployment.,Retain non-container deployment. Continue to enhance EC-pools as 3-way replication is a barrier to entry for tight budgets. Improve config/deployment management - Ceph Ansible or other.,United States,"No, telemetry is not enabled on any clusters",Other (Please specify) - I didn't know this existed.,,,"No, I didn't realize they existed",,1,500,576,400,Nautilus (14.x),Distribution packages,Debian,No,Longer,Other (Please specify) - Distribution packages,Longer,Other (Please specify) - Distribution packages,"balancer,crash,devicehealth,diskprediction,pg_autoscaler",Other (Please specify) - RedBarnHPC/Tyan,AMD Epyc,"HDD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,"10 Gb/s Ethernet,DAC (Copper)",IPv4 only,"M.2,U.2 (2.5-inch),Add In Cards","Single,No preference / beats me",Single Epyc for cost.,10 to 100,BlueStore,Erasure coding,"Kubernetes,Other (Please specify) - Direct Linux access CephFS and RBD",Yes,Yes,Yes,Production,"Big Data & Analytics,Other (Please specify) - Database","No, it is not needed",Linux kernel RBD,No,Production,Big Data & Analytics,2,0,S3,Other (Please specify) - Unknown,Other (Please specify) - Not yet deployed,Other (Please specify) - Not yet deployed,0,Use case:tbd,No,,,,,,,,,Production,"Big Data & Analytics,Archive Storage",Linux kernel CephFS mount,1 - 5,2G - 3G,2,No,No,No,,10 (Promoter),Yes,5,5,5,5,5,5,0,0,1,0,5,Some Hosts data is only available per host - I would like combined network graphs.,Ceph Dashboard,Ansible (ceph-ansible),
Between 2-5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Academic,5,5,1,1,2,0,4,,5,5,5,5,5,3,3,5,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),RBD,Cephadm no-containers (use deb/rpm only) option Cephfs server side enforced quotas,United States,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,2,19585,24865,14919,Nautilus (14.x),Upstream packages,Ubuntu,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability",iostat,"Dell,Supermicro",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),At-rest encryption (dmcrypt),All-in-one OSD (default, colocated, no separate metadata device)",No,"10 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet",IPv4 only,,Dual,Cost/performance balance,1000 to 3000,BlueStore,"Replication (3x),Erasure coding","OpenNebula,KVM / QEMU,KRBD directly on Linux systems",Yes,No,Yes,Production,"Big Data & Analytics,Virtualization,Home Directories,Other (Please specify) - General lab storage","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,Production,"Big Data & Analytics,Archive Storage","Linux kernel CephFS mount,ceph-fuse",6 - 50,32G - 64G,1,No,No,Yes,,0 (Detractor),"No (Please specify) - CLI is faster, simpler and scriptable",0,0,0,0,0,0,0,0,0,0,0,"Simplicity, speed and script-ability","Grafana (custom),Graphite,Collectd","Ansible (ceph-ansible),Ansible (other / homebrew)",
More than 5 years,"Open source,Feature set,High availability,Data durability / reliability / integrity","Commercial,Personal",5,4,0,0,2,0,1,,5,4,4,4,3,3,4,5,"IRC / Slack / etc,Reporting issues via the bug tracker",10 (Promoter),first storage solution that has been up since it was deployed while being upgraded regularly,"- clear recommendation about bluestore osd settings, the current db/wall sizing recommendations in the docs sound like voodoo magic - ceph-ansible support in the future",Netherlands,"Yes, telemetry is enabled on some of my clusters",I haven't gotten around to it yet,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted)",I left 'ident' disabled (the default),Yes,Out of curiosity,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 1-2 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Commercial,5,4,0,0,0,0,3,,5,4,4,5,4,4,4,5,Mailing list,10 (Promoter),,better documentation about performance tuning/potential performance problems monitoring client side performance metrics for cephfs (iops/latency) monitoring min/mean/median/99%/max OSD/MDS operation latency,Austria,"No, telemetry is not enabled on any clusters","Enabling telemetry would require review by my organization's security team and I have chosen not request a review,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,Other (Please specify) - telemetry has lead to multiple HEALTH_ERROR states for other people in the past",,,"No, I didn't realize they existed",,2,56,60,20,"Octopus (15.x),Nautilus (14.x)",Upstream packages,Ubuntu,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort,I delay upgrades due to concerns about regression",Within a year,"I upgrade quickly to get new features,I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I upgrade quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,diskprediction,iostat",Dell,"Intel Xeon,AMD Epyc","SSD (SATA, SAS),NVMe","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Yes,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,performance per rack volume per cost,10 to 100,BlueStore,Replication (3x),"Kubernetes,KVM / QEMU,KRBD directly on Linux systems,OpenShift",Yes,Yes,Yes,Production,"Build,Cloud,Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,"Cloud,Archive Storage,Backup",2,1,S3,"Amazon SDK,s3cmd / s3tools",RGW (built-in),HAproxy,0,no,No,,,,,,,,,Production,"Build,Log,Cloud,Containers,Virtualization,Home Directories,Archive Storage",Linux kernel CephFS mount,101 - 500,4G - 15G,1,No,Yes,Yes,,6 (Detractor),No (Please specify) - learn more when using CLI and clearer what happens when using CLI,1,1,1,0,0,0,0,0,0,0,0,correct performance monitoring data/graphs and explanation what they show and why they show the correct thing (what do the various performance counters actually mean?!),"Ceph Dashboard,Prometheus,Nagios/icinga",Puppet,
Less than 1 year,"Scalability,Security,High availability,Data durability / reliability / integrity,Performance",Commercial,4,4,1,1,1,4,3,,4,5,4,4,4,4,4,5,Ceph events / conferences,5 (Detractor),,Please dont stop support for bare metal,That would be Telling :p,"No, telemetry is not enabled on any clusters",I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,,,"No, I didn't realize they existed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,4,4,0,0,0,0,1,,5,5,5,5,5,5,5,4,Mailing list,8 (Passive),,,That would be Telling :p,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I knew they existed by have not used them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies","Academic,Non-profit",5,3,0,0,0,0,4,ceph-cheatsheet,5,4,5,4,3,5,,4,"Mailing list,Reporting issues via the bug tracker,Contributing code,Contributing / enhancing documentation",10 (Promoter),the only scalable free software storage solution with comparable feature set,"better failure handling (data rediscovery for remapped data, recovery/backfill io load) better performance (per-disk io load is very high, hdd osd efficiency) pool migrations (erasure -> replica and vice versa, ec profile conversion, ...)",Germany,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I didn't realize they existed",,2,2200,3000,1840,Nautilus (14.x),"Upstream packages,Distribution packages",Ubuntu,No,Within a month after release,I delay upgrades due to concerns about regression,Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,crash,iostat,pg_autoscaler,Other (Please specify) - prometheus","Dell,HPE (Hewlett Packard),Supermicro,We build our own","Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt),All-in-one OSD (default, colocated, no separate metadata device)",No,"10 Gb/s Ethernet,40 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet,Other (Please specify) - FibreChannel",IPv4 only,U.2 (2.5-inch),No preference / beats me,depends on the specific server,500 to 1000,BlueStore,"Replication (3x),Replication (4 or more copies),Erasure coding","OpenStack,VMware,Proxmox,KVM / QEMU,KRBD directly on Linux systems,Other (Please specify) - CephFS on Linux",Yes,No,Yes,Production,"Build,Big Data & Analytics,AI/ML,HPC,Virtualization,Home Directories,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,,,,,,,,,,,,,,,,,,,,Production,"Scratch,Build,Video,Big Data & Analytics,AI/ML,Home Directories,Archive Storage","Linux kernel CephFS mount,NFS (nfs-ganesha),NFS (Kernel NFS server),CIFS (SMB, Samba)",101 - 500,16G - 31G,1,No,Yes,Yes,"Inline Data,LazyIO",5 (Detractor),"No (Please specify) - the dashboard was meant to be a dashboard, and not a control interface. it needs huge improvements to compete with the cli.",1,1,1,0,0,0,0,0,0,0,0,"intuitive navigation and good information display (hide unnecessary stuff, format important data nicely): maybe use VMware vCenters as inspiration.","Prometheus,Grafana (custom),node_exporter",ceph-deploy,
More than 5 years,"Open source,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Personal,5,3,0,0,0,0,1,,5,3,3,2,3,1,2,4,Mailing list,9 (Promoter),,,United States,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,2,55,56,16,"Luminous (12.x),Nautilus (14.x)","Upstream packages,Distribution packages",Debian,No,Longer,Other (Please specify) - Internet quota and throttling (otherwise minor upgrades are simple to apply),Longer,"I delay upgrades due to concerns about stability,Other (Please specify) - Lack of Debian builds for some releases",I don't know / default set,"HPE (Hewlett Packard),We build our own","Intel Xeon,AMD (other)","HDD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",No,"1000 Mb/s Ethernet (GigE, gigabit),RJ45 (Copper)",IPv4 only,,No preference / beats me,Load is light so CPU is not an issue.  Cluster is a mix of single and dual.,10 to 100,BlueStore,Replication (3x),"Kubernetes,KVM / QEMU",Yes,No,Yes,Production,"Scratch,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",No,,,,,,,,,,,,,,,,,,,,Production,"Scratch,Video,Containers,Archive Storage","Linux kernel CephFS mount,CIFS (SMB, Samba)",1 - 5,1G,1,No,No,No,,4 (Detractor),No (Please specify) - Primary cluster is still Luminous (with separate metrics collection using telegraf),4,,,1,,,,,1,,,"Per-client information for RBD and CephFS consumers.  Note, have not yet used post-Luminous dashboard.","Ceph Dashboard,InfluxDB,Grafana (custom),Telegraf,Other (Please specify) - ceph --watch and ceph pg stat","Salt / DeepSea,ceph-deploy",
Less than 1 year,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity",Academic,5,5,3,0,0,0,1,Google,5,5,5,5,5,5,4,4,Mailing list,9 (Promoter),,,That would be Telling :p,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,3,414,450,80,Nautilus (14.x),Distribution packages,RHEL / CentOS / Fedora,"Yes, for performance reasons",Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues",Within half a year of release,"I delay upgrades to avoid a changed user experience due to new features,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability",I don't know / default set,Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),No,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,M.2,Dual,budget consideration,100 to 500,BlueStore,Replication (4 or more copies),None,Yes,Yes,Yes,"Development / Testing,Production,Proof of Concept (PoC)","Log,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Unknown,Development / Testing,"Log,Archive Storage",0,0,"S3,RGW admin API","Other (Please specify) - under development, not decided yet","Other (Please specify) - under development, not decided yet","Other (Please specify) - under development, not decided yet",0,Use case:,No,,,,,,,,,Development / Testing,"Log,Archive Storage","Linux kernel CephFS mount,ceph-fuse,NFS (nfs-ganesha),NFS (Kernel NFS server)",1 - 5,2G - 3G,6,Unknown,No,No,Multiple File Systems within a Ceph Cluster,7 (Passive),Yes,4,4,4,3,4,4,3,3,3,3,3,crush rule configuration,"Ceph Dashboard,Prometheus,Nagios/icinga","Ansible (ceph-ansible),ceph-deploy",
Between 2-5 years,"Open source,Scalability,Feature set,Integration with adjacent technologies",Commercial,5,0,0,2,0,0,5,youtube,3,4,5,0,3,5,5,5,Ceph events / conferences,10 (Promoter),"It's opensource first! The community is active and feature set are sufficient. Plenty of Interfaces for almost every storage usage.   After evolving, recent version is more stable and usable. Some of design  are very innovative.",,China,"No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I knew they existed by have not used them",,20,1000,5000,1250,Luminous (12.x),"We build our own packages,We built a custom version",RHEL / CentOS / Fedora,"Yes, for performance reasons",Never,I only upgrade as needed to address issues,Within a year,"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features","diskprediction,iostat,pg_autoscaler",Prefer not to say,"ARM,Intel Xeon","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet",Dual stack,"M.2,U.2 (2.5-inch)",Dual,"computing is not on storage, this is enough for hdd or hybrid storage config.",100 to 500,BlueStore,Replication (4 or more copies),"OpenStack,VMware,KRBD directly on Linux systems",Yes,Yes,Yes,Production,"Virtualization,Home Directories,Archive Storage","Yes, for DR","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,"Cloud,Archive Storage,Backup",100,10000,"S3,NFS re-export of S3 buckets",custom / in-house,LDAP,Nginx,1,NO,No,,,,,,,,,Production,"Log,Home Directories,Archive Storage","Linux kernel CephFS mount,NFS (nfs-ganesha),NFS (Kernel NFS server),CIFS (SMB, Samba)",101 - 500,128G+,1000,Yes,Yes,Yes,"Inline Data,Multiple File Systems within a Ceph Cluster,LazyIO",4 (Detractor),Yes,5,1,4,4,4,4,4,4,4,4,1,OSD/Pool management,"Prometheus,Nagios/icinga",Ansible (other / homebrew),
More than 5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity,Performance",Commercial,5,3,0,0,3,0,0,,5,5,4,3,3,4,3,5,IRC / Slack / etc,10 (Promoter),,cephadm needs to have the option to deploy without containers.,United States,"No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I knew they existed by have not used them",,3,800,2200,700,"Octopus (15.x),Luminous (12.x),Jewel (10.x)",Distribution packages,RHEL / CentOS / Fedora,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades because of the effort involved,I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Longer,"I delay upgrades due to the effort required to upgrade,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability",I don't know / default set,Supermicro,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",Mix of yes and no,"40 Gb/s Ethernet,100 Gb/s Ethernet,InfiniBand",IPv4 only,"U.2 (2.5-inch),E1.L (EDSFF, ruler form factor)",Dual,PCIe lanes,100 to 500,Both Filestore and BlueStore,"Replication (2x),Replication (3x)","OpenStack,Kubernetes,VMware,Proxmox,KVM / QEMU",Yes,No,No,Production,"Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD,iSCSI (LIO with /dev/rbd)",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8 (Passive),Yes,3,3,3,3,3,2,2,0,0,0,0,N/A,"Ceph Dashboard,Zabbix","ceph-deploy,cephadm",
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance",Commercial,5,,0,1,1,2,3,,5,3,3,2,1,4,2,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Contributing code,Contributing / enhancing documentation,Ceph events / conferences",9 (Promoter),,"1) Upgrade MDS(s) without a single second of downtime 2) More insight in ops performance, tracing slow ops more easily for example. 3) Better overview of data usage including rocksdb for example. Everything is available right now, but you need to dig a bit to get everything you want.",Netherlands,"Yes, telemetry is enabled on some of my clusters",My cluster(s) are on a protected network that does not have access to the internet,,I left 'ident' disabled (the default),Yes,Out of curiosity,1,524,524,140,Nautilus (14.x),"Upstream packages,Distribution packages",Ubuntu,No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Within half a year of release,"I delay upgrades due to concerns about performance regressions,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,iostat,restful","Supermicro,Fujitsu","Intel Xeon,AMD Epyc","SSD (SATA, SAS),NVMe",LVM (ceph-volume),Yes,"10 Gb/s Ethernet,DAC (Copper)",IPv6 only,U.2 (2.5-inch),Single,No advanced config with NUMA.,100 to 500,BlueStore,Replication (3x),"OpenNebula,KVM / QEMU,KRBD directly on Linux systems,rbd-nbd",Yes,No,Yes,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD,iSCSI (LIO with /dev/rbd),rbd-nbd",Yes,,,,,,,,,,,,,,,,,,,,Production,"Cloud,Virtualization,Archive Storage","Linux kernel CephFS mount,ceph-fuse,NFS (nfs-ganesha),NFS (Kernel NFS server),CIFS (SMB, Samba)",501 - 2500,128G+,1,No,No,Yes,"Multiple File Systems within a Ceph Cluster,LazyIO",9 (Promoter),"No (Please specify) - Not really using it in our way of life. Meaning, we're not using it because it's not useful or interesting, but we manage our cluster with the cli and we don't really use the dashboard in that process.",0,0,0,0,0,0,0,0,0,0,0,Hard to say if you're not really using it.,"Ceph-metrics,Grafana (custom),Nagios/icinga,Telegraf","ceph-deploy,Ansible (other / homebrew)",
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance",Commercial,5,4,0,0,0,2,3,,5,3,3,2,1,4,2,5,"Mailing list,Reporting issues via the bug tracker,Contributing / enhancing documentation,Ceph events / conferences",10 (Promoter),,"MDS availability during upgrades (no need to take cephfs offline, i.e. restarting last active node) Insight in ops performance (slow ops) throughout the cluster (tracing). Detailed Insight in data usage (RocksDB, WAL, data)",Netherlands,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,Yes,"Out of curiosity,To inform my decisions about when or whether to upgrade",1,21,21,6,Nautilus (14.x),"Upstream packages,Distribution packages",Ubuntu,No,Within a month after release,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,iostat,Other (Please specify) - telegraf, telemetry,dashboard",Dell,Intel Xeon,"SSD (SATA, SAS)",Containerized daemons,No,"10 Gb/s Ethernet,AOC / Fiber",IPv6 only,U.2 (2.5-inch),Single,no NUMA issues / difficulties,10 to 100,BlueStore,Replication (3x),"OpenNebula,rbd-nbd",Yes,No,Yes,Development / Testing,Cloud,"No, it is not needed","librbd (e.g., in combination with qemu),iSCSI (LIO with /dev/rbd),rbd-nbd",Yes,,,,,,,,,,,,,,,,,,,,Development / Testing,"Build,Cloud,Home Directories,Other (Please specify) - webhosting","Linux kernel CephFS mount,ceph-fuse",1 - 5,4G - 15G,1,No,No,Yes,"Multiple File Systems within a Ceph Cluster,LazyIO",10 (Promoter),No (Please specify) - used to type commands,5,0,0,0,0,0,0,0,0,0,0,To configure alerting,"Ceph Dashboard,InfluxDB,Ceph-metrics,Grafana (custom),Telegraf",Ansible (ceph-ansible),
Less than 1 year,"Scalability,Cost,High availability,Data durability / reliability / integrity",Commercial,5,0,0,0,0,1,0,,5,5,3,3,1,3,4,5,,5 (Detractor),"While Ceph is nice if it works and everything is set up, it is a trmendous way to get there. I already fear the first major problem. I hope i'll be able to fix it.",Management and Configuration for starter-clusters should become a NoOp.,Germany,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,30,30,12,Nautilus (14.x),Vendor packages,Debian,No,Longer,Other (Please specify) - the updates are vendor providef,Longer,Other (Please specify) - vendor provided,I don't know / default set,Supermicro,AMD Epyc,"SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",No,10 Gb/s Ethernet,IPv4 only,M.2,Single,Cheap,10 to 100,BlueStore,Replication (3x),"Kubernetes,Proxmox",Yes,Yes,Yes,Development / Testing,"Video,CDN",Other (Please specify) - what is rbd-mirror?,Linux kernel RBD,Unknown,Development / Testing,"Video,CDN",3,1,S3,Boto,RGW (built-in),None,0,Use case:,No,,,,,,,,,Development / Testing,Virtualization,"Linux kernel CephFS mount,ceph-fuse",51 - 100,2G - 3G,3,Unknown,Unknown,Yes,,5 (Detractor),No (Please specify) - havent seen it yet,0,0,0,0,0,0,0,0,0,0,0,dont know it!,"Proxmox,InfluxDB","Ansible (other / homebrew),Proxmox",
Between 1-2 years,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance",Commercial,4,0,0,4,4,0,5,,5,2,4,5,,4,4,4,"IRC / Slack / etc,Reporting issues via the bug tracker,Ceph events / conferences",10 (Promoter),"I was using Ceph in many companies over the years, I think is a solid product and I wish more people use it instead of blind belief in ""cloud"".","* Like in minio - S3 web portal for ppl and CLI client * CEPH CSI operator - adding CSI to k8s is not fun, and right now, docs might need to be updated here and there * A good tool to ""optimise"" S3 - with the whole monitoring thing, ceph might say what parameters change to improve performance. ( Like the problem with chunks, by default are diff that AWS S3 and this was a problem to debug why app was very slow )",United Kingdom,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I didn't realize they existed",,1,20,20,12,Luminous (12.x),Upstream packages,Debian,"Yes, for performance reasons",Within a month after release,Other (Please specify) - In past update broke my cluster,Within half a year of release,I delay upgrades due to concerns about stability,"balancer,crash,devicehealth,iostat,pg_autoscaler,restful",Other (Please specify) - AWS i3en,"Intel Xeon,AMD Epyc,Other (Please specify) - I wish to use ARM",NVMe,"LVM (ceph-volume),At-rest encryption (dmcrypt),All-in-one OSD (default, colocated, no separate metadata device)",Yes,10 Gb/s Ethernet,IPv4 only,"M.2,U.2 (2.5-inch)",No preference / beats me,,10 to 100,BlueStore,"Replication (3x),Erasure coding","Kubernetes,Proxmox,OpenShift",Yes,Yes,No,Production,Containers,"No, it is not needed",Linux kernel RBD,Yes,Production,"Build,Log,Big Data & Analytics,AI/ML,Cloud,Containers,Archive Storage,Backup,OLTP",2,5,"S3,RGW admin API,NFS re-export of S3 buckets","Amazon SDK,Boto3,Other (Please specify) - minio",RGW (built-in),"HAproxy,Other (Please specify) - traefik",0,Not right now,No,,,,,,,,,,,,,,,,,,,10 (Promoter),Yes,5,2,2,3,4,4,0,2,0,5,3,"Bucket management, create it or delete it.","Ceph Dashboard,ceph_exporter",cephadm,
More than 5 years,"Open source,Cost,Feature set,High availability,Data durability / reliability / integrity",Commercial,5,5,1,0,0,2,3,,5,3,5,4,1,3,2,3,"Mailing list,Reporting issues via the bug tracker",6 (Detractor),,,"Germany,United Kingdom,United States","No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I didn't realize they existed",,12,0,0,0,Nautilus (14.x),Distribution packages,RHEL / CentOS / Fedora,No,Within a month after release,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability","balancer,devicehealth,iostat",Dell,"Intel Xeon,AMD Epyc","HDD (SATA, SAS)","LVM (ceph-volume),At-rest encryption (dmcrypt)",Yes,10 Gb/s Ethernet,IPv4 only,,Single,NUMA issues,1000 to 3000,Both Filestore and BlueStore,Replication (4 or more copies),"Kubernetes,KVM / QEMU",Yes,Yes,No,Production,"Scratch,Log,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,"Log,Archive Storage,Backup",0,0,S3,"Boto3,custom / in-house","RGW (built-in),LDAP","Dedicated custom hardware (Alteon, F5, etc)",0,Use case:,No,,,,,,,,,,,,,,,,,,,6 (Detractor),No (Please specify) - -,4,1,4,0,0,0,0,0,0,0,0,-,"Ceph Dashboard,Prometheus,Grafana (custom),Graphite,node_exporter,ceph_exporter","Ansible (ceph-ansible),Puppet,ceph-deploy",
More than 5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Commercial,3,4,0,2,0,0,3,,4,2,3,3,4,5,4,3,"Mailing list,Reporting issues via the bug tracker,Contributing code,Contributing / enhancing documentation,Ceph events / conferences",9 (Promoter),It just works!,Lower latency I/O,Netherlands,"Yes, telemetry is enabled on some of my clusters",I haven't gotten around to it yet,,I enabled 'ident' and configured a cluster description and/or email address,"No, I didn't realize they existed",,15,1000,9000,3000,"Octopus (15.x),Nautilus (14.x)",Upstream packages,Ubuntu,No,Longer,"I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,crash,iostat,pg_autoscaler","Dell,Supermicro","Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",No,"10 Gb/s Ethernet,25 Gb/s Ethernet",IPv6 only,U.2 (2.5-inch),Single,"No NUMA and smaller nodes (1U, 1CPU, 10x NVMe)",100 to 500,BlueStore,"Replication (3x),Erasure coding","Cloudstack,KVM / QEMU",Yes,Yes,No,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,Production,CDN,4,75000,S3,"Amazon SDK,Boto,Boto3,s3cmd / s3tools",RGW (built-in),Varnish,0,Use case:,No,,,,,,,,,,,,,,,,,,,6 (Detractor),No (Please specify) - I never use it,,,,,,,,,,,,I barely look at the dashboard,"InfluxDB,Zabbix","Salt / DeepSea,ceph-deploy",
Between 2-5 years,"Open source,Scalability,Cost,Feature set,Data durability / reliability / integrity",Personal,5,4,0,0,3,0,2,,5,3,4,3,4,4,3,3,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker",8 (Passive),,,Switzerland,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,496,496,330,Octopus (15.x),Upstream packages,Ubuntu,"Yes, for performance reasons",Within a week after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a month of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes","balancer,crash,devicehealth,diskprediction,iostat,pg_autoscaler,restful,Other (Please specify) - prometheus",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),MicroSD or other low-end flash",LVM (ceph-volume),No,10 Gb/s Ethernet,Dual stack,"M.2,U.2 (2.5-inch)",Single,Better more smaller servers than less large ones.,10 to 100,BlueStore,"Replication (3x),Erasure coding",None,No,No,Yes,,,,,,,,,,,,,,,,,,,,,,,,,Production,"Video,Home Directories,Archive Storage","Linux kernel CephFS mount,NFS (nfs-ganesha)",6 - 50,16G - 31G,3,No,Yes,No,,10 (Promoter),Yes,5,3,3,4,4,0,0,0,4,0,0,i'm happy with it,"Ceph Dashboard,Prometheus,Grafana (custom),Telegraf",ceph-deploy,
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Commercial,5,1,0,0,0,0,0,,5,3,5,3,2,5,5,5,,9 (Promoter),,,China,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,Enabling telemetry would require review by my organization's security team and I have chosen not request a review,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.",,,"No, I didn't realize they existed",,15,2000,15283,5000,"Luminous (12.x),Jewel (10.x)",Upstream packages,RHEL / CentOS / Fedora,No,Never,I delay upgrades because of the effort involved,Longer,"I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability",I don't know / default set,"Dell,HPE (Hewlett Packard),Huawei,Other (Please specify) - Inspur","ARM,Intel Xeon","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Mix of yes and no,"10 Gb/s Ethernet,25 Gb/s Ethernet",IPv4 only,,Dual,,100 to 500,"Filestore (XFS, EXT4, BTRFS)",Replication (3x),"OpenStack,Kubernetes",Yes,Yes,No,"Development / Testing,Production",Containers,"Yes, to migrate volumes between clusters",Linux kernel RBD,No,"Development / Testing,Production","Build,Video,Containers,Archive Storage,Backup",20,230000,"S3,Swift","Amazon SDK,Boto,Boto3,s3cmd / s3tools,Other (Please specify) - python-swiftclient","RGW (built-in),Keystone","Dedicated custom hardware (Alteon, F5, etc)",0,"caching a large number of download, especially in small file like htmljs and so on",,,,,,,,,,,,,,,,,,,,9 (Promoter),Yes,,,,,,,,,,,,"my clusters version is luminous, haven't use dashboard","Grafana (custom),Telegraf","Ansible (ceph-ansible),ceph-deploy",
More than 5 years,"Open source,Scalability,Cost,Integration with adjacent technologies",Commercial,5,3,0,1,0,5,2,,3,4,2,5,3,3,5,4,"Mailing list,Reporting issues via the bug tracker,Contributing code,Ceph events / conferences",7 (Passive),Community support,"Improved observability (monitoring, metrics and logging)","Ireland {Republic},Netherlands,United States","No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations",,,Yes,Out of curiosity,13,304,4104,1368,Nautilus (14.x),Vendor packages,RHEL / CentOS / Fedora,No,Longer,I delay upgrades because of the effort involved,Within a year,I delay upgrades due to the effort required to upgrade,pg_autoscaler,Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe",Defaults / I don't know,No,"10 Gb/s Ethernet,25 Gb/s Ethernet",IPv4 only,,Dual,,10 to 100,BlueStore,"Replication (3x),Erasure coding","OpenStack,Kubernetes",Yes,Yes,No,Production,"Containers,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,"Cloud,Containers,Archive Storage",8,100,S3,"Amazon SDK,Boto3",RGW (built-in),"None,Other (Please specify)",0,Use case:,No,,,,,,,,,,,,,,,,,,,7 (Passive),Yes,4,1,2,1,1,1,0,0,0,1,4,"Visibility of clients connections, in/out traffic","Ceph Dashboard,Prometheus,Telegraf","Ansible (ceph-ansible),Chef",
Less than 1 year,"Scalability,Cost,High availability,Data durability / reliability / integrity,Performance","Government,Academic",5,3,0,0,0,0,0,,4,4,4,2,2,4,3,4,,7 (Passive),,,Japan,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,1200,1200,900,Octopus (15.x),Distribution packages,RHEL / CentOS / Fedora,No,Within a month after release,I only upgrade as needed to address issues,Within a year,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability,Other (Please specify) - No idea yet",,"Supermicro,We build our own,Other (Please specify) - other minor vendors","Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),No,40 Gb/s Ethernet,IPv4 only,,Single,,100 to 500,BlueStore,Erasure coding,None,No,No,Yes,,,,,,,,,,,,,,,,,,,,,,,,,Production,"Big Data & Analytics,HPC",Linux kernel CephFS mount,6 - 50,4G - 15G,1,Unknown,No,No,,,,,,,,,,,,,,,,,,
Between 1-2 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Government,5,3,0,0,0,0,3,youtube videos,3,4,3,5,2,5,3,3,,10 (Promoter),the stability and always available feeling,"1,Focus on container security. Way too many CVE in current (Centos based) releases. Trivy/Harbor lights up like a Christmas tree every time. I have a hard time explaining and justifying this. Maybe do Fedora builds....? 2, Cephfs assign rights to a directory without automatically allowing subdirectories. Maybe it is already possible but I have not been able to figure it out. Would make generic client mounting much easier.  3, ceph cli completion",Sweden,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.",,,"No, I knew they existed by have not used them",,0,0,0,0,Octopus (15.x),Upstream packages,Ubuntu,No,Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within half a year of release,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability",I don't know / default set,Dell,Intel Xeon,"SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",No,10 Gb/s Ethernet,IPv4 only,,No preference / beats me,,10 to 100,BlueStore,Replication (3x),Kubernetes,Yes,Yes,Yes,Production,Containers,"No, it is not needed",Linux kernel RBD,No,Development / Testing,"AI/ML,Containers",3,0,S3,None,RGW (built-in),None,0,Use case:,No,,,,,,,,"Pulsar, NATS",Production,"AI/ML,Containers,Home Directories","Linux kernel CephFS mount,Other (Please specify)",6 - 50,4G - 15G,2,No,No,No,,10 (Promoter),Yes,5,4,1,4,1,1,0,1,3,1,0,More details on cephfs clients.,"Ceph Dashboard,Prometheus",Rook,
Between 1-2 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Commercial,5,2,0,0,0,0,2,,4,3,3,3,2,4,2,4,"Reporting issues via the bug tracker,Contributing code",9 (Promoter),,,"Germany,Iceland","No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,Yes,Out of curiosity,2,450,500,300,Octopus (15.x),Upstream packages,Debian,No,Within a month after release,I delay upgrades due to concerns about regression,Within half a year of release,I delay upgrades due to concerns about stability,"balancer,iostat",Other (Please specify) - Celestica,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),At-rest encryption (dmcrypt),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Yes,25 Gb/s Ethernet,IPv4 only,"U.2 (2.5-inch),Add In Cards","Single,Dual",,100 to 500,BlueStore,"Replication (3x),Erasure coding",OpenStack,Yes,No,No,"Staging,Production","AI/ML,HPC,Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4 (Detractor),No (Please specify) - No experience,0,0,0,0,0,0,0,0,0,0,0,"n/a, not using it","Prometheus,Grafana (custom),node_exporter",cephadm,
Between 1-2 years,"Open source,Scalability,Cost,Feature set,High availability",Commercial,5,4,0,1,0,0,0,,5,3,3,4,5,4,3,5,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),"open source, scalable, reliable, highly available",,Germany,"Yes, telemetry is enabled on some of my clusters",My cluster(s) are on a protected network that does not have access to the internet,,I left 'ident' disabled (the default),Yes,Out of curiosity,1,36,36,28,Nautilus (14.x),Distribution packages,Debian,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes","balancer,crash,devicehealth,diskprediction,pg_autoscaler,restful","HPE (Hewlett Packard),Supermicro","Intel Xeon,Intel (other)","SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Yes,"1000 Mb/s Ethernet (GigE, gigabit),RJ45 (Copper)",IPv4 only,"M.2,U.2 (2.5-inch)",Single,scale out instead of scale up; costs per server,10 or fewer,BlueStore,Replication (3x),"Cloudstack,KVM / QEMU,Other (Please specify) - libvirt",Yes,No,No,Production,"Cloud,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10 (Promoter),Yes,5,4,5,4,4,4,0,0,0,0,3,none,"Ceph Dashboard,Prometheus,Nagios/icinga",Ansible (other / homebrew),
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability",Academic,4,5,0,0,0,0,2,,5,4,3,4,2,4,3,3,"Mailing list,Reporting issues via the bug tracker,Other (Please specify) - ceph BoF (SC)",8 (Passive),"It's very dependent on the level of expertise available. Would recommend where there are enough resources and interest, perhaps not otehrwise...","cephadm to support non-containerized environments (""take over"" from ceph-deploy)",United States,"Yes, telemetry is enabled on some of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,3,8100,10000,6000,"Luminous (12.x),Nautilus (14.x)",Distribution packages,RHEL / CentOS / Fedora,No,Within a month after release,I delay upgrades due to concerns about regression,Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,pg_autoscaler",HPE (Hewlett Packard),Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),At-rest encryption (dmcrypt),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",No,"40 Gb/s Ethernet,25 Gb/s Ethernet",IPv4 only,U.2 (2.5-inch),No preference / beats me,"we default to 2 socket servers with 24-28 OSDs per server, but different core counts per socket could easily change that if available",1000 to 3000,"Filestore (XFS, EXT4, BTRFS),BlueStore","Replication (3x),Erasure coding",OpenStack,Yes,Yes,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,"Development / Testing,Production","Big Data & Analytics,HPC",4,500,S3,"s3cmd / s3tools,Other (Please specify) - minio client (mc), globus",RGW (built-in),Other (Please specify) - bgp-ecmp,0,Use case: have not yet thought - will consider when we reach Octopus,No,,,,,,,,,Development / Testing,"Scratch,HPC",Linux kernel CephFS mount,1 - 5,4G - 15G,2,No,No,No,Multiple File Systems within a Ceph Cluster,5 (Detractor),No (Please specify) - too used to using the cli... rarely look at dashboard. Monitor health via nagios. I would like to use the dashboard more :-),2,0,0,0,0,0,0,0,0,0,0,can't say...,Nagios/icinga,"Puppet,ceph-deploy",
Between 2-5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity",Commercial,5,2,0,0,1,0,0,,5,1,5,3,1,3,3,5,,8 (Passive),"Depends on the needs and the budget.  If it matches both, I would recommend Ceph for sure.",,France,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,2,72,144,48,Luminous (12.x),"Upstream packages,Vendor packages",Debian,No,Longer,I delay upgrades because there are too many updates,Never (clusters remain on major release they were deployed with),I delay upgrades due to concerns about stability,I don't know / default set,Dell,Intel Xeon,"HDD (SATA, SAS)",LVM (ceph-volume),Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",IPv4 only,,Single,,10 to 100,BlueStore,"Replication (2x),Replication (3x)",KVM / QEMU,Yes,No,No,Production,"Virtualization,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu)",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0 (Detractor),No (Please specify) - I don't use Ceph Dashboard because I never tried it,0,0,0,0,0,0,0,0,0,0,0,I don't use Ceph Dashboard because I never tried it,Nagios/icinga,ceph-deploy,
Less than 1 year,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies","Commercial,Academic",5,3,0,0,0,0,0,,4,4,0,1,3,5,5,3,,10 (Promoter),,,Indonesia,"Yes, telemetry is enabled on some of my clusters",,,I left 'ident' disabled (the default),Yes,"To make decisions about which type of storage devices to buy (HDD vs SSD),To make decisions about which vendor(s) or model(s) of storage devices to buy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 1-2 years,"Open source,Scalability,Security,Performance","Government,Personal",4,3,2,0,0,0,3,,4,4,5,5,0,0,2,3,Contributing / enhancing documentation,7 (Passive),,,United States,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,5,100,700,200,Nautilus (14.x),"Upstream packages,Distribution packages",Ubuntu,"Yes, for performance reasons",Longer,"I apply upgrades quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about new functionality",Within half a year of release,"I delay upgrades due to concerns about performance regressions,I delay upgrades to avoid a changed user experience due to new features,I upgrade quickly to get any available bug fixes or security fixes,I only upgrade as needed to address specific bugs or needed features","crash,devicehealth,diskprediction,iostat,restful","Dell,Supermicro","ARM,Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Mix of yes and no,"10 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet",IPv4 only,Add In Cards,Dual,,100 to 500,"Filestore (XFS, EXT4, BTRFS),BlueStore,Other (Please specify) - zfs",Replication (2x),"OpenNebula,KVM / QEMU",Yes,Yes,Yes,Proof of Concept (PoC),"Scratch,Log,Containers","No, it is not needed",Linux kernel RBD,Yes,Proof of Concept (PoC),"Scratch,CDN,Cloud,Virtualization",10,800,"S3,Swift,NFS re-export of S3 buckets",Boto3,LDAP,"HAproxy,Nginx",2,Use case:,No,,,,,,,Http,,"Development / Testing,Staging,Production,Proof of Concept (PoC)","Scratch,Build,Log,Video,CDN,Cloud,Containers,Virtualization","libcephfs,NFS (nfs-ganesha),NFS (Kernel NFS server)",501 - 2500,16G - 31G,5,No,No,No,Multiple File Systems within a Ceph Cluster,8 (Passive),Yes,3,2,4,5,4,3,1,1,2,3,0,zfs pool status,"Grafana (custom),Nagios/icinga,Telegraf","Ansible (other / homebrew),cephadm",
Between 2-5 years,"Open source,Scalability,High availability,Performance,Integration with adjacent technologies",Academic,5,4,0,2,0,0,0,Reddit,5,3,5,3,4,5,4,5,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),"Highly flexible, reliable and scalable solution",,Croatia,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,512,512,170,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues",Longer,I delay upgrades due to the effort required to upgrade,"iostat,restful,Other (Please specify) - zabbix, dashboard",Dell,AMD Epyc,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,10 Gb/s Ethernet,IPv4 only,,No preference / beats me,,10 to 100,"Filestore (XFS, EXT4, BTRFS)",Replication (3x),"OpenStack,KVM / QEMU",Yes,No,No,Production,"Scratch,Big Data & Analytics,Cloud","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10 (Promoter),Yes,5,0,3,0,0,0,0,0,0,0,0,N/A,Zabbix,ceph-deploy,
Less than 1 year,"Open source,Scalability,Data durability / reliability / integrity,Performance","Commercial,Personal",4,1,0,0,0,0,0,Community Guides out there,5,4,3,4,4,5,5,4,,10 (Promoter),"Already pushing it, especially with the addition of cephadm",iSCSI VMware integrations,United States,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,Enabling telemetry would require review by my organization's security team and I have chosen not request a review",,,"No, I didn't realize they existed",,4,256,300,128,Octopus (15.x),Upstream packages,Ubuntu,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within half a year of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes","devicehealth,iostat,I don't know / default set","Dell,HPE (Hewlett Packard),Supermicro,Other (Please specify) - Gigabyte","Intel Xeon,AMD Epyc","SSD (SATA, SAS)","Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device)",Mix of yes and no,"10 Gb/s Ethernet,25 Gb/s Ethernet,100 Gb/s Ethernet",IPv4 only,"U.2 (2.5-inch),Add In Cards",No preference / beats me,As long as it has enough IO and can keep up a single CPU is fine,100 to 500,BlueStore,Replication (3x),"VMware,KVM / QEMU",Yes,No,Yes,"Development / Testing,Proof of Concept (PoC)","Scratch,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,"Development / Testing,Proof of Concept (PoC)","Scratch,Virtualization,Home Directories",Linux kernel CephFS mount,6 - 50,2G - 3G,5,Unknown,Unknown,No,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,4,5,4,4,4,3,3,,5,,4,RBD iSCSI managment,"Ceph Dashboard,Prometheus,Grafana (custom),node_exporter",cephadm,
Between 2-5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies","Commercial,Non-profit,Personal",5,4,0,0,0,0,1,internet-forums,5,5,4,4,4,4,3,5,Mailing list,10 (Promoter),software defined storage at its best,more and better control of scrubbing. Do all over the weekend or only once a month. Do some energy-saving (CO2). ....and lesser logs to spare SSDs.  easy redeploy of with no reason destroyed deamons within containers. (centOS 8 updateproblems 15.2.7->.8 impossible to update to .9 or .10)   better support fo,Germany,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I didn't realize they existed",,3,78,165,75,"Octopus (15.x),Nautilus (14.x)",Vendor packages,"Ubuntu,Debian,RHEL / CentOS / Fedora,SUSE",No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a month of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,Other (Please specify) - education know-how experience","pg_autoscaler,I don't know / default set",We build our own,AMD (other),"HDD (SATA, SAS),NVMe","Partitions,All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons,Defaults / I don't know",Yes,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet",Dual stack,M.2,Single,"cheap, AMD B550 Board, 3200G or 3400G AMD CPU, new 2021 will have 4650G AMD.",10 to 100,BlueStore,"Replication (3x),Erasure coding","OpenStack,Microsoft Windows,KVM / QEMU,KRBD directly on Linux systems",Yes,No,Yes,"Development / Testing,Production","Scratch,Build,Log,CDN,Cloud,Containers,Virtualization,Home Directories","Other (Please specify) - Yes, we evaluate this","librbd (e.g., in combination with qemu),Linux kernel RBD",No,,,,,,,,,,,,,,,,,,,,Production,"Scratch,Build,Log,CDN,Cloud,Containers,Virtualization,Home Directories,Archive Storage","Linux kernel CephFS mount,ceph-fuse,CIFS (SMB, Samba)",51 - 100,4G - 15G,2,Yes,No,No,Multiple File Systems within a Ceph Cluster,10 (Promoter),Yes,4,5,3,5,5,4,0,0,4,0,3,"1. tuning of scrubbing. 2. logs went off by change of dashboard-host 3. which client is consuming what, with a little history on that.","Ceph Dashboard,Zabbix",cephadm,
Between 2-5 years,"Open source,Scalability,High availability,Data durability / reliability / integrity","Commercial,Government",5,4,0,0,0,0,3,,5,2,2,2,2,5,,3,Mailing list,10 (Promoter),It solves a lot of problems once you exceed the needs of say a single NFS/ZFS server.,,Netherlands,"No, telemetry is not enabled on any clusters",My cluster(s) are on a protected network that does not have access to the internet,,,"No, I knew they existed by have not used them",,3,980,1580,530,Nautilus (14.x),Upstream packages,Debian,No,Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","iostat,Other (Please specify) - prometheus","HPE (Hewlett Packard),Supermicro",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","All-in-one OSD (default, colocated, no separate metadata device)",Yes,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,This is cheaper than CPUs with a high frequency,100 to 500,BlueStore,Replication (3x),"OpenStack,Proxmox,KVM / QEMU",Yes,Yes,No,Production,"Virtualization,Archive Storage","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Proof of Concept (PoC),"CDN,Big Data & Analytics,Backup",3,1000,S3,"Amazon SDK,Boto3",RGW (built-in),"HAproxy,Varnish",0,Use case:no,No,,,,,,,,,,,,,,,,,,,5 (Detractor),No (Please specify) - Don't use it.,0,0,0,0,0,0,0,0,0,0,0,N/A,"Prometheus,node_exporter","ceph-deploy,cephadm,Proxmox",
Between 1-2 years,"Open source,High availability,Data durability / reliability / integrity","Commercial,Government",4,4,3,0,2,5,4,,5,3,5,5,3,4,4,,"Mailing list,Reporting issues via the bug tracker",7 (Passive),It all depends on the use case. For some use cases GlusterFS is better and other a simple NFS server,"Metrics, with do not costs so much resources. In a 1300+ OSD. The manager is to busy and can choke in all data. Lower latency < 10ms (but almost impossible with Ceph) one command to replace a osd (mark it out destroy and when there is a new disk, auto config it) Lees impact on the cluster, when a server went down.","Germany,Netherlands","No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.,I do not believe that the Ceph community should be collecting any of this information",,,"No, I didn't realize they existed",,3,15000,22000,7000,"Octopus (15.x),Nautilus (14.x)","Upstream packages,Distribution packages","Ubuntu,RHEL / CentOS / Fedora",No,Longer,"I delay upgrades due to concerns about new functionality,I delay upgrades due to concerns about regression",Within half a year of release,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,restful",Supermicro,"Intel Xeon,AMD Epyc","HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet",IPv6 only,U.2 (2.5-inch),Dual,To spread to load,1000 to 3000,BlueStore,Replication (3x),"OpenStack,KVM / QEMU,rbd-nbd",Yes,Yes,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Production,Backup,4,-1,"S3,Swift,RGW admin API","Amazon SDK,Boto,Boto3,s3cmd / s3tools,custom / in-house",RGW (built-in),Varnish,3,Use case:,No,,,,,,,,,Proof of Concept (PoC),"Log,Containers",ceph-fuse,6 - 50,4G - 15G,2,Unknown,Unknown,Unknown,,7 (Passive),Yes,,,,,,,,,,,,none,"Ceph Dashboard,Zabbix",Salt / DeepSea,
More than 5 years,"Open source,Scalability,Cost",Commercial,5,4,1,1,1,4,3,,4,5,2,2,2,5,2,5,"Mailing list,Ceph events / conferences",7 (Passive),Open source and no vender lock,Better RBD performance scaling Recommended values for ceph.conf parameters More flexible options for Ceph deployment,United States,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,I am worried that even the anonymized information reported by telemetry may compromise my organization's security.",,,"No, I didn't realize they existed",,3,450,690,230,"Octopus (15.x),Nautilus (14.x)","Upstream packages,Distribution packages,Vendor packages","Ubuntu,RHEL / CentOS / Fedora",No,Longer,I only upgrade as needed to address issues,Within half a year of release,"I upgrade quickly to get new features,I delay upgrades due to the effort required to upgrade,I only upgrade as needed to address specific bugs or needed features",balancer,"Supermicro,QCT (Quanta),Intel",Intel Xeon,"HDD (SATA, SAS),NVMe","LVM (ceph-volume),Partitions,lvmcache,Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),All-in-one OSD (default, colocated, no separate metadata device),bcache,CAS (Intel),Containerized daemons",Mix of yes and no,"25 Gb/s Ethernet,100 Gb/s Ethernet,DAC (Copper)",IPv4 only,"U.2 (2.5-inch),E1.L (EDSFF, ruler form factor)",Dual,Need PCIe lanes,10 to 100,BlueStore,"Replication (2x),Replication (3x),Erasure coding","KVM / QEMU,KRBD directly on Linux systems",Yes,Yes,Yes,"Development / Testing,Proof of Concept (PoC)","Scratch,Cloud,Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD",No,"Development / Testing,Proof of Concept (PoC)","Video,CDN,Big Data & Analytics,Cloud,Backup",5,1,S3,"Boto,Boto3,s3cmd / s3tools",RGW (built-in),"HAproxy,DNS round-robin,None",0,Use case:,No,,,,,,,,,"Development / Testing,Proof of Concept (PoC)","Build,HPC,Virtualization","Linux kernel CephFS mount,NFS (nfs-ganesha)",6 - 50,32G - 64G,3,No,No,No,Multiple File Systems within a Ceph Cluster,7 (Passive),No (Please specify) - Most commands are scripted,0,0,0,0,0,0,0,0,0,0,0,N/A,"Ceph Dashboard,Prometheus,Grafana (custom),node_exporter","Ansible (ceph-ansible),cephadm",
Less than 1 year,"Open source,Scalability,Cost,Feature set,Security,High availability,Data durability / reliability / integrity,Integration with adjacent technologies,Other (Please specify) - The only software of its kind - reliably store data on a random selection of computers and HDDs",Personal,5,3,0,0,0,0,4,,5,5,3,2,5,4,5,5,"Contributing code,Contributing / enhancing documentation",10 (Promoter),It actually works as described and is one of the only projects of its kind.,"CIFS/SMB gateway GUI in the dashboard OSD partitioning/provisioning Gui in the dashboard (for example, to use part of a boot drive as an OSD) Mobile app which connects to the cluster and can push notifications on events",Canada,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,Yes,"Out of curiosity,To inform my decisions about when or whether to upgrade",2,400,450,300,Octopus (15.x),Upstream packages,Ubuntu,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within half a year of release,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades to avoid a changed user experience due to new features,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,iostat,pg_autoscaler","Supermicro,We build our own","Intel Xeon,AMD (other)","HDD (SATA, SAS)","All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",No,"10 Gb/s Ethernet,RJ45 (Copper),DAC (Copper)",IPv4 only,,Single,NUMA is annoying and now AMD can deliver incredibly dense core counts in single package,10 to 100,BlueStore,"Replication (3x),Erasure coding","Microsoft Windows,KVM / QEMU",No,No,Yes,,,,,,,,,,,,,,,,,,,,,,,,,Production,"Video,Virtualization,Archive Storage","Linux kernel CephFS mount,ceph-fuse,CIFS (SMB, Samba)",6 - 50,2G - 3G,2,No,No,No,,10 (Promoter),Yes,5,5,5,5,5,0,0,2,5,0,4,Samba export creation,"Ceph Dashboard,Prometheus,Grafana (custom)",cephadm,
Between 1-2 years,"Open source,Scalability,Data durability / reliability / integrity",Commercial,4,4,0,0,2,0,3,,5,3,4,5,3,4,3,4,Mailing list,8 (Passive),,,United States,"No, telemetry is not enabled on any clusters",My organization's security policy does not allow storage systems to communicate with vendors or external organizations,,,"No, I didn't realize they existed",,3,144,420,250,Octopus (15.x),Distribution packages,Ubuntu,No,Longer,I delay upgrades because of the effort involved,Longer,I delay upgrades due to the effort required to upgrade,"balancer,crash,iostat",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),No,"1000 Mb/s Ethernet (GigE, gigabit),10 Gb/s Ethernet,RJ45 (Copper),AOC / Fiber",IPv4 only,U.2 (2.5-inch),Dual,,10 to 100,BlueStore,"Replication (2x),Replication (3x),Erasure coding",None,No,Yes,No,,,,,,Production,"Archive Storage,Backup",1,5,"S3,RGW admin API",Amazon SDK,RGW (built-in),None,3,Use case:,Yes,1,No,1,Global,Check if you use an archive zone,,,,,,,,,,,,,,8 (Passive),Yes,4,1,2,4,3,0,0,0,0,3,3,CRUSH (erasure code) profiles,"Ceph Dashboard,Graphite",cephadm,
Between 2-5 years,"Scalability,Cost,High availability,Data durability / reliability / integrity,Performance",Personal,5,1,0,0,0,0,0,,2,4,3,0,0,0,3,1,,10 (Promoter),,,Russian Federation,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,1,325,325,160,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort",Within a month of release,"I upgrade quickly to get new features,I upgrade quickly because it is easy,I delay upgrades due to concerns about stability","balancer,devicehealth,pg_autoscaler","Supermicro,We build our own","Intel Xeon,Intel (other)","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,"1000 Mb/s Ethernet (GigE, gigabit),RJ45 (Copper)",IPv4 only,U.2 (2.5-inch),Single,,10 to 100,BlueStore,"Replication (2x),Replication (3x)",OpenNebula,Yes,No,No,Production,Virtualization,"No, it is not needed",Linux kernel RBD,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10 (Promoter),Yes,4,4,4,2,2,2,0,0,0,0,0,setting cluster wide parameters like ceph osd set-backfillfull-ratio ...,"Ceph Dashboard,Zabbix",Other (Please specify) - manual deployment (wan't to know where is my data stored),
Less than 1 year,"Open source,Scalability,Feature set,Data durability / reliability / integrity,Performance",Non-profit,5,3,2,2,2,2,2,,4,3,4,5,3,5,5,3,"IRC / Slack / etc,Mailing list",8 (Passive),,,United States,"Yes, telemetry is enabled on all of my clusters",,,I enabled 'ident' and configured a cluster description and/or email address,"No, I knew they existed by have not used them",,1,3481,3481,2785,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues",Within half a year of release,"I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","balancer,crash,iostat,pg_autoscaler",Supermicro,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),Yes,100 Gb/s Ethernet,IPv4 only,"M.2,U.2 (2.5-inch)",Dual,to get more cores and memory channels to run more OSDs pr server,100 to 500,BlueStore,"Replication (3x),Erasure coding",None,Yes,No,Yes,Production,Archive Storage,"No, it is not needed",Linux kernel RBD,No,,,,,,,,,,,,,,,,,,,,Production,"Video,Archive Storage",Linux kernel CephFS mount,6 - 50,4G - 15G,3,No,No,Yes,Multiple File Systems within a Ceph Cluster,9 (Promoter),Yes,5,4,5,3,5,5,1,0,2,0,4,nope at the momnet,"Ceph Dashboard,Prometheus,Ceph-metrics,node_exporter",Ansible (ceph-ansible),
Less than 1 year,Open source,Commercial,,,,,,,,,,,,,,,,,,5 (Detractor),,,Afghanistan,"Yes, telemetry is enabled on some of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,4,10,10,10,Octopus (15.x),Upstream packages,Ubuntu,"Yes, in front of EC pools for functionality",Within a week after release,I apply upgrades quickly to get any available bug fixes or security fixes,Within a month of release,I upgrade quickly to get any available bug fixes or security fixes,balancer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,4,1,0,1,0,0,5,,3,3,3,3,3,5,4,4,"Mailing list,Reporting issues via the bug tracker,Contributing code,Ceph events / conferences",7 (Passive),,,China,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,My cluster(s) are running a Ceph version older than Luminous",,,"No, I knew they existed by have not used them",,8,3000,20000,10000,"Luminous (12.x),Infernalis (9.x) or older","We build our own packages,We built a custom version","Debian,RHEL / CentOS / Fedora",No,Longer,"I delay upgrades because of the effort involved,I only upgrade as needed to address issues",Never (clusters remain on major release they were deployed with),"I delay upgrades due to the effort required to upgrade,I only upgrade as needed to address specific bugs or needed features","balancer,restful","Dell,Other (Please specify) - INSPUR","Intel Xeon,Intel (other)","HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),All-in-one OSD (default, colocated, no separate metadata device)",Yes,10 Gb/s Ethernet,IPv4 only,Add In Cards,Dual,,1000 to 3000,"Filestore (XFS, EXT4, BTRFS),BlueStore","Replication (3x),Erasure coding","OpenStack,Kubernetes",Yes,No,Yes,Production,Cloud,"No, it is not needed","librbd (e.g., in combination with qemu)",Yes,,,,,,,,,,,,,,,,,,,,Production,"AI/ML,Containers","Linux kernel CephFS mount,ceph-fuse",101 - 500,65G - 127G,6,Yes,No,Yes,"Mantle (Programmable Metadata Balancer),LazyIO",3 (Detractor),No (Please specify) - we haven't use dashboard,0,0,0,0,0,0,0,0,0,0,0,we haven't use dashboard,"Prometheus,Other (Please specify) - private tools","ceph-deploy,Rook",
Between 2-5 years,"Scalability,Cost,High availability,Data durability / reliability / integrity",Other (Please specify) - Cloud Services Provider,5,,,,,,,,5,5,,,,5,5,,,10 (Promoter),Reliability,,New Zealand,"No, telemetry is not enabled on any clusters",I haven't gotten around to it yet,,,"No, I didn't realize they existed",,1,80,80,80,Nautilus (14.x),Distribution packages,Debian,No,Longer,"I delay upgrades because there are too many updates,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability",I don't know / default set,HPE (Hewlett Packard),Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore)",Yes,10 Gb/s Ethernet,IPv4 only,"M.2,U.2 (2.5-inch)",Dual,Hyperconverged,10 to 100,BlueStore,Replication (3x),"Proxmox,KVM / QEMU",Yes,No,No,Production,"Cloud,Virtualization","No, it is not needed",Other (Please specify) - Not sure,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7 (Passive),No (Please specify) - Used for monitoring only,5,,,,,,,,,,,N/A,"Ceph Dashboard,Proxmox,Zabbix",Proxmox,
Less than 1 year,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Integration with adjacent technologies",Other (Please specify) - Manufacturing,5,3,0,0,0,0,1,,5,5,5,5,3,5,5,3,"Mailing list,Reporting issues via the bug tracker",10 (Promoter),,Cephfs asynchronous replication with file locking. (Strech cluster?) Not sure if this would be possible.,United States,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,Enabling telemetry would require review by my organization's security team and I have chosen not request a review",,,"No, I knew they existed by have not used them",,1,48,48,24,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about regression",Within a month of release,"I upgrade quickly to get new features,I delay upgrades due to concerns about performance regressions,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy,I delay upgrades due to concerns about stability","balancer,crash,iostat,pg_autoscaler",HPE (Hewlett Packard),Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","At-rest encryption (dmcrypt),All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",Yes,"10 Gb/s Ethernet,DAC (Copper),AOC / Fiber",IPv4 only,"M.2,U.2 (2.5-inch)",No preference / beats me,,10 to 100,BlueStore,"Replication (3x),Erasure coding","OpenStack,Microsoft Windows,KRBD directly on Linux systems",Yes,No,Yes,Development / Testing,"Containers,Virtualization","Other (Please specify) - No, have not tested but would like to implement","Linux kernel RBD,iSCSI (tcmu-runner)",No,,,,,,,,,,,,,,,,,,,,Production,Archive Storage,"Linux kernel CephFS mount,CIFS (SMB, Samba)",101 - 500,4G - 15G,1,No,Yes,No,"Mantle (Programmable Metadata Balancer),Multiple File Systems within a Ceph Cluster,LazyIO",8 (Passive),No (Please specify) - I've got used to using CLI,5,1,1,3,4,4,2,0,4,0,5,CLI access from dashboard,"Ceph Dashboard,Prometheus,node_exporter,ceph_exporter",cephadm,
Less than 1 year,Open source,Commercial,,,,,,,,,,,,,,,,,,0 (Detractor),,,Afghanistan,"No, telemetry is not enabled on any clusters",,,,,,1,1,1,1,Octopus (15.x),Upstream packages,Ubuntu,No,Never,I apply upgrades quickly to get any available bug fixes or security fixes,Within a month of release,I upgrade quickly to get new features,,IBM,ARM,"HDD (SATA, SAS)",LVM (ceph-volume),Yes,100 Mb/s Ethernet,IPv4 only,,Single,,5000+,"Filestore (XFS, EXT4, BTRFS)",Replication (2x),"OpenStack,Microsoft Windows",Yes,Yes,Yes,,Scratch,"Yes, for DR","librbd (e.g., in combination with qemu)",Yes,,Scratch,1,1,S3,Amazon SDK,RGW (built-in),HAproxy,1,Use case:,,,,,,,,,,,Scratch,Linux kernel CephFS mount,1 - 5,1G,1,Yes,Yes,Yes,,0 (Detractor),Yes,,,,,,,,,,,,1,,,
More than 5 years,"Open source,Scalability,Cost,High availability,Data durability / reliability / integrity",Government,5,1,0,0,0,0,1,,5,5,4,2,3,5,5,5,,8 (Passive),Fits most storage needs,"Transparent SSD cache, e.g. Lvm cache",Germany,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,1,72,72,48,Octopus (15.x),Upstream packages,Ubuntu,No,Longer,I delay upgrades due to concerns about regression,Within half a year of release,"I delay upgrades due to concerns about performance regressions,I only upgrade as needed to address specific bugs or needed features,I delay upgrades due to concerns about stability","crash,devicehealth,diskprediction,iostat,pg_autoscaler",We build our own,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)","Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Yes,10 Gb/s Ethernet,IPv4 only,"M.2,Add In Cards",Single,Usually higher frequency,10 to 100,BlueStore,Replication (2x),OpenNebula,Yes,Yes,Yes,Production,Virtualization,"No, it is not needed","librbd (e.g., in combination with qemu)",No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 5 years,"Scalability,Feature set,High availability,Data durability / reliability / integrity,Performance",Commercial,5,3,0,0,0,0,0,,5,3,5,5,2,4,3,4,Mailing list,10 (Promoter),,,Germany,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy does not allow storage systems to communicate with vendors or external organizations,My cluster(s) are running a Ceph version older than Luminous",,,"No, I didn't realize they existed",,2,1024,1144,381,"Octopus (15.x),Infernalis (9.x) or older",Upstream packages,"Debian,RHEL / CentOS / Fedora",No,Within a month after release,I apply upgrades quickly to get any available bug fixes or security fixes,Longer,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to the effort required to upgrade,I delay upgrades due to concerns about stability","crash,devicehealth",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),Yes,10 Gb/s Ethernet,IPv4 only,,Dual,,500 to 1000,Both Filestore and BlueStore,Replication (3x),None,Yes,No,Yes,Production,"Log,Big Data & Analytics","No, it is not needed",Linux kernel RBD,No,,,,,,,,,,,,,,,,,,,,Production,"Log,Big Data & Analytics,Archive Storage","ceph-fuse,NFS (nfs-ganesha),CIFS (SMB, Samba)",101 - 500,16G - 31G,1,No,No,Yes,"Inline Data,Multiple File Systems within a Ceph Cluster",7 (Passive),No (Please specify),5,0,0,0,0,0,0,0,0,0,0,"none, prefer cli","Ceph Dashboard,Ceph-Dash,Prometheus,Grafana (custom),Nagios/icinga",Puppet,
Between 1-2 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance","Commercial,Personal",5,1,0,0,1,0,1,,5,3,3,5,3,5,4,5,IRC / Slack / etc,9 (Promoter),,multiple device classes per pool move rocksdb to separate device after the fact more consistent cli commands,Germany,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,1,200,200,80,Octopus (15.x),Upstream packages,"Debian,RHEL / CentOS / Fedora","Yes, for performance reasons",Within a week after release,"I apply upgrades quickly to get any available bug fixes or security fixes,I apply upgrades quickly because they require little effort,I delay upgrades due to concerns about regression",Within a month of release,"I upgrade quickly to get new features,I upgrade quickly to get any available bug fixes or security fixes,I upgrade quickly because it is easy,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,pg_autoscaler","Supermicro,Raspberry Pi,We build our own","ARM,Intel Xeon,AMD Epyc,AMD (other)","HDD (SATA, SAS),SSD (SATA, SAS),NVMe,Optane","All-in-one OSD (default, colocated, no separate metadata device),Containerized daemons",No,10 Gb/s Ethernet,IPv4 only,"M.2,U.2 (2.5-inch)",No preference / beats me,,10 to 100,BlueStore,"Replication (3x),Erasure coding","VMware,Proxmox,KVM / QEMU,OpenShift,None",No,No,Yes,,,,,,,,,,,,,,,,,,,,,,,,,Production,"Scratch,Build,Log,Video,CDN,Containers,Virtualization,Home Directories,Archive Storage","Linux kernel CephFS mount,NFS (nfs-ganesha),NFS (Kernel NFS server),CIFS (SMB, Samba)",6 - 50,16G - 31G,12,No,Yes,Yes,"Inline Data,Multiple File Systems within a Ceph Cluster",10 (Promoter),Yes,5,5,3,4,5,5,5,5,5,4,3,fs management,"Ceph Dashboard,Prometheus,node_exporter,ceph_exporter","Rook,cephadm",
Between 1-2 years,"Open source,Security,Data durability / reliability / integrity",Academic,5,1,3,0,0,0,5,no local group in Japan,5,3,3,5,3,4,4,5,Reporting issues via the bug tracker,5 (Detractor),under evaluation phases,consistent crash recovery tools work load analytics for performance tuning,Japan,"No, telemetry is not enabled on any clusters","My cluster(s) are on a protected network that does not have access to the internet,My organization's security policy has reviewed the information reported by telemetry and judged that it should not be shared",,,"No, I knew they existed by have not used them",,2,2000,3000,1500,Octopus (15.x),Distribution packages,RHEL / CentOS / Fedora,No,Longer,"I apply upgrades quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about new functionality,I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to the effort required to upgrade,I upgrade quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about stability","crash,devicehealth,diskprediction,iostat",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet",IPv4 only,,Dual,,100 to 500,BlueStore,"Replication (3x),Erasure coding",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Between 2-5 years,"Open source,Scalability,Data durability / reliability / integrity,Integration with adjacent technologies",Commercial,4,3,1,4,0,5,3,,5,4,3,4,5,2,3,5,IRC / Slack / etc,10 (Promoter),The most stable and well-used open source storage software.,Deduplication Converged method for inter-cluster replication #currently rbd-mirror and rgw multisite are too different and difficult to use together S3 client tool like as s3cmd,Japan,"No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,My cluster(s) are on a protected network that does not have access to the internet",,,"No, I didn't realize they existed",,2,20,30,10,"Octopus (15.x),Nautilus (14.x)","Upstream packages,Distribution packages",RHEL / CentOS / Fedora,No,Longer,I only upgrade as needed to address issues,Within a month of release,I upgrade quickly to get new features,"balancer,iostat,pg_autoscaler","HPE (Hewlett Packard),Supermicro",Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS),NVMe","LVM (ceph-volume),Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Containerized daemons",Yes,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,,10 to 100,BlueStore,"Replication (2x),Replication (3x),Erasure coding","OpenStack,KRBD directly on Linux systems,OpenShift",Yes,Yes,Yes,Development / Testing,"Cloud,Containers","Other (Please specify) - Yes, but just for testing","librbd (e.g., in combination with qemu),Linux kernel RBD",Yes,Development / Testing,"Cloud,Containers,Backup",3,100,S3,"Amazon SDK,s3cmd / s3tools",RGW (built-in),HAproxy,2,No,Yes,1,No,1,Per bucket,,,Kafka,,Development / Testing,"Cloud,Containers",ceph-fuse,1 - 5,2G - 3G,2,No,No,No,"Inline Data,Mantle (Programmable Metadata Balancer),Multiple File Systems within a Ceph Cluster,LazyIO",10 (Promoter),Yes,5,4,3,3,5,3,0,0,3,4,4,visualized CRUSH tree/map,"Ceph Dashboard,Prometheus","Ansible (ceph-ansible),Rook",
Less than 1 year,"Open source,Security,Data durability / reliability / integrity",Academic,5,1,1,0,0,0,5,,5,4,3,5,0,3,4,5,"Reporting issues via the bug tracker,Ceph events / conferences",5 (Detractor),we are still under early deployment phase,consistent recovery tools analytics for stability/performance tuning,Japan,"No, telemetry is not enabled on any clusters","My organization's security policy does not allow storage systems to communicate with vendors or external organizations,My organization's security policy has reviewed the information reported by telemetry and judged that it should not be shared",,,"No, I knew they existed by have not used them",,2,2000,3000,1200,Octopus (15.x),Distribution packages,RHEL / CentOS / Fedora,No,Longer,"I apply upgrades quickly to get any available bug fixes or security fixes,I only upgrade as needed to address issues,I delay upgrades due to concerns about regression",Within a year,"I delay upgrades due to the effort required to upgrade,I upgrade quickly to get any available bug fixes or security fixes,I delay upgrades due to concerns about stability","crash,devicehealth,diskprediction,iostat",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",Separate journal device (Filestore) or RocksDB/WAL device (BlueStore),Yes,"10 Gb/s Ethernet,25 Gb/s Ethernet",IPv4 only,,Dual,,100 to 500,Both Filestore and BlueStore,"Replication (3x),Erasure coding",OpenStack,Yes,Yes,Yes,"Development / Testing,Production,Proof of Concept (PoC)","Log,Big Data & Analytics,HPC,Containers,Archive Storage",Other (Please specify) - under consideration,"librbd (e.g., in combination with qemu),Linux kernel RBD",Unknown,Development / Testing,"Log,Cloud,Archive Storage,Backup",2,300,S3,s3cmd / s3tools,"RGW (built-in),Keystone","HAproxy,Nginx,DNS round-robin",0,Use case:,No,,,,,,,,,Development / Testing,"Containers,Home Directories","Linux kernel CephFS mount,libcephfs,NFS (Kernel NFS server)",51 - 100,4G - 15G,20,Unknown,Unknown,Unknown,Multiple File Systems within a Ceph Cluster,8 (Passive),Yes,5,5,5,5,3,3,3,1,1,3,3,tireing,Ceph Dashboard,"Ansible (ceph-ansible),Ansible (other / homebrew)",
Between 2-5 years,"Open source,Scalability,Feature set,High availability",Commercial,5,4,0,1,2,0,0,,3,3,2,2,1,3,3,2,"Mailing list,Reporting issues via the bug tracker,Ceph events / conferences",8 (Passive),,,Germany,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I didn't realize they existed",,2,200,250,80,Nautilus (14.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,I delay upgrades due to concerns about regression,Within half a year of release,I delay upgrades due to concerns about stability,"crash,devicehealth,diskprediction,iostat,pg_autoscaler",Dell,AMD (other),"HDD (SATA, SAS),SSD (SATA, SAS),NVMe",LVM (ceph-volume),No,10 Gb/s Ethernet,IPv4 only,U.2 (2.5-inch),Dual,,10 to 100,BlueStore,Replication (3x),"Kubernetes,Proxmox,Xen,rbd-nbd",Yes,Yes,Yes,Production,"Cloud,Containers,Virtualization","No, it is not needed","librbd (e.g., in combination with qemu),Linux kernel RBD,iSCSI (tcmu-runner),rbd-nbd",Yes,Production,"Archive Storage,Backup",3,500,S3,"Amazon SDK,Boto3",RGW (built-in),HAproxy,0,Use case:-,No,,,,,,,,,Production,Containers,Linux kernel CephFS mount,1 - 5,2G - 3G,3,Unknown,Yes,Yes,,9 (Promoter),Yes,5,2,4,4,5,5,2,3,3,4,5,-,"Ceph Dashboard,Prometheus,Grafana (custom),Nagios/icinga,node_exporter,ceph_exporter","Ansible (ceph-ansible),Rook",
Less than 1 year,"Open source,Scalability,High availability,Data durability / reliability / integrity,Performance",Commercial,5,4,0,3,0,1,1,,5,5,4,4,3,3,4,5,,10 (Promoter),,,"Germany,Poland","No, telemetry is not enabled on any clusters","I haven't gotten around to it yet,Enabling telemetry would require review by my organization's security team and I have chosen not request a review",,,"No, I didn't realize they existed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Less than 1 year,"Open source,Scalability,High availability,Data durability / reliability / integrity",Commercial,5,4,0,0,0,0,0,,5,5,4,4,4,5,5,5,"Mailing list,Reporting issues via the bug tracker",7 (Passive),,,Argentina,"Yes, telemetry is enabled on all of my clusters",,,I left 'ident' disabled (the default),"No, I knew they existed by have not used them",,1,90,90,90,Octopus (15.x),Upstream packages,RHEL / CentOS / Fedora,No,Within a month after release,"I delay upgrades due to concerns about new functionality,Other (Please specify) - to be pretty sure that we don't have new problems",Within a month of release,"I delay upgrades due to concerns about performance regressions,I delay upgrades due to concerns about stability","balancer,crash,devicehealth,diskprediction,iostat",Dell,Intel Xeon,"HDD (SATA, SAS),SSD (SATA, SAS)",LVM (ceph-volume),No,10 Gb/s Ethernet,IPv4 only,M.2,Single,,10 to 100,Not sure,Replication (3x),"Cloudstack,KVM / QEMU",Yes,No,Yes,Production,"Cloud,Containers,Virtualization,Archive Storage","No, it is incompatible with my performance requirements","librbd (e.g., in combination with qemu),Linux kernel RBD",Unknown,,,,,,,,,,,,,,,,,,,,Production,Archive Storage,NFS (nfs-ganesha),101 - 500,2G - 3G,4,Unknown,Unknown,No,"Inline Data,Mantle (Programmable Metadata Balancer),Multiple File Systems within a Ceph Cluster,LazyIO",5 (Detractor),"No (Please specify) - Dashboard is not really complete to perform several actions. Like monitoring, logs, or repair the cluster",5,5,5,5,5,5,2,4,4,2,5,Full cluster health monitoring,"Ceph Dashboard,Prometheus,Grafana (custom),Zabbix,Nagios/icinga",cephadm,
More than 5 years,"Open source,Scalability,Cost,Feature set,High availability,Data durability / reliability / integrity,Performance,Integration with adjacent technologies",Academic,5,5,0,0,3,0,4,,5,2,4,3,3,5,3,5,"IRC / Slack / etc,Mailing list,Reporting issues via the bug tracker,Contributing code,Ceph events / conferences,Member of the Ceph Foundation",10 (Promoter),,,"France,Switzerland","Yes, telemetry is enabled on all of my clusters",,"basic: cluster capacity, cluster size, ceph version, number of pools and file systems, which configuration options have been adjusted)",I left 'ident' disabled (the default),Yes,To inform my decisions about when or whether to upgrade,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
